{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERT_Only.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tgk_opqd2prV",
        "xnOxcgwk5WiU",
        "M2mT9dqsm5Vy",
        "hV30ckW6NElG",
        "beqWAJVP2_aP",
        "lom9KYDHpGFM",
        "hFtpg_GR4iNJ",
        "3O5iEjYD7H-s",
        "mwKqXD75KiPI",
        "ZuGYeCAq97yQ",
        "kYlESuc6HmGF",
        "MQ3f5gO7Hhu0"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "visualqa",
      "display_name": "Python 3.8.5 64-bit ('visualqa': conda)"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHAMYwNfv5Th"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKIzzX6J3QtS"
      },
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "from enum import Enum\n",
        "from torch.nn import functional as F\n",
        "import time\n",
        "import logging\n",
        "import numpy as np\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "random.seed(13)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quJKUGIXv8zk"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1t9DLnOsFN_"
      },
      "source": [
        "def pytorch_cos_sim(a: torch.Tensor, b: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
        "    This function can be used as a faster replacement for 1-scipy.spatial.distance.cdist(a,b)\n",
        "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
        "    \"\"\"\n",
        "    if not isinstance(a, torch.Tensor):\n",
        "        a = torch.tensor(a)\n",
        "\n",
        "    if not isinstance(b, torch.Tensor):\n",
        "        b = torch.tensor(b)\n",
        "\n",
        "    if len(a.shape) == 1:\n",
        "        a = a.unsqueeze(0)\n",
        "\n",
        "    if len(b.shape) == 1:\n",
        "        b = b.unsqueeze(0)\n",
        "\n",
        "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
        "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
        "    return torch.mm(a_norm, b_norm.transpose(0, 1))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQdj5p8fzl-1"
      },
      "source": [
        "def string_sentence(i,qr):\n",
        "    return qr.loc[i,'Title'] + ' '  + ' '.join(qr.loc[i,'Tags']) + ' '  + qr.loc[i,'Text']"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STnKB8R7NVEa"
      },
      "source": [
        "# Model Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DebNqaCF2uE_",
        "outputId": "80f3f059-7a3e-46a5-f1ae-562fd10ac2a9"
      },
      "source": [
        "folder_quora = '/home/ckm/visualqatickets/superuser/Data'\n",
        "folder = './related/Models'\n",
        "os.makedirs(folder,exist_ok = True)\n",
        "BATCH_SIZE = 8\n",
        "fin_BATCH_SIZE = 32\n",
        "n_worker = 8\n",
        "margin = 0.9\n",
        "\n",
        "#---------\n",
        "#Note that amp cannot be used without sigmoid\n",
        "use_amp = True\n",
        "use_sig = True\n",
        "#----------\n",
        "\n",
        "use_sig_eval = False\n",
        "# eval_BATCH_SIZE = 4 \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgk_opqd2prV"
      },
      "source": [
        "# Creating Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnOxcgwk5WiU"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1RqPrOk5Ynm"
      },
      "source": [
        "with open(folder_quora+'/pandas/split.txt','rb') as a:\n",
        "    train_qr=pickle.load(a)\n",
        "    dev_qr = pickle.load(a)\n",
        "    test_qr = pickle.load(a)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5qm94QOFH2V"
      },
      "source": [
        "with open(folder_quora+'/pairs_related/train_dev_test.txt','rb') as a:\n",
        "    train_data=pickle.load(a)\n",
        "    train_score = pickle.load(a)\n",
        "    dev_data = pickle.load(a)\n",
        "    dev_score = pickle.load(a)\n",
        "    test_data = pickle.load(a)\n",
        "    test_score = pickle.load(a)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik9u67jbYLXB"
      },
      "source": [
        "with open(folder_quora+'/pairs_related/data_pos_neg.txt','rb') as a:\n",
        "    train_data_pos = pickle.load(a)\n",
        "    train_data_neg = pickle.load(a)\n",
        "    dev_data_pos = pickle.load(a)\n",
        "    dev_data_neg = pickle.load(a)\n",
        "    test_data_pos = pickle.load(a)\n",
        "    test_data_neg = pickle.load(a)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkaN37_BkO9P"
      },
      "source": [
        "qr = pd.concat([train_qr,dev_qr,test_qr])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2mT9dqsm5Vy"
      },
      "source": [
        "### Creating Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6I9YioKE8uK"
      },
      "source": [
        "from transformers import BertTokenizerFast, BertConfig,BertModel\n",
        "config = BertConfig()\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXPoomQx4Zby"
      },
      "source": [
        "class custom_dataset(Dataset):\n",
        "    def __init__(self,qr,qr_idx,label,transform = None):\n",
        "        self.qr = qr\n",
        "        self.qr_idx = qr_idx\n",
        "        self.label = label\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        id1 = self.qr_idx[idx][0]\n",
        "        id2 = self.qr_idx[idx][1]\n",
        "\n",
        "        sample = dict()\n",
        "\n",
        "        t1 = self.qr.loc[id1,'Title'] + ' ' + ' '.join(self.qr.loc[id1,'Tags']) + ' ' + self.qr.loc[id1,'Text']\n",
        "        t2 = self.qr.loc[id2,'Title']+ ' ' + ' '.join(self.qr.loc[id2,'Tags']) + ' ' + self.qr.loc[id2,'Text'] \n",
        "        indexed_t1 = tokenizer.encode(t1,max_length = 350,truncation = True)\n",
        "        indexed_t2 = tokenizer.encode(t2,max_length = 350,truncation = True)\n",
        "\n",
        "        while(len(indexed_t1)<350):\n",
        "            indexed_t1.append(0)\n",
        "        while(len(indexed_t2)<350):\n",
        "            indexed_t2.append(0)\n",
        "\n",
        "        ten_t1 = torch.tensor(indexed_t1)\n",
        "        ten_t2 = torch.tensor(indexed_t2)\n",
        "\n",
        "        # print(ten_t1)\n",
        "        # print(ten_t2)\n",
        "\n",
        "        label_cos = self.label[idx]\n",
        "        if(label_cos==0.0):\n",
        "            label_cos = -1.0\n",
        "\n",
        "        try:\n",
        "            sample[\"label\"] = self.label[idx]\n",
        "            sample[\"token\"] = [ten_t1,ten_t2] # torch.Size([batch_size, 512])\n",
        "            sample[\"label_cos\"] = label_cos\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "        \n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk3AIIM8Yk0G"
      },
      "source": [
        "class pos_dataset(Dataset):\n",
        "    def __init__(self,qr,qr_idx):\n",
        "        self.qr = qr\n",
        "        self.qr_idx = qr_idx\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        id1 = self.qr_idx[idx][0]\n",
        "        id2 = self.qr_idx[idx][1]\n",
        "\n",
        "        sample = dict()\n",
        "\n",
        "        t1 = '[CLS]' + self.qr.loc[id1,'Title'] + ' ' + ' '.join(self.qr.loc[id1,'Tags']) + ' ' + self.qr.loc[id1,'Text'] + '[SEP]'\n",
        "        t2 = '[CLS]' + self.qr.loc[id2,'Title']+ ' ' + ' '.join(self.qr.loc[id2,'Tags']) + ' ' + self.qr.loc[id2,'Text'] + '[SEP]'\n",
        "        indexed_t1 = tokenizer.encode(t1,max_length = 512,truncation = True)\n",
        "        indexed_t2 = tokenizer.encode(t2,max_length = 512,truncation = True)\n",
        "\n",
        "        while(len(indexed_t1)<512):\n",
        "            indexed_t1.append(0)\n",
        "        while(len(indexed_t2)<512):\n",
        "            indexed_t2.append(0)\n",
        "\n",
        "        ten_t1 = torch.tensor(indexed_t1)[:512]\n",
        "        ten_t2 = torch.tensor(indexed_t2)[:512]\n",
        "\n",
        "    \n",
        "\n",
        "        try:\n",
        "            sample[\"token\"] = [ten_t1,ten_t2] # torch.Size([batch_size, 512])\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "        \n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.qr_idx)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSXhMsWFZFdD"
      },
      "source": [
        "class ConcatDataset(Dataset):\n",
        "    def __init__(self, *datasets):\n",
        "        self.datasets = datasets\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return tuple(d[i] for d in self.datasets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(d) for d in self.datasets)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV30ckW6NElG"
      },
      "source": [
        "## Train, dev and test dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRNteBxHOomg"
      },
      "source": [
        "train_dataset = custom_dataset(train_qr,\n",
        "                               train_data,\n",
        "                               train_score)\n",
        "\n",
        "# train_MNR_dataset = pos_dataset(train_qr,\n",
        "#                                 train_data_pos)\n",
        "\n",
        "# fin_train_dataset = ConcatDataset(train_dataset,train_MNR_dataset)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d78ebAQBSdJI"
      },
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    pin_memory=True,\n",
        "    num_workers = n_worker,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        "    )"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU4CNMybfgqI"
      },
      "source": [
        "dev_dataset = custom_dataset(pd.concat([train_qr,dev_qr]),\n",
        "                             dev_data,\n",
        "                             dev_score)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUvKZ6rifxQW"
      },
      "source": [
        "dev_loader = DataLoader(\n",
        "    dev_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    pin_memory=True,\n",
        "    num_workers = n_worker,\n",
        "    drop_last = True\n",
        "    )"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe1vYfiifhM2"
      },
      "source": [
        "test_dataset = custom_dataset(pd.concat([train_qr,test_qr]),\n",
        "                              test_data,\n",
        "                              test_score)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP_N_Kb5f1jD"
      },
      "source": [
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    pin_memory=True,\n",
        "    num_workers = n_worker,\n",
        "    drop_last = True\n",
        "    )"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVzPL-EMTXpM"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beqWAJVP2_aP"
      },
      "source": [
        "## Loading Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Ye9P1XUUToUA"
      },
      "source": [
        "from torchvision import models\n",
        "\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "bert = bert.to(device)\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in bert.embeddings.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in bert.encoder.layer[0].parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lom9KYDHpGFM"
      },
      "source": [
        "## Model Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtngyhwHmEll"
      },
      "source": [
        "class bertmodel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.fc = nn.Linear(768,512)\n",
        "    \n",
        "    def forward(self,X1):\n",
        "        #Taking average embeddings of all the words\n",
        "        # bert_out = torch.mean(self.bert(X1).last_hidden_state,dim=1) #[BATCH_SIZE,768] \n",
        "        # out = self.fc(bert_out)\n",
        "\n",
        "        out = self.bert(X1).last_hidden_state[:,0,:]\n",
        "        # print(out.shape)\n",
        "        return out"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uH4JgQ7oVtB"
      },
      "source": [
        "model = bertmodel().to(device)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2Mw6eG5KqVn"
      },
      "source": [
        "# Losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFtpg_GR4iNJ"
      },
      "source": [
        "## Multiple Negative Ranking loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMIA3Ydod65P"
      },
      "source": [
        "class negrankloss():\n",
        "    def __init__(self,scale: float = 20.0):\n",
        "        # self.cosine_sim = nn.CosineSimilarity()\n",
        "        self.scale = scale\n",
        "        self.cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "    def cal_loss(self,emb1: torch.Tensor,emb2: torch.Tensor):\n",
        "        scores  = pytorch_cos_sim(emb1,emb2) *self.scale\n",
        "        # print(f'The scores of cosine similarity for MNRloss is {scores}')\n",
        "        labels = torch.tensor(range(len(scores)), dtype=torch.long, device=scores.device)\n",
        "        loss = self.cross_entropy(scores, labels)\n",
        "        return loss"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMbsWT-DlN1G"
      },
      "source": [
        "MNRloss = negrankloss()"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O5iEjYD7H-s"
      },
      "source": [
        "## Online Constrantive Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv3V30gQ7LKy"
      },
      "source": [
        "class SiameseDistanceMetric(Enum):\n",
        "    \"\"\"\n",
        "    The metric for the contrastive loss\n",
        "    \"\"\"\n",
        "    EUCLIDEAN = lambda x, y: F.pairwise_distance(x, y, p=2)\n",
        "    MANHATTAN = lambda x, y: F.pairwise_distance(x, y, p=1)\n",
        "    COSINE_DISTANCE = lambda x, y: 1-F.cosine_similarity(x, y)\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zSArl7MQ4sO"
      },
      "source": [
        "class OnlineConstrantiveLoss():\n",
        "    def __init__(self,distance_metric=SiameseDistanceMetric.COSINE_DISTANCE,margin: float = 0.8):\n",
        "        self.distance_metric = distance_metric\n",
        "        self.margin = margin\n",
        "\n",
        "    def cal_loss(self,emb1,emb2,labels,size_average=False):\n",
        "        distance_matrix = self.distance_metric(emb1,emb2)\n",
        "        # print(f'distance matrix of OCloss is {distance_matrix}')\n",
        "        negs = distance_matrix[labels == 0]\n",
        "        poss = distance_matrix[labels == 1]\n",
        "        # print(f'positive and negatives are {poss} and {negs}')\n",
        "\n",
        "        # select hard positive and hard negative pairs\n",
        "        negative_pairs = negs[negs < (poss.max() if len(poss) > 1 else negs.mean())]\n",
        "        positive_pairs = poss[poss > (negs.min() if len(negs) > 1 else poss.mean())]\n",
        "\n",
        "        # print(f'positive and negative pairs are {positive_pairs} and {negative_pairs}')\n",
        "\n",
        "        positive_loss = positive_pairs.pow(2).sum()\n",
        "        negative_loss = F.relu(self.margin - negative_pairs).pow(2).sum()\n",
        "        # print(f'positive loss is {positive_loss} and negative loss is {negative_loss}')\n",
        "        loss = positive_loss + negative_loss\n",
        "        return loss\n",
        "    "
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr5sO1wM8NIc"
      },
      "source": [
        "OCloss = OnlineConstrantiveLoss(margin = margin)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76ht5qjQMVsm"
      },
      "source": [
        "## Cross Entropy Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSxbp43HMVHn"
      },
      "source": [
        "class crossentropy():\n",
        "    def __init__(self):\n",
        "        self.cross_entropy = nn.BCELoss()\n",
        "        self.cos_sim =  nn.CosineSimilarity(dim=1,eps=1e-6)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        self.BCE_with_sig = nn.BCEWithLogitsLoss()\n",
        "    \n",
        "    def cal_loss(self,emb1:torch.Tensor,emb2:torch.Tensor,label:torch.Tensor):\n",
        "        sim = (self.cos_sim(emb1,emb2))\n",
        "\n",
        "        if(use_sig == True):\n",
        "            loss = self.BCE_with_sig(sim,label)\n",
        "        else:\n",
        "            sim[sim<0] = 0\n",
        "            loss = self.cross_entropy(sim,label)\n",
        "        return loss"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfdnKk2HUW9X"
      },
      "source": [
        "BCEloss = crossentropy()"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQZLDcbJeYuF"
      },
      "source": [
        "## Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0frkfuWbpal"
      },
      "source": [
        "class cos_sim():\n",
        "    def __init__(self):\n",
        "        self.cossim = nn.CosineEmbeddingLoss()\n",
        "\n",
        "    def cal_loss(self,emb1,emb2,target):\n",
        "        loss = self.cossim(emb1,emb2,target)\n",
        "        return loss"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKJD374XhaAI"
      },
      "source": [
        "Cosloss = cos_sim()"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaMPRBwZdBjN"
      },
      "source": [
        "# Evaluators: AUC(0.05) and Binary accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAUnsRkLdQLC"
      },
      "source": [
        "import sklearn.metrics as skm"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2Z1Yk_TEPj9"
      },
      "source": [
        "class AUC():\n",
        "    def __init__(self,max_fpr:float = 0.05):\n",
        "        self.max_fpr = 0.05\n",
        "        self.cos_sim =  nn.CosineSimilarity(dim=1,eps=1e-6)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def cal(self,model:bertmodel,loader:DataLoader):\n",
        "        y_pred = []\n",
        "        y_true = []\n",
        "        y_pred_sig = []\n",
        "        start = time.time()\n",
        "        model.eval()\n",
        "        for i,batch in enumerate(loader):\n",
        "            with torch.no_grad():\n",
        "                label = batch[\"label\"]\n",
        "                label = label.float()\n",
        "                token = batch[\"token\"]\n",
        "\n",
        "                token[0],token[1] = token[0].to(device), token[1].to(device)\n",
        "                label = label.to(device)\n",
        "            \n",
        "                # compute the model output\n",
        "                yhat1 = model(token[0])\n",
        "                yhat2 = model(token[1])\n",
        "\n",
        "                if(i%1000==0):\n",
        "                    print(f'    {i} iterations have been done in {time.time()-start} seconds')\n",
        "\n",
        "                sim = (self.cos_sim(yhat1,yhat2))\n",
        "                a=self.sig(sim)\n",
        "                a[a>=0.5] = 1\n",
        "                a[a<0.5] = 0\n",
        "\n",
        "                y_pred_sig.append(a.cpu().numpy())\n",
        "                y_pred.append(sim.cpu().numpy())\n",
        "                y_true.append(label.cpu().numpy())\n",
        "\n",
        "                del  label, token,yhat1,yhat2,sim,a \n",
        "        \n",
        "        y_true = np.array(y_true).flatten()\n",
        "        y_pred = np.array(y_pred).flatten()\n",
        "        y_pred_sig = np.array(y_pred_sig).flatten()\n",
        "        Bin_score = skm.accuracy_score(y_true,y_pred_sig) \n",
        "        AUC_score = skm.roc_auc_score(y_true,y_pred,max_fpr = self.max_fpr)\n",
        "        conf_matrix = skm.confusion_matrix(y_true,y_pred_sig)\n",
        "        plt.hist(y_pred,bins='auto')\n",
        "        plt.show()\n",
        "        return AUC_score,Bin_score,conf_matrix\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXrynrKZLn2M"
      },
      "source": [
        "AUC_evaluator = AUC()"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwKqXD75KiPI"
      },
      "source": [
        "# Calculating loss and executing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKJz98PeGje6"
      },
      "source": [
        "save_dir = folder + '/model_state_dict'\n",
        "os.makedirs(save_dir,exist_ok = True)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gc2jj5PKkGP"
      },
      "source": [
        "from torch.optim import Adam\n",
        "from torch.nn import BCELoss\n",
        "import time\n",
        "import copy\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "def train_model(train_loader, model,num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    since = time.time()\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=4e-6)\n",
        "    # optimizer = Adam([{'params': model.res50.parameters(), 'lr': 4e-6},\n",
        "    #                   {'params': model.bert.parameters(), 'lr': 4e-6},\n",
        "    #                   {'params': model.bertl_text.parameters(), 'lr': 4e-6},\n",
        "    #                   {'params': model.bertl_attribute.parameters(), 'lr': 4e-6},\n",
        "    #                   {'params': model.bridge_seven_conv.parameters()},\n",
        "    #                   {'params': model.bridge_one_conv.parameters()},\n",
        "    #                   {'params': model.flatten.parameters()},\n",
        "    #                   {'params': model.linear_2d.parameters()},\n",
        "    #                   {'params': model.soft_2d.parameters()},\n",
        "    #                   {'params': model.final_2d.parameters()},\n",
        "    #                   {'params': model.fc.parameters()},\n",
        "    #                   {'params': model.fc2.parameters()},\n",
        "    #                   {'params': model.co_attention.parameters()}\n",
        "    #                   ], lr=4e-4, weight_decay=1e-2)\n",
        "    train_steps = num_epochs\n",
        "    \n",
        "    best_acc = 0.0\n",
        "\n",
        "    acc_steps = fin_BATCH_SIZE/BATCH_SIZE\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "    cos_sim = nn.CosineSimilarity(dim=1,eps=1e-6)\n",
        "    sig = nn.Sigmoid()\n",
        "    # torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "    # Evaluating on dev loader\n",
        "    # print('Running dev_evaluator')\n",
        "    # a,b,c = AUC_evaluator.cal(model,dev_loader)\n",
        "    # print(f'AUC score is {a}  while binary score is {b} and the confusion matrix is {c} on dev loader before training')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # enumerate mini batches\n",
        "        # avg_loss = 0\n",
        "        \n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "        \n",
        "        running_OCloss = 0.0\n",
        "        running_MNRloss = 0.0\n",
        "        running_BCEloss = 0.0\n",
        "        running_Cosloss = 0.0\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        y_pred = []\n",
        "        y_true = []\n",
        "        y_pred_sig = []\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            model.train()\n",
        "\n",
        "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "                \n",
        "                if(i%10000 == 0 and i!=0 ):\n",
        "                    print('Running dev_evaluator for 2000 samples i.e. 1000 iterations. Time taken: 5 min')\n",
        "                    a,b,c = AUC_evaluator.cal(model,dev_loader)\n",
        "                    print(f'AUC score is {a}  while binary score is {b} and the confusion matrix is {c} on dev loader after {i} iterations and {epoch+1} epoch')\n",
        "                    torch.save(model.state_dict(), save_dir+'/bert_cos.bin')\n",
        "                    print(f'Model is saved after {i} iterations ')\n",
        "                    model.train()\n",
        "        \n",
        "                label = batch[\"label\"]\n",
        "                label = label.float()\n",
        "                token = batch[\"token\"]\n",
        "                label_cos = batch['label_cos']\n",
        "\n",
        "                # pos_token = batch[1]['token']\n",
        "\n",
        "                token[0],token[1] = token[0].to(device), token[1].to(device)\n",
        "                # pos_token[0],pos_token[1] = pos_token[0].to(device), pos_token[1].to(device)\n",
        "                label = label.to(device)\n",
        "                label_cos = label_cos.to(device)\n",
        "                \n",
        "                # compute the model output\n",
        "                yhat1= model(token[0])\n",
        "                yhat2 = model(token[1])\n",
        "\n",
        "                sim = (cos_sim(yhat1,yhat2))\n",
        "                a=sig(sim)\n",
        "                a[a>=0.5] = 1\n",
        "                a[a<0.5] = 0\n",
        "\n",
        "                y_pred_sig.append(a.detach().cpu().numpy())\n",
        "                y_pred.append(sim.detach().cpu().numpy())\n",
        "                y_true.append(label.detach().cpu().numpy())\n",
        "\n",
        "                # yhat1_pos = model(pos_token[0])\n",
        "                # yhat2_pos = model(pos_token[1])\n",
        "                \n",
        "                BCEloss_val = BCEloss.cal_loss(yhat1,yhat2,label)\n",
        "                running_BCEloss += BCEloss_val.item()*BATCH_SIZE\n",
        "                BCEloss_val = BCEloss_val/acc_steps\n",
        "\n",
        "                # MNRloss_val = MNRloss.cal_loss(yhat1_pos,yhat2_pos)\n",
        "                # running_MNRloss += MNRloss_val.item()*BATCH_SIZE\n",
        "                # MNRloss_val = MNRloss_val/acc_steps\n",
        "\n",
        "                OCloss_val = OCloss.cal_loss(yhat1,yhat2,label)\n",
        "                running_OCloss +=  OCloss_val.item()*BATCH_SIZE\n",
        "                OCloss_val = OCloss_val/acc_steps\n",
        "\n",
        "                Cosloss_val = Cosloss.cal_loss(yhat1,yhat2,label_cos)\n",
        "                running_Cosloss += Cosloss_val.item()*BATCH_SIZE\n",
        "                Cosloss_val = Cosloss_val/acc_steps\n",
        "\n",
        "                # cos_sim = nn.CosineSimilarity(dim=1)\n",
        "                # print(label_cos, cos_sim(yhat1,yhat2))\n",
        "            \n",
        "            scaler.scale(Cosloss_val).backward(retain_graph=True)\n",
        "            scaler.scale(BCEloss_val).backward(retain_graph=True)\n",
        "            # scaler.scale(MNRloss_val).backward(retain_graph=True)\n",
        "            scaler.scale(OCloss_val).backward()\n",
        "\n",
        "            if((i+1)%acc_steps==0):\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                model.zero_grad()\n",
        "            \n",
        "            if(i%500 == 0 ):\n",
        "                print('OCloss is {} and BCEloss is {} and MNRloss is {} and Cosloss is {}  and time taken is {} after {} iterations'.format(\n",
        "                    running_OCloss/((i+1)*BATCH_SIZE),\n",
        "                    running_BCEloss/((i+1)*BATCH_SIZE),\n",
        "                    running_MNRloss/(i+1)*BATCH_SIZE,\n",
        "                    running_Cosloss/((i+1)*BATCH_SIZE),\n",
        "                    time.time()-since,\n",
        "                    i))\n",
        "            \n",
        "            del  yhat1,yhat2, label, token,Cosloss_val,label_cos\n",
        "\n",
        "        epoch_OC = running_OCloss / len(train_dataset)\n",
        "        epoch_BCE = running_BCEloss / len(train_dataset)\n",
        "        epoch_MNR = running_MNRloss / len(train_dataset)\n",
        "        epoch_Cos = running_Cosloss/len(train_dataset)\n",
        "        print(f'{epoch+1} Epoch completed. OCloss is {epoch_OC} and BCEloss is {epoch_BCE} and MNRloss is {epoch_MNR} and Cosloss is {epoch_Cos}')\n",
        "        # a,b,c = AUC_evaluator.cal(model,train_loader)\n",
        "        # print(f'AUC and Bin_acc after training {epoch+1} on train dataloader is {a,b} and confusion matrix is {c}')\n",
        "        y_true = np.array(y_true).flatten()\n",
        "        y_pred = np.array(y_pred).flatten()\n",
        "        y_pred_sig = np.array(y_pred_sig).flatten()\n",
        "        Bin_score = skm.accuracy_score(y_true,y_pred_sig) \n",
        "        AUC_score = skm.roc_auc_score(y_true,y_pred,max_fpr = 0.05)\n",
        "        conf_matrix = skm.confusion_matrix(y_true,y_pred_sig)\n",
        "        plt.hist(y_pred,bins='auto')\n",
        "        plt.show()\n",
        "        print(f'AUC and Bin_acc after training {epoch+1} on train dataloader is {AUC_score,Bin_score} and confusion matrix is {conf_matrix}')\n",
        "    a,b,c = AUC_evaluator.cal(model,dev_loader)\n",
        "    print(f'AUC and Bin_acc after training {epoch+1} on dev dataloader is {a,b} and the confusion matris is {c}')\n",
        "    torch.save(model.state_dict(), save_dir+'/bert_cos.bin')\n",
        "    print(f'Model is saved after {epoch+1} epochs ')\n",
        "    \n",
        "        "
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AqQUAXTDy7f"
      },
      "source": [
        "# model.load_state_dict(torch.load(folder+'/model.bin'))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qdEeWYIKgbK"
      },
      "source": [
        "# os.makedirs('./model',exist_ok=True)\n",
        "# torch.save(model.state_dict(), '/gdrive/MyDrive/MultiModal/model.bin')"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXsl7QFAQ1UR"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2cb3syg4xdu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd88008c-918e-432e-9b4d-14600d6a2a25"
      },
      "source": [
        "# model.load_state_dict(torch.load(save_dir+'/bert_cos.bin'))"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaOYt_HqWcSO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "outputId": "070dd62b-789d-46ad-9c8c-ab3bd0d9c213",
        "tags": []
      },
      "source": [
        "#Overall parameters in model\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "train_model(train_loader,model,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuGYeCAq97yQ"
      },
      "source": [
        "# Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYlESuc6HmGF"
      },
      "source": [
        "## Binary Classification Evaluator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ-NKaASKXoM"
      },
      "source": [
        "# from . import SentenceEvaluator\n",
        "import logging\n",
        "import os\n",
        "import csv\n",
        "from sklearn.metrics.pairwise import paired_cosine_distances, paired_euclidean_distances, paired_manhattan_distances\n",
        "from sklearn.metrics import average_precision_score\n",
        "import numpy as np\n",
        "from typing import List\n",
        "# from ..readers import InputExample\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWrcz7PMHlaJ"
      },
      "source": [
        "class BinaryClassificationEvaluator():\n",
        "    \"\"\"\n",
        "    Evaluate a model based on the similarity of the embeddings by calculating the accuracy of identifying similar and\n",
        "    dissimilar sentences.\n",
        "    The metrics are the cosine similarity as well as euclidean and Manhattan distance\n",
        "    The returned score is the accuracy with a specified metric.\n",
        "    The results are written in a CSV. If a CSV already exists, then values are appended.\n",
        "    The labels need to be 0 for dissimilar pairs and 1 for similar pairs.\n",
        "    :param sentences1: The first column of sentences\n",
        "    :param sentences2: The second column of sentences\n",
        "    :param labels: labels[i] is the label for the pair (sentences1[i], sentences2[i]). Must be 0 or 1\n",
        "    :param name: Name for the output\n",
        "    :param batch_size: Batch size used to compute embeddings\n",
        "    :param show_progress_bar: If true, prints a progress bar\n",
        "    :param write_csv: Write results to a CSV file\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 dataset,\n",
        "                 name: str = '',\n",
        "                 batch_size: int = 32,\n",
        "                 show_progress_bar: bool = False,\n",
        "                 write_csv: bool = True\n",
        "                 ):\n",
        "        \n",
        "        self.dataset = dataset\n",
        "        self.labels = list()\n",
        "        self.write_csv = write_csv\n",
        "        self.name = name\n",
        "        self.batch_size = batch_size\n",
        "        self.dataloader = DataLoader(\n",
        "            self.dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            pin_memory=True,\n",
        "            num_workers = 8,\n",
        "            shuffle = True\n",
        "        )\n",
        "\n",
        "        if show_progress_bar is None:\n",
        "            show_progress_bar = (logger.getEffectiveLevel() == logging.INFO or logger.getEffectiveLevel() == logging.DEBUG)\n",
        "        self.show_progress_bar = show_progress_bar\n",
        "\n",
        "        self.csv_file = \"binary_classification_evaluation\" + (\"_\"+name if name else '') + \"_results.csv\"\n",
        "        self.csv_headers = [\"epoch\", \"steps\",\n",
        "                            \"cosine_acc\", \"cosine_acc_threshold\", \"cosine_f1\", \"cosine_precision\", \"cosine_recall\", \"cosine_f1_threshold\", \"cosine_average_precision\",\n",
        "                            \"manhatten_acc\", \"manhatten_acc_threshold\", \"manhatten_f1\", \"manhatten_precision\", \"manhatten_recall\", \"manhatten_f1_threshold\", \"manhatten_average_precision\",\n",
        "                            \"eucledian_acc\", \"eucledian_acc_threshold\", \"eucledian_f1\", \"eucledian_precision\", \"eucledian_recall\", \"eucledian_f1_threshold\", \"eucledian_average_precision\"]\n",
        "\n",
        "\n",
        "    # @classmethod\n",
        "    # def from_input_examples(cls, examples: List[InputExample], **kwargs):\n",
        "    #     sentences1 = []\n",
        "    #     sentences2 = []\n",
        "    #     scores = []\n",
        "\n",
        "    #     for example in examples:\n",
        "    #         sentences1.append(example.texts[0])\n",
        "    #         sentences2.append(example.texts[1])\n",
        "    #         scores.append(example.label)\n",
        "    #     return cls(sentences1, sentences2, scores, **kwargs)\n",
        "\n",
        "    def __call__(self, model, output_path: str = None, epoch: int = -1, steps: int = -1) -> float:\n",
        "\n",
        "        if epoch != -1:\n",
        "            if steps == -1:\n",
        "                out_txt = f\" after epoch {epoch}:\"\n",
        "            else:\n",
        "                out_txt = f\" in epoch {epoch} after {steps} steps:\"\n",
        "        else:\n",
        "            out_txt = \":\"\n",
        "\n",
        "        logger.info(\"Binary Accuracy Evaluation of the model on \" + self.name + \" dataset\" + out_txt)\n",
        "        \n",
        "        embeddings1 = list()\n",
        "        embeddings2 = list()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for i, batch in enumerate(self.dataloader):\n",
        "                images = batch[\"image\"]\n",
        "                label = batch[\"label\"]\n",
        "                label = label.float()\n",
        "                token = batch[\"token\"]\n",
        "                \n",
        "                # print(images,label,token)\n",
        "\n",
        "                images[0],images[1] = images[0].to(device),images[1].to(device)\n",
        "                token[0],token[1] = token[0].to(device), token[1].to(device)\n",
        "                label = label.to(device)\n",
        "                \n",
        "                # compute the model output\n",
        "                yhat1 = model(images[0], token[0])\n",
        "                yhat2 = model(images[1],token[1])\n",
        "\n",
        "                for j in yhat1:\n",
        "                    embeddings1.append(j.cpu().detach().numpy())\n",
        "                for j in yhat2:\n",
        "                    embeddings2.append(j.cpu().detach().numpy())\n",
        "                for j in label:\n",
        "                    self.labels.append(float(j))\n",
        "                \n",
        "                if(i%30==0 and i!=0):\n",
        "                    print(f'Completed {i} iterations')\n",
        "\n",
        "        cosine_scores = 1-paired_cosine_distances(embeddings1, embeddings2)\n",
        "        manhattan_distances = paired_manhattan_distances(embeddings1, embeddings2)\n",
        "        euclidean_distances = paired_euclidean_distances(embeddings1, embeddings2)\n",
        "\n",
        "\n",
        "        labels = np.asarray(self.labels)\n",
        "\n",
        "        file_output_data = [epoch, steps]\n",
        "\n",
        "        main_score = None\n",
        "        for name, scores, reverse in [['Cosine-Similarity', cosine_scores, True], ['Manhatten-Distance', manhattan_distances, False], ['Euclidean-Distance', euclidean_distances, False]]:\n",
        "            acc, acc_threshold = self.find_best_acc_and_threshold(scores, labels, reverse)\n",
        "            f1, precision, recall, f1_threshold = self.find_best_f1_and_threshold(scores, labels, reverse)\n",
        "            ap = average_precision_score(labels, scores * (1 if reverse else -1))\n",
        "\n",
        "            logger.info(\"Accuracy with {}:           {:.2f}\\t(Threshold: {:.4f})\".format(name, acc * 100, acc_threshold))\n",
        "            logger.info(\"F1 with {}:                 {:.2f}\\t(Threshold: {:.4f})\".format(name, f1 * 100, f1_threshold))\n",
        "            logger.info(\"Precision with {}:          {:.2f}\".format(name, precision * 100))\n",
        "            logger.info(\"Recall with {}:             {:.2f}\".format(name, recall * 100))\n",
        "            logger.info(\"Average Precision with {}:  {:.2f}\\n\".format(name, ap * 100))\n",
        "\n",
        "            file_output_data.extend([acc, acc_threshold, f1, precision, recall, f1_threshold, ap])\n",
        "\n",
        "            if main_score is None: #Use AveragePrecision with Cosine-Similarity as main score\n",
        "                main_score = ap\n",
        "\n",
        "        if output_path is not None and self.write_csv:\n",
        "            csv_path = os.path.join(output_path, self.csv_file)\n",
        "            if not os.path.isfile(csv_path):\n",
        "                with open(csv_path, mode=\"w\", encoding=\"utf-8\") as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow(self.csv_headers)\n",
        "                    writer.writerow(file_output_data)\n",
        "            else:\n",
        "                with open(csv_path, mode=\"a\", encoding=\"utf-8\") as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow(file_output_data)\n",
        "\n",
        "        return main_score\n",
        "\n",
        "    @staticmethod\n",
        "    def find_best_acc_and_threshold(scores, labels, high_score_more_similar: bool):\n",
        "        # assert len(scores) == len(labels)\n",
        "        rows = list(zip(scores, labels))\n",
        "\n",
        "        rows = sorted(rows, key=lambda x: x[0], reverse=high_score_more_similar)\n",
        "\n",
        "        max_acc = 0\n",
        "        best_threshold = -1\n",
        "\n",
        "        positive_so_far = 0\n",
        "        remaining_negatives = sum(labels == 0)\n",
        "\n",
        "        for i in range(len(rows)-1):\n",
        "            score, label = rows[i]\n",
        "            if label == 1:\n",
        "                positive_so_far += 1\n",
        "            else:\n",
        "                remaining_negatives -= 1\n",
        "\n",
        "            acc = (positive_so_far + remaining_negatives) / len(labels)\n",
        "            if acc > max_acc:\n",
        "                max_acc = acc\n",
        "                best_threshold = (rows[i][0] + rows[i+1][0]) / 2\n",
        "\n",
        "        return max_acc, best_threshold\n",
        "\n",
        "    @staticmethod\n",
        "    def find_best_f1_and_threshold(scores, labels, high_score_more_similar: bool):\n",
        "        # assert len(scores) == len(labels)\n",
        "\n",
        "        scores = np.asarray(scores)\n",
        "        labels = np.asarray(labels)\n",
        "\n",
        "        rows = list(zip(scores, labels))\n",
        "\n",
        "        rows = sorted(rows, key=lambda x: x[0], reverse=high_score_more_similar)\n",
        "\n",
        "        best_f1 = best_precision = best_recall = 0\n",
        "        threshold = 0\n",
        "        nextract = 0\n",
        "        ncorrect = 0\n",
        "        total_num_duplicates = sum(labels)\n",
        "\n",
        "        for i in range(len(rows)-1):\n",
        "            score, label = rows[i]\n",
        "            nextract += 1\n",
        "\n",
        "            if label == 1:\n",
        "                ncorrect += 1\n",
        "\n",
        "            if ncorrect > 0:\n",
        "                precision = ncorrect / nextract\n",
        "                recall = ncorrect / total_num_duplicates\n",
        "                f1 = 2 * precision * recall / (precision + recall)\n",
        "                if f1 > best_f1:\n",
        "                    best_f1 = f1\n",
        "                    best_precision = precision\n",
        "                    best_recall = recall\n",
        "                    threshold = (rows[i][0] + rows[i + 1][0]) / 2\n",
        "\n",
        "        return best_f1, best_precision, best_recall, threshold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSnnK9vk-U_J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "9e5c6720-05cb-4df4-e247-40e73e13d6cc"
      },
      "source": [
        "dev_BCEvaluator = BinaryClassificationEvaluator(dev_dataset,batch_size=BATCH_SIZE,show_progress_bar=True)\n",
        "os.makedirs(folder+'/dev',exist_ok = True)\n",
        "dev_BCEvaluator(model,output_path=folder+'/dev')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdO-HPtiEcIn"
      },
      "source": [
        "train_BCEvaluator = BinaryClassificationEvaluator(train_dataset,batch_size=BATCH_SIZE,show_progress_bar=True)\n",
        "os.makedirs(folder+'/train',exist_ok = True)\n",
        "train_BCEvaluator(model,output_path=folder+'/train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ3f5gO7Hhu0"
      },
      "source": [
        "## Information retreival evaluator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4yW9Gn2DL9i"
      },
      "source": [
        "import torch\n",
        "import logging\n",
        "from tqdm import tqdm, trange\n",
        "import os\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Dict, Set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izMNQie5XZmi"
      },
      "source": [
        "class infodataset(Dataset):\n",
        "    def __init__(self,qr,qr_idx,img_dir,transform = None):\n",
        "        self.qr = qr\n",
        "        self.qr_idx = qr_idx\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def image_adder(self,id1):\n",
        "        img_id1 = list()\n",
        "        if((self.qr.at[id1,'Attachments'])!=None):\n",
        "            for i in self.qr.at[id1,'Attachments']:\n",
        "                try:\n",
        "                    img_path = os.path.join(self.img_dir,i)\n",
        "                    img = Image.open(img_path).convert('RGB')\n",
        "                    if(self.transform):\n",
        "                        img = self.transform(img)\n",
        "                        img.reshape(3,224,224)\n",
        "                    img_id1.append(img)\n",
        "                except Exception as e: \n",
        "                    print(e)\n",
        "        else:\n",
        "            img_id1.append(torch.zeros(3,224,224))\n",
        "\n",
        "        # Work on this, for few examples, it is still saying list index out of range\n",
        "        if(len(img_id1)==0):\n",
        "            # print('No attachments found for id {}'.format(id1))\n",
        "            print(f'Something went wrong with the image of id {id1}')\n",
        "            img_id1.append(torch.zeros(3,224,224))\n",
        "\n",
        "        return img_id1\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        id1 = self.qr_idx[idx]\n",
        "        img_id1 = self.image_adder(id1)\n",
        "\n",
        "        # print(len(img_id1))\n",
        "\n",
        "        # print('Printing id1 {} and len {} and id2 {} and len {} '.format(\n",
        "        #     id1,len(img_id1),\n",
        "        #     id2, len(img_id2)\n",
        "        # ))\n",
        "\n",
        "        # print('Printing id1 shape {} and id2  shape {}'.format(\n",
        "        #     img_id1[0].shape,\n",
        "        #     img_id2[0].shape\n",
        "        # ))        \n",
        "\n",
        "        sample = {\n",
        "            'image': img_id1[0]    #Currently taking only one input image\n",
        "        }\n",
        "\n",
        "        t1 = '[CLS]' + self.qr.loc[id1,'Title'] + ' ' + ' '.join(self.qr.loc[id1,'Tags']) + ' ' + self.qr.loc[id1,'Text'] + '[SEP]'\n",
        "        tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "        t1_token = tokenizer.tokenize(t1)\n",
        "        indexed_t1 = tokenizer.convert_tokens_to_ids(t1_token)\n",
        "        \n",
        "        while(len(indexed_t1)<512):\n",
        "            indexed_t1.append(0)\n",
        "        \n",
        "        ten_t1 = torch.tensor(indexed_t1)[:512]\n",
        "        \n",
        "        try:\n",
        "            sample[\"token\"] = ten_t1 # torch.Size([batch_size, 512])\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "        \n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.qr_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhOlgHW9El3T"
      },
      "source": [
        "class InformationRetreivalEvaluator():\n",
        "    def __init__(self,\n",
        "                 qr,\n",
        "                 queries: Dict[str, str],  #qid => query\n",
        "                 corpus: Dict[str, str],  #cid => doc\n",
        "                 relevant_docs: Dict[str, Set[str]],  #qid => Set[cid]\n",
        "                 corpus_chunk_size: int = 50000,\n",
        "                 mrr_at_k: List[int] = [10],\n",
        "                 ndcg_at_k: List[int] = [10],\n",
        "                 accuracy_at_k: List[int] = [1, 3, 5, 10],\n",
        "                 precision_recall_at_k: List[int] = [1, 3, 5, 10],\n",
        "                 map_at_k: List[int] = [100],\n",
        "                 show_progress_bar: bool = False,\n",
        "                 batch_size: int = 32,\n",
        "                 name: str = '',\n",
        "                 write_csv: bool = True\n",
        "                 ):\n",
        "        \n",
        "        self.qr = qr\n",
        "        self.queries_ids = []\n",
        "        for qid in queries:\n",
        "            if qid in relevant_docs and len(relevant_docs[qid]) > 0:\n",
        "                self.queries_ids.append(qid)\n",
        "\n",
        "        self.queries = [queries[qid] for qid in self.queries_ids]\n",
        "\n",
        "        self.corpus_ids = list(corpus.keys())\n",
        "        self.corpus = [corpus[cid] for cid in self.corpus_ids]\n",
        "\n",
        "        self.relevant_docs = relevant_docs\n",
        "        self.corpus_chunk_size = corpus_chunk_size\n",
        "        self.mrr_at_k = mrr_at_k\n",
        "        self.ndcg_at_k = ndcg_at_k\n",
        "        self.accuracy_at_k = accuracy_at_k\n",
        "        self.precision_recall_at_k = precision_recall_at_k\n",
        "        self.map_at_k = map_at_k\n",
        "\n",
        "        self.show_progress_bar = show_progress_bar\n",
        "        self.batch_size = batch_size\n",
        "        self.name = name\n",
        "        self.write_csv = write_csv\n",
        "\n",
        "        if name:\n",
        "            name = \"_\" + name\n",
        "\n",
        "        self.csv_file: str = \"Information-Retrieval_evaluation\" + name + \"_results.csv\"\n",
        "        self.csv_headers = [\"epoch\", \"steps\"]\n",
        "\n",
        "\n",
        "        for k in accuracy_at_k:\n",
        "            self.csv_headers.append(\"Accuracy@{}\".format(k))\n",
        "\n",
        "        for k in precision_recall_at_k:\n",
        "            self.csv_headers.append(\"Precision@{}\".format(k))\n",
        "            self.csv_headers.append(\"Recall@{}\".format(k))\n",
        "\n",
        "        for k in mrr_at_k:\n",
        "            self.csv_headers.append(\"MRR@{}\".format(k))\n",
        "\n",
        "        for k in ndcg_at_k:\n",
        "            self.csv_headers.append(\"NDCG@{}\".format(k))\n",
        "\n",
        "        for k in map_at_k:\n",
        "            self.csv_headers.append(\"MAP@{}\".format(k))\n",
        "    \n",
        "    def __call__(self,model : BridgeModel,output_path: str = None,epoch: int = -1, steps: int = -1) ->float:\n",
        "        if epoch != -1:\n",
        "            out_txt = \" after epoch {}:\".format(epoch) if steps == -1 else \" in epoch {} after {} steps:\".format(epoch, steps)\n",
        "        else:\n",
        "            out_txt = \":\"\n",
        "\n",
        "        logger.info(\"Information Retrieval Evaluation on \" + self.name + \" dataset\" + out_txt)\n",
        "\n",
        "        max_k = max(max(self.mrr_at_k), max(self.ndcg_at_k), max(self.accuracy_at_k), max(self.precision_recall_at_k), max(self.map_at_k))\n",
        "\n",
        "        query_embeddings = self.get_embeddings(model,self.qr,self.queries_ids)\n",
        "\n",
        "        queries_result_list = [[] for _ in range(len(query_embeddings))]\n",
        "\n",
        "        itr = range(0, len(self.corpus), self.corpus_chunk_size)\n",
        "\n",
        "        if self.show_progress_bar:\n",
        "            itr = tqdm(itr, desc='Corpus Chunks')\n",
        "\n",
        "        #Iterate over chunks of the corpus\n",
        "        for corpus_start_idx in itr:\n",
        "            corpus_end_idx = min(corpus_start_idx + self.corpus_chunk_size, len(self.corpus))\n",
        "\n",
        "            #Encode chunk of corpus\n",
        "            sub_corpus_embeddings = self.get_embeddings(model,self.qr,self.corpus_ids[corpus_start_idx:corpus_end_idx])\n",
        "\n",
        "            #Compute cosine similarites\n",
        "            cos_scores = pytorch_cos_sim(query_embeddings, sub_corpus_embeddings)\n",
        "            del sub_corpus_embeddings\n",
        "\n",
        "            #Get top-k values\n",
        "            cos_scores_top_k_values, cos_scores_top_k_idx = torch.topk(cos_scores, min(max_k, len(cos_scores[0])), dim=1, largest=True, sorted=False)\n",
        "            cos_scores_top_k_values = cos_scores_top_k_values.cpu().tolist()\n",
        "            cos_scores_top_k_idx = cos_scores_top_k_idx.cpu().tolist()\n",
        "            del cos_scores\n",
        "\n",
        "            for query_itr in range(len(query_embeddings)):\n",
        "                for sub_corpus_id, score in zip(cos_scores_top_k_idx[query_itr], cos_scores_top_k_values[query_itr]):\n",
        "                    corpus_id = self.corpus_ids[corpus_start_idx+sub_corpus_id]\n",
        "                    queries_result_list[query_itr].append({'corpus_id': corpus_id, 'score': score})\n",
        "\n",
        "\n",
        "        #Compute scores\n",
        "        scores = self.compute_metrics(queries_result_list)\n",
        "\n",
        "        #Output\n",
        "        self.output_scores(scores)\n",
        "\n",
        "\n",
        "        # logger.info(\"Queries: {}\".format(len(self.queries)))\n",
        "        # logger.info(\"Corpus: {}\\n\".format(len(self.corpus)))\n",
        "\n",
        "        if output_path is not None and self.write_csv:\n",
        "            csv_path = os.path.join(output_path, self.csv_file)\n",
        "            if not os.path.isfile(csv_path):\n",
        "                fOut = open(csv_path, mode=\"w\", encoding=\"utf-8\")\n",
        "                fOut.write(\",\".join(self.csv_headers))\n",
        "                fOut.write(\"\\n\")\n",
        "\n",
        "            else:\n",
        "                fOut = open(csv_path, mode=\"a\", encoding=\"utf-8\")\n",
        "\n",
        "            output_data = [epoch, steps]\n",
        "            for k in self.accuracy_at_k:\n",
        "                output_data.append(scores['accuracy@k'][k])\n",
        "\n",
        "            for k in self.precision_recall_at_k:\n",
        "                output_data.append(scores['precision@k'][k])\n",
        "                output_data.append(scores['recall@k'][k])\n",
        "\n",
        "            for k in self.mrr_at_k:\n",
        "                output_data.append(scores['mrr@k'][k])\n",
        "\n",
        "            for k in self.ndcg_at_k:\n",
        "                output_data.append(scores['ndcg@k'][k])\n",
        "\n",
        "            for k in self.map_at_k:\n",
        "                output_data.append(scores['map@k'][k])\n",
        "\n",
        "            fOut.write(\",\".join(map(str,output_data)))\n",
        "            fOut.write(\"\\n\")\n",
        "            fOut.close()\n",
        "\n",
        "        return scores['map@k'][max(self.map_at_k)]\n",
        "\n",
        "\n",
        "    def compute_metrics(self, queries_result_list: List[object]):\n",
        "        # Init score computation values\n",
        "        num_hits_at_k = {k: 0 for k in self.accuracy_at_k}\n",
        "        precisions_at_k = {k: [] for k in self.precision_recall_at_k}\n",
        "        recall_at_k = {k: [] for k in self.precision_recall_at_k}\n",
        "        MRR = {k: 0 for k in self.mrr_at_k}\n",
        "        ndcg = {k: [] for k in self.ndcg_at_k}\n",
        "        AveP_at_k = {k: [] for k in self.map_at_k}\n",
        "\n",
        "        # Compute scores on results\n",
        "        for query_itr in range(len(queries_result_list)):\n",
        "            query_id = self.queries_ids[query_itr]\n",
        "\n",
        "            # Sort scores\n",
        "            top_hits = sorted(queries_result_list[query_itr], key=lambda x: x['score'], reverse=True)\n",
        "            query_relevant_docs = self.relevant_docs[query_id]\n",
        "\n",
        "            # Accuracy@k - We count the result correct, if at least one relevant doc is accross the top-k documents\n",
        "            for k_val in self.accuracy_at_k:\n",
        "                for hit in top_hits[0:k_val]:\n",
        "                    if hit['corpus_id'] in query_relevant_docs:\n",
        "                        num_hits_at_k[k_val] += 1\n",
        "                        break\n",
        "\n",
        "            # Precision and Recall@k\n",
        "            for k_val in self.precision_recall_at_k:\n",
        "                num_correct = 0\n",
        "                for hit in top_hits[0:k_val]:\n",
        "                    if hit['corpus_id'] in query_relevant_docs:\n",
        "                        num_correct += 1\n",
        "\n",
        "                precisions_at_k[k_val].append(num_correct / k_val)\n",
        "                recall_at_k[k_val].append(num_correct / len(query_relevant_docs))\n",
        "\n",
        "            # MRR@k\n",
        "            for k_val in self.mrr_at_k:\n",
        "                for rank, hit in enumerate(top_hits[0:k_val]):\n",
        "                    if hit['corpus_id'] in query_relevant_docs:\n",
        "                        MRR[k_val] += 1.0 / (rank + 1)\n",
        "                        break\n",
        "\n",
        "            # NDCG@k\n",
        "            for k_val in self.ndcg_at_k:\n",
        "                predicted_relevance = [1 if top_hit['corpus_id'] in query_relevant_docs else 0 for top_hit in top_hits[0:k_val]]\n",
        "                true_relevances = [1] * len(query_relevant_docs)\n",
        "\n",
        "                ndcg_value = self.compute_dcg_at_k(predicted_relevance, k_val) / self.compute_dcg_at_k(true_relevances, k_val)\n",
        "                ndcg[k_val].append(ndcg_value)\n",
        "\n",
        "            # MAP@k\n",
        "            for k_val in self.map_at_k:\n",
        "                num_correct = 0\n",
        "                sum_precisions = 0\n",
        "\n",
        "                for rank, hit in enumerate(top_hits[0:k_val]):\n",
        "                    if hit['corpus_id'] in query_relevant_docs:\n",
        "                        num_correct += 1\n",
        "                        sum_precisions += num_correct / (rank + 1)\n",
        "\n",
        "                avg_precision = sum_precisions / min(k_val, len(query_relevant_docs))\n",
        "                AveP_at_k[k_val].append(avg_precision)\n",
        "\n",
        "        # Compute averages\n",
        "        for k in num_hits_at_k:\n",
        "            num_hits_at_k[k] /= len(self.queries_ids)\n",
        "\n",
        "        for k in precisions_at_k:\n",
        "            precisions_at_k[k] = np.mean(precisions_at_k[k])\n",
        "\n",
        "        for k in recall_at_k:\n",
        "            recall_at_k[k] = np.mean(recall_at_k[k])\n",
        "\n",
        "        for k in ndcg:\n",
        "            ndcg[k] = np.mean(ndcg[k])\n",
        "\n",
        "        for k in MRR:\n",
        "            MRR[k] /= len(self.queries_ids)\n",
        "\n",
        "        for k in AveP_at_k:\n",
        "            AveP_at_k[k] = np.mean(AveP_at_k[k])\n",
        "\n",
        "\n",
        "        return {'accuracy@k': num_hits_at_k, 'precision@k': precisions_at_k, 'recall@k': recall_at_k, 'ndcg@k': ndcg, 'mrr@k': MRR, 'map@k': AveP_at_k}\n",
        "\n",
        "\n",
        "    def output_scores(self, scores):\n",
        "        for k in scores['accuracy@k']:\n",
        "            logger.info(\"Accuracy@{}: {:.2f}%\".format(k, scores['accuracy@k'][k]*100))\n",
        "\n",
        "        for k in scores['precision@k']:\n",
        "            logger.info(\"Precision@{}: {:.2f}%\".format(k, scores['precision@k'][k]*100))\n",
        "\n",
        "        for k in scores['recall@k']:\n",
        "            logger.info(\"Recall@{}: {:.2f}%\".format(k, scores['recall@k'][k]*100))\n",
        "\n",
        "        for k in scores['mrr@k']:\n",
        "            logger.info(\"MRR@{}: {:.4f}\".format(k, scores['mrr@k'][k]))\n",
        "\n",
        "        for k in scores['ndcg@k']:\n",
        "            logger.info(\"NDCG@{}: {:.4f}\".format(k, scores['ndcg@k'][k]))\n",
        "\n",
        "        for k in scores['map@k']:\n",
        "            logger.info(\"MAP@{}: {:.4f}\".format(k, scores['map@k'][k]))\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_dcg_at_k(relevances, k):\n",
        "        dcg = 0\n",
        "        for i in range(min(len(relevances), k)):\n",
        "            dcg += relevances[i] / np.log2(i + 2)  #+2 as we start our idx at 0\n",
        "        return dcg\n",
        "    \n",
        "    def get_embeddings(self,model,qr,qr_idx):\n",
        "        info_dataset = infodataset(qr,qr_idx,img_dir,transform = transform_pipe)\n",
        "        info_loader = DataLoader(\n",
        "            info_dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            pin_memory=True,\n",
        "            num_workers = 8,\n",
        "            )\n",
        "        \n",
        "        embeddings = list()\n",
        "        since = time.time()\n",
        "        for i,batch in enumerate(info_loader):\n",
        "            model.eval()\n",
        "\n",
        "            text = batch['token']\n",
        "            images = batch['image']\n",
        "\n",
        "            text,images = torch.tensor(text).to(device), torch.tensor(images).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                yhat = model.forward(images,text)\n",
        "            \n",
        "            for j in yhat:\n",
        "                embeddings.append(j.cpu().detach().numpy())\n",
        "            \n",
        "            if(i%40==0):\n",
        "                print(f'{i} iterations hase been completed, and model is running for {time.time()-since}')\n",
        "        \n",
        "        return embeddings\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZFzkQT1oKEP"
      },
      "source": [
        "with open(folder_quora+'/devinfo_100.txt','rb') as a:\n",
        "    queries_dev = pickle.load(a)\n",
        "    rel_docs_dev= pickle.load(a)\n",
        "\n",
        "with open(folder_quora+'/testinfo_100.txt','rb') as a:\n",
        "    queries_test= pickle.load(a)\n",
        "    rel_docs_test= pickle.load(a)\n",
        "\n",
        "with open(folder_quora+'/traininfo_100.txt','rb') as a:\n",
        "    queries_train= pickle.load(a)\n",
        "    rel_docs_train= pickle.load(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XTuVNlbeF-5"
      },
      "source": [
        "def corpus(qr):\n",
        "    corpus = dict()\n",
        "    for i in qr.index.values:\n",
        "        corpus[i] = 'I dont care'\n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFwNBTJCoOS4"
      },
      "source": [
        "train_inforet = InformationRetreivalEvaluator(train_qr,queries_train,corpus(train_qr),rel_docs_train)\n",
        "os.makedirs(folder+'/train',exist_ok=True)\n",
        "train_inforet(model,folder+'/train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WktSxgTYEqg1"
      },
      "source": [
        "dev_inforet = InformationRetreivalEvaluator(pd.concat([train_qr,dev_qr]),queries_dev,corpus(train_qr),rel_docs_dev)\n",
        "os.makedirs(folder+'/dev',exist_ok=True)\n",
        "dev_inforet(model,folder+'/dev')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}