{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MultiModal.ipynb","provenance":[],"collapsed_sections":["ZuGYeCAq97yQ","kYlESuc6HmGF"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rXKu5-xjGwOa","executionInfo":{"status":"ok","timestamp":1619794092330,"user_tz":-330,"elapsed":1149,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"8812f697-fbc0-4133-c7e2-dcb973e6d58d"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fri Apr 30 14:48:12 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Mz76Bz490lqB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619794092927,"user_tz":-330,"elapsed":1730,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"d77d61da-2da1-4a3e-bbf8-7f3a65cf106e"},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JHAMYwNfv5Th"},"source":["# Imports"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"_pwgo3m2wVQS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619794095651,"user_tz":-330,"elapsed":4448,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"56b700dd-c2be-495b-c9ae-d005dfd5c8b3"},"source":["!pip install  transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jKIzzX6J3QtS"},"source":["import pandas as pd\n","import pickle\n","import random\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader, Dataset\n","from enum import Enum\n","from torch.nn import functional as F\n","import time\n","import logging\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import sys\n","\n","logger = logging.getLogger(__name__)\n","random.seed(13)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"quJKUGIXv8zk"},"source":["# Utilities"]},{"cell_type":"code","metadata":{"id":"u1t9DLnOsFN_"},"source":["def pytorch_cos_sim(a: torch.Tensor, b: torch.Tensor):\n","    \"\"\"\n","    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n","    This function can be used as a faster replacement for 1-scipy.spatial.distance.cdist(a,b)\n","    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n","    \"\"\"\n","    if not isinstance(a, torch.Tensor):\n","        a = torch.tensor(a)\n","\n","    if not isinstance(b, torch.Tensor):\n","        b = torch.tensor(b)\n","\n","    if len(a.shape) == 1:\n","        a = a.unsqueeze(0)\n","\n","    if len(b.shape) == 1:\n","        b = b.unsqueeze(0)\n","\n","    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n","    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n","    return torch.mm(a_norm, b_norm.transpose(0, 1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DQdj5p8fzl-1"},"source":["def string_sentence(i,qr):\n","    return qr.loc[i,'Title'] + ' '  + ' '.join(qr.loc[i,'Tags']) + ' '  + qr.loc[i,'Text']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"STnKB8R7NVEa"},"source":["# Model Hyper-Parameters"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DebNqaCF2uE_","executionInfo":{"status":"ok","timestamp":1619794095653,"user_tz":-330,"elapsed":4425,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"3a0afc19-1a93-4e43-fd07-bad572de2258"},"source":["folder_quora = '/gdrive/MyDrive/Linked/new_splits'\n","folder = '/gdrive/MyDrive/Linked/Multimodal'\n","img_dir = '/gdrive/MyDrive/MultiModal/data'\n","os.makedirs(folder,exist_ok = True)\n","BATCH_SIZE = 8\n","fin_BATCH_SIZE = 32\n","n_worker = 2\n","margin  = 0.9\n","#---------\n","#Note that amp cannot be used without sigmoid\n","use_amp = True\n","use_sig = True # For BCEloss\n","#----------\n","\n","# eval_BATCH_SIZE = 4 \n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tgk_opqd2prV"},"source":["# Creating Dataset and Dataloader"]},{"cell_type":"markdown","metadata":{"id":"xnOxcgwk5WiU"},"source":["###Loading Data"]},{"cell_type":"code","metadata":{"id":"N1RqPrOk5Ynm"},"source":["with open(folder_quora+'/data/splits/pandas_split.txt','rb') as a:\n","    train_qr=pickle.load(a)\n","    dev_qr = pickle.load(a)\n","    test_qr = pickle.load(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z5qm94QOFH2V"},"source":["with open(folder_quora+'/data/splits/train_dev_test.txt','rb') as a:\n","    train_data=pickle.load(a)\n","    train_score = pickle.load(a)\n","    dev_data = pickle.load(a)\n","    dev_score = pickle.load(a)\n","    test_data = pickle.load(a)\n","    test_score = pickle.load(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oo8Ihxt3lEyK"},"source":["with open(folder_quora+'/data/splits/data_pos_neg.txt','rb') as a:\n","    train_data_pos = pickle.load(a)\n","    train_data_neg = pickle.load(a)\n","    dev_data_pos = pickle.load(a)\n","    dev_data_neg = pickle.load(a)\n","    test_data_pos = pickle.load(a)\n","    test_data_neg = pickle.load(a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M2mT9dqsm5Vy"},"source":["### Creating Dataset "]},{"cell_type":"code","metadata":{"id":"X86sdgPk3lQx"},"source":["from PIL import Image\n","from torchvision import transforms\n","transform_pipe = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E6I9YioKE8uK"},"source":["from transformers import BertTokenizerFast, BertConfig,BertModel\n","config = BertConfig()\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KXPoomQx4Zby"},"source":["class custom_dataset(Dataset):\n","    def __init__(self,qr,qr_idx,img_dir,label,transform = None):\n","        self.qr = qr\n","        self.qr_idx = qr_idx\n","        self.img_dir = img_dir\n","        self.label = label\n","        self.transform = transform\n","\n","    def image_adder(self,id1):\n","        img_id1 = list()\n","        if((self.qr.at[id1,'Attachments'])!=None):\n","            for i in self.qr.at[id1,'Attachments']:\n","                try:\n","                    img_path = os.path.join(self.img_dir,i)\n","                    img = Image.open(img_path).convert('RGB')\n","                    if(self.transform):\n","                        img = self.transform(img)\n","                        img.reshape(3,224,224)\n","                    img_id1.append(img)\n","                except Exception as e: \n","                    print(e)\n","        else:\n","            img_id1.append(torch.randn(3,224,224))\n","\n","        # Work on this, for few examples, it is still saying list index out of range\n","        if(len(img_id1)==0):\n","            # print('No attachments found for id {}'.format(id1))\n","            print(f'Something went wrong with the image of id {id1}')\n","            img_id1.append(torch.randn(3,224,224))\n","\n","        return img_id1\n","    \n","    def __getitem__(self,idx):\n","        id1 = self.qr_idx[idx][0]\n","        id2 = self.qr_idx[idx][1]\n","        img_id1 = self.image_adder(id1)\n","        img_id2 = self.image_adder(id2)\n","\n","        # print(len(img_id1))\n","\n","        # print('Printing id1 {} and len {} and id2 {} and len {} '.format(\n","        #     id1,len(img_id1),\n","        #     id2, len(img_id2)\n","        # ))\n","\n","        # print('Printing id1 shape {} and id2  shape {}'.format(\n","        #     img_id1[0].shape,\n","        #     img_id2[0].shape\n","        # ))        \n","\n","        sample = {\n","            'image': [img_id1[0],img_id2[0]]    #Currently taking only one input image\n","        }\n","\n","        t1 = self.qr.loc[id1,'Title'] + ' ' + ' '.join(self.qr.loc[id1,'Tags']) + ' ' + self.qr.loc[id1,'Text']\n","        t2 = self.qr.loc[id2,'Title'] + ' ' + ' '.join(self.qr.loc[id2,'Tags']) + ' ' + self.qr.loc[id2,'Text']\n","        indexed_t1 = tokenizer.encode(t1,max_length = 512,truncation = True)\n","        indexed_t2 = tokenizer.encode(t2,max_length = 512,truncation = True)\n","\n","        while(len(indexed_t1)<512):\n","            indexed_t1.append(0)\n","        while(len(indexed_t2)<512):\n","            indexed_t2.append(0)\n","\n","        ten_t1 = torch.tensor(indexed_t1)[:512]\n","        ten_t2 = torch.tensor(indexed_t2)[:512]\n","\n","        label_cos = self.label[idx]\n","        if(label_cos==0.0):\n","            label_cos = -1.0\n","\n","\n","        try:\n","            sample[\"label\"] = self.label[idx]\n","            sample[\"token\"] = [ten_t1,ten_t2] # torch.Size([batch_size, 512])\n","            sample[\"label_cos\"] = label_cos\n","        except Exception as e:\n","            print(e)\n","        \n","        return sample\n","\n","    def __len__(self):\n","        return len(self.label)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hV30ckW6NElG"},"source":["## Train, dev and test dataloader"]},{"cell_type":"code","metadata":{"id":"qRNteBxHOomg"},"source":["train_dataset = custom_dataset(train_qr,\n","                               train_data,\n","                               img_dir,\n","                               train_score,\n","                               transform = transform_pipe)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d78ebAQBSdJI"},"source":["train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    pin_memory=True,\n","    num_workers = n_worker,\n","    drop_last = True\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IU4CNMybfgqI"},"source":["dev_dataset = custom_dataset(pd.concat([train_qr,dev_qr]),\n","                             dev_data,\n","                             img_dir,\n","                             dev_score,\n","                             transform = transform_pipe)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AUvKZ6rifxQW"},"source":["dev_loader = DataLoader(\n","    dev_dataset,\n","    batch_size=BATCH_SIZE,\n","    pin_memory=True,\n","    num_workers = n_worker,\n","    drop_last = True\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fe1vYfiifhM2"},"source":["test_dataset = custom_dataset(pd.concat([train_qr,test_qr]),\n","                              test_data,\n","                              img_dir,\n","                              test_score,\n","                              transform = transform_pipe)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RP_N_Kb5f1jD"},"source":["test_loader = DataLoader(\n","    test_dataset,\n","    batch_size=BATCH_SIZE,\n","    pin_memory=True,\n","    num_workers = n_worker,\n","    drop_last = True\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rVzPL-EMTXpM"},"source":["#Model"]},{"cell_type":"markdown","metadata":{"id":"beqWAJVP2_aP"},"source":["## Loading Model "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"Ye9P1XUUToUA"},"source":["from torchvision import models\n","res50 = models.resnet50(pretrained = True)\n","res50 = res50.to(device)\n","for param in res50.parameters():\n","    param.requires_grad = True\n","# res50.eval()\n","\n","bert = BertModel.from_pretrained('bert-base-uncased')\n","bert = bert.to(device)\n","for param in bert.parameters():\n","    param.requires_grad = True\n","# bert.eval()\n","\n","for param in bert.embeddings.parameters():\n","    param.requires_grad = False\n","\n","for param in bert.encoder.layer[0].parameters():\n","    param.requires_grad = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9H9pkv80TYxA"},"source":["def get_activation(name, activation):\n","    def hook(model, input, output):\n","        activation[name] = output.detach()\n","    return hook"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Io4zt1KC27nl"},"source":["## Final 2d attention"]},{"cell_type":"code","metadata":{"id":"T967YR1fTcFo"},"source":["import math\n","class Final2d(torch.nn.Module):\n","    \"\"\" Custom Linear layer for equation (9) in paper --> 2D intra attention layer\"\"\" \n","    def __init__(self, size_in, size_out):\n","        super().__init__()\n","        self.size_in, self.size_out = size_in, size_out\n","        weights = torch.Tensor(size_out, size_in)\n","        self.weights = torch.nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n","\n","        # initialize weights and biases\n","        torch.nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\n","        fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weights)\n","\n","    def forward(self, x, a):\n","        # x.shape = batch_size, 768, 119\n","        # a.shape = batch_size, 119\n","        n = x.shape[2]\n","        h_cap = torch.zeros(x.shape[0], x.shape[1], device = device)\n","        # for k in range(x.shape[0]):\n","        #     for i in range(n):\n","        #         h_cap[k] = h_cap[k] + a[k][i]*torch.matmul(self.weights, x[k, :,i])\n","        z = torch.matmul(self.weights, x)\n","        for k in range(x.shape[0]):\n","            h_cap[k] = torch.sum(a[k, :]*z[k, :, :], dim = 1)\n","        return h_cap"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0B-Z266v25jR"},"source":["## Mask"]},{"cell_type":"code","metadata":{"id":"GvmgGBtPTt3W"},"source":["# n_val = 463+49 #360 is the text input vector length, and 49 is the image vector length\n","# sz = (BATCH_SIZE, n_val*n_val, 1)\n","# mask = torch.zeros(sz, dtype=torch.bool, device = device)\n","# for i in range(n_val):\n","#     mask[:, i*n_val+i] = 1\n","        \n","# for i in range(463-1, n_val):\n","#     for j in range(463-1, n_val):\n","#         mask[:, i*n_val+j] = 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rEHU_G8u2zQ0"},"source":["## Bridge Model"]},{"cell_type":"code","metadata":{"id":"zv3UCSWbUNGO"},"source":["class BridgeModel(torch.nn.Module):\n","    # define model elements\n","    def __init__(self):\n","        super(BridgeModel, self).__init__()\n","        # input to first hidden layer\n","        self.activation = {}\n","        self.res50 = res50.to(device)\n","        self.res50.layer4[2].relu.register_forward_hook(get_activation('conv1', self.activation))\n","        self.res50.avgpool.register_forward_hook(get_activation('conv2', self.activation))\n","        self.bridge_seven_conv = torch.nn.Conv2d(in_channels=2048, out_channels=768, kernel_size=1, stride=1, padding = 0)\n","        self.bridge_one_conv = torch.nn.Conv2d(in_channels=2048, out_channels=768, kernel_size=1, stride=1, padding = 0)\n","        self.flatten = torch.nn.Flatten(start_dim = 2)\n","        self.bert = bert.to(device)\n","        self.mod_v = 463 #Overall text token length\n","        self.linear_2d = torch.nn.Linear(768*2, 1)\n","        self.soft_2d = torch.nn.Softmax(dim = 1)\n","        self.final_2d = Final2d(768, 768) #2D intra attention layer\n","        self.fc = torch.nn.Linear(768+2048, 768)\n","\n","    # forward propagate input\n","    def forward(self, X1, X2):\n","        # input to first hidden layer/\n","        res_out = self.res50(X1)  # torch.Size([batch_size, 1000])\n","        seven_conv = self.activation['conv1'] # torch.Size([batch_size, 2048, 7, 7])\n","        one_conv = self.activation['conv2'] # torch.Size([batch_size, 2048, 1, 1])\n","        out3 = one_conv # torch.Size([batch_size, 2048, 1, 1])\n","        out3 = out3[:, :, 0, 0].clone()\n","        seven_conv = self.bridge_seven_conv(seven_conv)  # torch.Size([batch_size, 768, 7, 7])\n","        one_conv = self.bridge_one_conv(one_conv)  # torch.Size([batch_size, 768, 1, 1])\n","        seven_conv = self.flatten(seven_conv) # torch.Size([batch_size, 768, 49])\n","        one_conv = self.flatten(one_conv) # torch.Size([batch_size, 768, 1])\n","        bridge_cat_temp = torch.cat((one_conv, seven_conv), dim = 2) # torch.Size([1, 768, 50])\n","        bridge_cat = bridge_cat_temp.permute(0, 2, 1) # torch.Size([1, 50, 768])\n","        bert_embed = self.bert.embeddings(input_ids = X2) # torch.Size([1, 512, 768])\n","        bert_embed = bert_embed[:,:self.mod_v,:]\n","        prev = torch.cat((bert_embed, bridge_cat), dim = 1) #562\n","        prev2 = prev[:, self.mod_v:, :]\n","        \n","        for i in range(12):\n","            part1 = self.bert.encoder.layer[i](prev)[0] # torch.Size([1, 562, 768])\n","            part2 = torch.matmul(part1[:, self.mod_v:, :], self.bert.encoder.layer[i].attention.self.value.weight)\n","            part1 = part1[:, :self.mod_v, :] # torch.Size([1, 512, 768])\n","            part1 = torch.cat((part1, part2), dim = 1)\n","            prev = part1\n","            prev2 = prev[:, self.mod_v:, :]\n","\n","        # Mean Pooling\n","        out1 = torch.mean(prev,dim=1) #[BATCH_SIZE,768]\n","            \n","        # out1 = prev[:, 0, :]\n","        # prev = prev[:, 1:, :]\n","        \n","        # n = self.mod_v + 49 # length(n) = |v| + |g| \n","        # hij = torch.zeros(prev.shape[0], n*n, 2*prev.shape[2], device = device) # batch_size, 119*119=14161, 768+768\n","        # # for k in range(prev.shape[0]):\n","        # #    for i in range(n):\n","        # #        for j in range(n):\n","        # #            hij[k][i*n+j] = torch.cat((prev[k][i], prev[k][j]), dim = 0)\n","\n","        # prev_repeat1 =  prev.repeat(1, n, 1) \n","        # prev_repeat2 = torch.repeat_interleave(prev, repeats = n, dim = 1)\n","        # hij = torch.cat((prev_repeat1, prev_repeat2), dim = 2)\n","\n","        # sij = self.linear_2d(hij) # batch_size, 14161, 1\n","        \n","        # # aij = self.soft_2d(sij) # batch_size, 14161, 1\n","        \n","        # sij[mask] = -1000\n","\n","        # aij_tmp = self.soft_2d(sij) # batch_size, 14161, 1\n","\n","        # aij = aij_tmp.reshape(aij_tmp.shape[0], n, n) # batch_size, n, n\n","        \n","        # ai_cap = torch.zeros(prev.shape[0], n, device = device) # batch_size, n\n","        \n","        # ai_cap = (torch.sum(aij, dim = 1) + torch.sum(aij, dim = 2))/2\n","        \n","        # #for k in range(prev.shape[0]):\n","        #  #   for i in range(n):\n","        #   #      for j in range(n):\n","        #    #         ai_cap[k][i] = ai_cap[k][i] + aij[k][i*n+j][0]/2\n","        #     #        ai_cap[k][i] = ai_cap[k][i] + aij[k][j*n+i][0]/2\n","        \n","        # new_hi = prev.permute(0, 2, 1)\n","        # out2 = self.final_2d(new_hi, ai_cap) # torch.Size([batch_size, 768])\n","        \n","        out = torch.cat((out1,out3), dim = 1) # torch.Size([batch_size, 2048+768+768])\n","        \n","        out = self.fc(out) # torch.Size([batch_size, 1])\n","        # yhat = self.sigm(out)\n","        # yhat = yhat[:, 0]\n","        # # print(\"Activation dict len : \" + str(len(self.activation)))\n","        return out\n","\n","    def mask_creation(batch):\n","        n_val = 463+49 #360 is the text input vector length, and 49 is the image vector length\n","        sz = (batch, n_val*n_val, 1)\n","        mask = torch.zeros(sz, dtype=torch.bool, device = device)\n","        for i in range(n_val):\n","            mask[:, i*n_val+i] = 1\n","                \n","        for i in range(463-1, n_val):\n","            for j in range(463-1, n_val):\n","                mask[:, i*n_val+j] = 1\n","        return mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zo9KJIqd7IGb"},"source":["model = BridgeModel().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J2Mw6eG5KqVn"},"source":["# Losses"]},{"cell_type":"markdown","metadata":{"id":"hFtpg_GR4iNJ"},"source":["## Multiple Negative Ranking loss"]},{"cell_type":"code","metadata":{"id":"xMIA3Ydod65P"},"source":["class negrankloss():\n","    def __init__(self,scale: float = 20.0):\n","        # self.cosine_sim = nn.CosineSimilarity()\n","        self.scale = scale\n","        self.cross_entropy = nn.CrossEntropyLoss()\n","\n","    def cal_loss(self,emb1: torch.Tensor,emb2: torch.Tensor, labels: torch.Tensor):\n","        scores  = pytorch_cos_sim(emb1,emb2) *self.scale\n","        # print(f'The scores of cosine similarity for MNRloss is {scores}')\n","        labels = torch.tensor(range(len(scores)), dtype=torch.long, device=scores.device)\n","        loss = self.cross_entropy(scores, labels)\n","        return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tMbsWT-DlN1G"},"source":["MNRloss = negrankloss()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3O5iEjYD7H-s"},"source":["## Online Constrantive Loss\n"]},{"cell_type":"code","metadata":{"id":"pv3V30gQ7LKy"},"source":["class SiameseDistanceMetric(Enum):\n","    \"\"\"\n","    The metric for the contrastive loss\n","    \"\"\"\n","    EUCLIDEAN = lambda x, y: F.pairwise_distance(x, y, p=2)\n","    MANHATTAN = lambda x, y: F.pairwise_distance(x, y, p=1)\n","    COSINE_DISTANCE = lambda x, y: 1-F.cosine_similarity(x, y)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0zSArl7MQ4sO"},"source":["class OnlineConstrantiveLoss():\n","    def __init__(self,distance_metric=SiameseDistanceMetric.COSINE_DISTANCE,margin: float = 0.5):\n","        self.distance_metric = distance_metric\n","        self.margin = margin\n","\n","    def cal_loss(self,emb1,emb2,labels,size_average=False):\n","        distance_matrix = self.distance_metric(emb1,emb2)\n","        # print(f'distance matrix of OCloss is {distance_matrix}')\n","        negs = distance_matrix[labels == 0]\n","        poss = distance_matrix[labels == 1]\n","        # print(f'positive and negatives are {poss} and {negs}')\n","\n","        # select hard positive and hard negative pairs\n","        negative_pairs = negs[negs < (poss.max() if len(poss) > 1 else negs.mean())]\n","        positive_pairs = poss[poss > (negs.min() if len(negs) > 1 else poss.mean())]\n","\n","        negative_pairs = negs\n","        positive_pairs = poss\n","\n","        # print(f'positive and negative pairs are {positive_pairs} and {negative_pairs}')\n","\n","        positive_loss = positive_pairs.pow(2).sum()\n","        negative_loss = F.relu(self.margin - negative_pairs).pow(2).sum()\n","        # print(f'positive loss is {positive_loss} and negative loss is {negative_loss}')\n","        loss = positive_loss + negative_loss\n","        return loss\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cr5sO1wM8NIc"},"source":["OCloss = OnlineConstrantiveLoss(margin = margin)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"76ht5qjQMVsm"},"source":["## Cross Entropy Loss"]},{"cell_type":"code","metadata":{"id":"iSxbp43HMVHn"},"source":["class crossentropy():\n","    def __init__(self):\n","        self.cross_entropy = nn.BCELoss()\n","        self.cos_sim =  nn.CosineSimilarity(dim=1,eps=1e-6)\n","        self.sig = nn.Sigmoid()\n","        self.BCE_with_sig = nn.BCEWithLogitsLoss()\n","    \n","    def cal_loss(self,emb1:torch.Tensor,emb2:torch.Tensor,label:torch.Tensor):\n","        sim = (self.cos_sim(emb1,emb2))\n","        \n","        if(use_sig == True):\n","            sim = (sim+1)/2\n","            loss = self.BCE_with_sig(sim,label)\n","        else:\n","            sim = (sim+1)/2\n","            loss = self.cross_entropy(sim,label)\n","        return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tfdnKk2HUW9X"},"source":["BCEloss = crossentropy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QowUWC1A4F6C"},"source":["## Cosine Similarity Loss"]},{"cell_type":"code","metadata":{"id":"RAiSxopL4Fpz"},"source":["class cos_sim():\n","    def __init__(self):\n","        self.cossim = nn.CosineEmbeddingLoss()\n","\n","    def cal_loss(self,emb1,emb2,target):\n","        loss = self.cossim(emb1,emb2,target)\n","        return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DfuqR7LB4MRm"},"source":["Cosloss = cos_sim()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iaMPRBwZdBjN"},"source":["# Evaluators: AUC(0.05) and Binary accuracy"]},{"cell_type":"code","metadata":{"id":"mAUnsRkLdQLC"},"source":["import sklearn.metrics as skm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N2Z1Yk_TEPj9"},"source":["class AUC():\n","    def __init__(self,max_fpr:float = 0.05):\n","        self.max_fpr = 0.05\n","        self.cos_sim =  nn.CosineSimilarity(dim=1,eps=1e-6)\n","        self.sig = nn.Sigmoid()\n","\n","    def cal(self,model:BridgeModel,loader:DataLoader):\n","        y_pred = []\n","        y_true = []\n","        y_pred_sig = []\n","        start = time.time()\n","        model.eval()\n","        for i,batch in enumerate(loader):\n","            with torch.no_grad():\n","                images = batch[\"image\"]\n","                label = batch[\"label\"]\n","                label = label.float()\n","                token = batch[\"token\"]\n","\n","                images[0],images[1] = images[0].to(device),images[1].to(device)\n","                token[0],token[1] = token[0].to(device), token[1].to(device)\n","                label = label.to(device)\n","            \n","                # compute the model output\n","                yhat1 = model(images[0], token[0])\n","                yhat2 = model(images[1],token[1])\n","\n","                if(i%1000==0):\n","                    print(f'    {i} iterations have been done in {time.time()-start} seconds')\n","\n","                sim = (self.cos_sim(yhat1,yhat2))\n","                a=self.sig(sim)\n","                a[a>0.5] = 1\n","                a[a<0.5] = 0\n","\n","                y_pred_sig.append(a.cpu().numpy())\n","                y_pred.append(sim.cpu().numpy())\n","                y_true.append(label.cpu().numpy())\n","\n","                del  label, token,yhat1,yhat2,sim,a,images \n","        \n","        y_true = np.array(y_true).flatten()\n","        y_pred = np.array(y_pred).flatten()\n","        y_pred_sig = np.array(y_pred_sig).flatten()\n","        Bin_score = skm.accuracy_score(y_true,y_pred_sig) \n","        AUC_score = skm.roc_auc_score(y_true,y_pred,max_fpr = self.max_fpr)\n","        conf_matrix = skm.confusion_matrix(y_true,y_pred_sig)\n","        plt.hist(y_pred,bins='auto')\n","        plt.show()\n","        return AUC_score,Bin_score,conf_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CXrynrKZLn2M"},"source":["AUC_evaluator = AUC()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mwKqXD75KiPI"},"source":["# Calculating loss and executing"]},{"cell_type":"code","metadata":{"id":"bKJz98PeGje6"},"source":["save_dir = folder + '/model_state_dict'\n","os.makedirs(save_dir,exist_ok = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c0yCE0MjkWKT"},"source":["from torch.optim import Adam\n","from torch.nn import BCELoss\n","import time\n","import copy\n","from transformers import get_linear_schedule_with_warmup\n","\n","def train_model(train_loader, model,num_epochs):\n","    model.train()\n","\n","    since = time.time()\n","\n","    optimizer = Adam(model.parameters(), lr=1e-4)\n","    optimizer = Adam([{'params': model.res50.parameters(), 'lr': 4e-6},\n","                      {'params': model.bert.parameters(), 'lr': 4e-6}\n","                      ], lr=4e-4)\n","    train_steps = num_epochs\n","    \n","    best_acc = 0.0\n","\n","    acc_steps = fin_BATCH_SIZE/BATCH_SIZE\n","\n","    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n","    cos_sim = nn.CosineSimilarity(dim=1,eps=1e-6)\n","    sig = nn.Sigmoid()\n","\n","    #Clipping grads\n","    torch.nn.utils.clip_grad_value_(model.parameters(),-1)\n","\n","    # model.eval()\n","    # Evaluating on dev loader\n","    # print('Running dev_evaluator for 2000 samples i.e. 1000 iterations. Time taken: 5 min')\n","    # a,b,c = AUC_evaluator.cal(model,dev_loader)\n","    # print(f'AUC score is {a}  while binary score is {b} and the confusion matrix is {c} on dev loader before training')\n","\n","    # enumerate epochs\n","    for epoch in range(num_epochs):\n","        model.train()\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","        \n","        running_OCloss = 0.0\n","        running_MNRloss = 0.0\n","        running_BCEloss = 0.0\n","        running_Cosloss = 0.0\n","\n","        model.zero_grad()\n","        optimizer.zero_grad()\n","\n","        y_pred = []\n","        y_true = []\n","        y_pred_sig = []\n","\n","        for i, batch in enumerate(train_loader):\n","\n","            with torch.cuda.amp.autocast(enabled=use_amp):\n","                \n","                # if(i%1000 == 0 and i!=0):\n","                #     print('Running dev_evaluator for 2000 samples i.e. 1000 iterations. Time taken: 5 min')\n","                #     a,b,c = AUC_evaluator.cal(model,dev_loader)\n","                #     print(f'AUC score is {a}  while binary score is {b} and the confusion matrix is {c} on dev loader')\n","                #     # torch.save(model.state_dict(), save_dir+'/multimodal_cos.bin')\n","                #     # print(f'Model is saved after {i} iterations ')\n","        \n","                images = batch[\"image\"]\n","                label = batch[\"label\"]\n","                label = label.float()\n","                token = batch[\"token\"]\n","                label_cos = batch['label_cos']\n","                \n","                images[0],images[1] = images[0].to(device),images[1].to(device)\n","                token[0],token[1] = token[0].to(device), token[1].to(device)\n","                label = label.to(device)\n","                label_cos = label_cos.to(device)\n","                \n","                yhat1= model(images[0],token[0])\n","                yhat2 = model(images[1],token[1])\n","\n","                sim = (cos_sim(yhat1,yhat2))\n","                a=sig(sim)\n","                a[a>=0.5] = 1\n","                a[a<0.5] = 0\n","\n","                y_pred_sig.append(a.detach().cpu().numpy())\n","                y_pred.append(sim.detach().cpu().numpy())\n","                y_true.append(label.detach().cpu().numpy())\n","\n","                # yhat1_pos = model(pos_token[0])\n","                # yhat2_pos = model(pos_token[1])\n","                \n","                BCEloss_val = BCEloss.cal_loss(yhat1,yhat2,label)\n","                running_BCEloss += BCEloss_val.item()*BATCH_SIZE\n","                BCEloss_val = BCEloss_val/acc_steps\n","\n","                # MNRloss_val = MNRloss.cal_loss(yhat1_pos,yhat2_pos)\n","                # running_MNRloss += MNRloss_val.item()*BATCH_SIZE\n","                # MNRloss_val = MNRloss_val/acc_steps\n","\n","                OCloss_val = OCloss.cal_loss(yhat1,yhat2,label)\n","                running_OCloss +=  OCloss_val.item()*BATCH_SIZE\n","                OCloss_val = OCloss_val/acc_steps\n","\n","                Cosloss_val = Cosloss.cal_loss(yhat1,yhat2,label_cos)\n","                running_Cosloss += Cosloss_val.item()*BATCH_SIZE\n","                Cosloss_val = Cosloss_val/acc_steps\n","            \n","            scaler.scale(Cosloss_val).backward(retain_graph = True)\n","            scaler.scale(BCEloss_val).backward(retain_graph = True)\n","            # scaler.scale(MNRloss_val).backward(retain_graph=True)\n","            scaler.scale(OCloss_val).backward()\n","\n","            if((i+1)%acc_steps==0):\n","                scaler.step(optimizer)\n","                scaler.update()\n","                model.zero_grad()\n","            \n","            if(i%500 == 0 ):\n","                print('OCloss is {} and BCEloss is {} and MNRloss is {} and Cosloss is {}  and time taken is {} after {} iterations'.format(\n","                    running_OCloss/((i+1)*BATCH_SIZE),\n","                    running_BCEloss/((i+1)*BATCH_SIZE),\n","                    running_MNRloss/((i+1)*BATCH_SIZE),\n","                    running_Cosloss/((i+1)*BATCH_SIZE),\n","                    time.time()-since,\n","                    i))\n","            \n","            del  yhat1,yhat2, label, token, images,Cosloss_val,label_cos,BCEloss_val,OCloss_val\n","\n","        epoch_OC = running_OCloss / len(train_data)\n","        epoch_BCE = running_BCEloss / len(train_data)\n","        epoch_MNR = running_MNRloss / len(train_data)\n","        epoch_Cos = running_Cosloss/len(train_data)\n","        print(f'{epoch+1} Epoch completed. OCloss is {epoch_OC} and BCEloss is {epoch_BCE} and MNRloss is {epoch_MNR} and Cosloss is {epoch_Cos}')\n","        y_true = np.array(y_true).flatten()\n","        y_pred = np.array(y_pred).flatten()\n","        y_pred_sig = np.array(y_pred_sig).flatten()\n","        Bin_score = skm.accuracy_score(y_true,y_pred_sig) \n","        AUC_score = skm.roc_auc_score(y_true,y_pred,max_fpr = 0.05)\n","        conf_matrix = skm.confusion_matrix(y_true,y_pred_sig)\n","        plt.hist(y_pred,bins='auto')\n","        plt.show()\n","        print(f'AUC and Bin_acc after training {epoch+1} on train dataloader is {AUC_score,Bin_score} and confusion matrix is {conf_matrix}')\n","        a,b,c = AUC_evaluator.cal(model,dev_loader)\n","        print(f'AUC and Bin_acc after training {epoch+1} on dev dataloader is {a,b} and the confusion matris is {c}')\n","        torch.save(model.state_dict(), save_dir+'/multimodal_cos.bin')\n","        print(f'Model is saved after {epoch+1} epochs ')\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zXsl7QFAQ1UR"},"source":["# Training the model"]},{"cell_type":"code","metadata":{"id":"LUJqW9Xu5FnX"},"source":["# model.load_state_dict(torch.load(save_dir+'/multimodal_cos.bin'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"GaOYt_HqWcSO","outputId":"214fa072-b710-4e44-bac5-dfb9ddb10abe"},"source":["#Overall parameters in model\n","print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n","train_model(train_loader,model,4)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["110016297\n","Epoch 0/3\n","----------\n","OCloss is 3.207693576812744 and BCEloss is 1.1194026470184326 and MNRloss is 0.0 and Cosloss is 0.6996538043022156  and time taken is 2.140812873840332 after 0 iterations\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569.0\n","OCloss is 1.6922134843891015 and BCEloss is 0.7635013608042589 and MNRloss is 0.0 and Cosloss is 0.4874415889590562  and time taken is 853.1937642097473 after 500 iterations\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["OCloss is 1.6757144437922344 and BCEloss is 0.76636767304027 and MNRloss is 0.0 and Cosloss is 0.489870426359472  and time taken is 1711.1353659629822 after 1000 iterations\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["OCloss is 1.6717075900107046 and BCEloss is 0.7688184743877413 and MNRloss is 0.0 and Cosloss is 0.4914855961796445  and time taken is 2569.2956459522247 after 1500 iterations\n","OCloss is 1.6627555295921814 and BCEloss is 0.7690914628626048 and MNRloss is 0.0 and Cosloss is 0.49146596733657555  and time taken is 3429.5306017398834 after 2000 iterations\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569\n","OCloss is 1.6550191767880174 and BCEloss is 0.7696122419042903 and MNRloss is 0.0 and Cosloss is 0.49086630189480757  and time taken is 4289.5559694767 after 2500 iterations\n","OCloss is 1.640027474340142 and BCEloss is 0.7672288714846306 and MNRloss is 0.0 and Cosloss is 0.4886426098821005  and time taken is 5148.798741102219 after 3000 iterations\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569.0\n","OCloss is 1.619040816077434 and BCEloss is 0.7658802942526882 and MNRloss is 0.0 and Cosloss is 0.4844813345756166  and time taken is 6007.239842891693 after 3500 iterations\n","1 Epoch completed. OCloss is 1.6164446180373908 and BCEloss is 0.7655192398227778 and MNRloss is 0.0 and Cosloss is 0.4839043296867289\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["AUC and Bin_acc after training 1 on train dataloader is (0.5088725885554933, 0.5000351222253442) and confusion matrix is [[    1 14235]\n"," [    0 14236]]\n","    0 iterations have been done in 0.7464830875396729 seconds\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["AUC and Bin_acc after training 1 on dev dataloader is (0.5466134689461752, 0.5) and the confusion matris is [[  0 488]\n"," [  0 488]]\n","Model is saved after 1 epochs \n","Epoch 1/3\n","----------\n","OCloss is 1.9586334228515625 and BCEloss is 1.0671637058258057 and MNRloss is 0.0 and Cosloss is 0.5566268563270569  and time taken is 6201.776463985443 after 0 iterations\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569.0\n","OCloss is 1.3864558930406552 and BCEloss is 0.7429718771975435 and MNRloss is 0.0 and Cosloss is 0.4312719586723579  and time taken is 7060.358925819397 after 500 iterations\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["OCloss is 1.3175836399301781 and BCEloss is 0.7423962096532027 and MNRloss is 0.0 and Cosloss is 0.41311071266780247  and time taken is 7917.852725982666 after 1000 iterations\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["OCloss is 1.290129232305356 and BCEloss is 0.743014099993442 and MNRloss is 0.0 and Cosloss is 0.4049392652106555  and time taken is 8773.70732164383 after 1500 iterations\n","OCloss is 1.2573674988264087 and BCEloss is 0.7414047986730702 and MNRloss is 0.0 and Cosloss is 0.39562813465533286  and time taken is 9628.031178236008 after 2000 iterations\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569\n","OCloss is 1.2322838030210355 and BCEloss is 0.7405577703672902 and MNRloss is 0.0 and Cosloss is 0.3883948149989958  and time taken is 10481.078969717026 after 2500 iterations\n","OCloss is 1.2114412619377604 and BCEloss is 0.7372815177584441 and MNRloss is 0.0 and Cosloss is 0.3818785048144851  and time taken is 11334.095707178116 after 3000 iterations\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569.0\n","OCloss is 1.1889289070098477 and BCEloss is 0.7354645155661789 and MNRloss is 0.0 and Cosloss is 0.37561708898995133  and time taken is 12187.255404949188 after 3500 iterations\n","2 Epoch completed. OCloss is 1.186967879441656 and BCEloss is 0.7350798900809372 and MNRloss is 0.0 and Cosloss is 0.37501899906426006\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["AUC and Bin_acc after training 2 on train dataloader is (0.5705854516381687, 0.5285543692048328) and confusion matrix is [[  861 13375]\n"," [   48 14188]]\n","    0 iterations have been done in 0.36812829971313477 seconds\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["AUC and Bin_acc after training 2 on dev dataloader is (0.5659338198306217, 0.5604508196721312) and the confusion matris is [[ 61 427]\n"," [  2 486]]\n","Model is saved after 2 epochs \n","Epoch 2/3\n","----------\n","OCloss is 1.3859823942184448 and BCEloss is 1.0265365839004517 and MNRloss is 0.0 and Cosloss is 0.4250079095363617  and time taken is 12366.306389093399 after 0 iterations\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569.0\n","OCloss is 1.0235391755750078 and BCEloss is 0.7155620402204776 and MNRloss is 0.0 and Cosloss is 0.3307491547006095  and time taken is 13219.744075775146 after 500 iterations\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"ZuGYeCAq97yQ"},"source":["# Evaluator"]},{"cell_type":"markdown","metadata":{"id":"kYlESuc6HmGF"},"source":["## Binary Classification Evaluator"]},{"cell_type":"code","metadata":{"id":"NJ-NKaASKXoM"},"source":["# from . import SentenceEvaluator\n","import logging\n","import os\n","import csv\n","from sklearn.metrics.pairwise import paired_cosine_distances, paired_euclidean_distances, paired_manhattan_distances\n","from sklearn.metrics import average_precision_score\n","import numpy as np\n","from typing import List\n","# from ..readers import InputExample\n","\n","\n","logger = logging.getLogger(__name__)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWrcz7PMHlaJ"},"source":["class BinaryClassificationEvaluator():\n","    \"\"\"\n","    Evaluate a model based on the similarity of the embeddings by calculating the accuracy of identifying similar and\n","    dissimilar sentences.\n","    The metrics are the cosine similarity as well as euclidean and Manhattan distance\n","    The returned score is the accuracy with a specified metric.\n","    The results are written in a CSV. If a CSV already exists, then values are appended.\n","    The labels need to be 0 for dissimilar pairs and 1 for similar pairs.\n","    :param sentences1: The first column of sentences\n","    :param sentences2: The second column of sentences\n","    :param labels: labels[i] is the label for the pair (sentences1[i], sentences2[i]). Must be 0 or 1\n","    :param name: Name for the output\n","    :param batch_size: Batch size used to compute embeddings\n","    :param show_progress_bar: If true, prints a progress bar\n","    :param write_csv: Write results to a CSV file\n","    \"\"\"\n","\n","    def __init__(self,\n","                 dataset,\n","                 name: str = '',\n","                 batch_size: int = 32,\n","                 show_progress_bar: bool = False,\n","                 write_csv: bool = True\n","                 ):\n","        \n","        self.dataset = dataset\n","        self.labels = list()\n","        self.write_csv = write_csv\n","        self.name = name\n","        self.batch_size = batch_size\n","        self.dataloader = DataLoader(\n","            self.dataset,\n","            batch_size=self.batch_size,\n","            pin_memory=True,\n","            num_workers = 8,\n","            shuffle = True\n","        )\n","\n","        if show_progress_bar is None:\n","            show_progress_bar = (logger.getEffectiveLevel() == logging.INFO or logger.getEffectiveLevel() == logging.DEBUG)\n","        self.show_progress_bar = show_progress_bar\n","\n","        self.csv_file = \"binary_classification_evaluation\" + (\"_\"+name if name else '') + \"_results.csv\"\n","        self.csv_headers = [\"epoch\", \"steps\",\n","                            \"cosine_acc\", \"cosine_acc_threshold\", \"cosine_f1\", \"cosine_precision\", \"cosine_recall\", \"cosine_f1_threshold\", \"cosine_average_precision\",\n","                            \"manhatten_acc\", \"manhatten_acc_threshold\", \"manhatten_f1\", \"manhatten_precision\", \"manhatten_recall\", \"manhatten_f1_threshold\", \"manhatten_average_precision\",\n","                            \"eucledian_acc\", \"eucledian_acc_threshold\", \"eucledian_f1\", \"eucledian_precision\", \"eucledian_recall\", \"eucledian_f1_threshold\", \"eucledian_average_precision\"]\n","\n","\n","    # @classmethod\n","    # def from_input_examples(cls, examples: List[InputExample], **kwargs):\n","    #     sentences1 = []\n","    #     sentences2 = []\n","    #     scores = []\n","\n","    #     for example in examples:\n","    #         sentences1.append(example.texts[0])\n","    #         sentences2.append(example.texts[1])\n","    #         scores.append(example.label)\n","    #     return cls(sentences1, sentences2, scores, **kwargs)\n","\n","    def __call__(self, model, output_path: str = None, epoch: int = -1, steps: int = -1) -> float:\n","\n","        if epoch != -1:\n","            if steps == -1:\n","                out_txt = f\" after epoch {epoch}:\"\n","            else:\n","                out_txt = f\" in epoch {epoch} after {steps} steps:\"\n","        else:\n","            out_txt = \":\"\n","\n","        logger.info(\"Binary Accuracy Evaluation of the model on \" + self.name + \" dataset\" + out_txt)\n","        \n","        embeddings1 = list()\n","        embeddings2 = list()\n","\n","        with torch.no_grad():\n","            model.eval()\n","            for i, batch in enumerate(self.dataloader):\n","                images = batch[\"image\"]\n","                label = batch[\"label\"]\n","                label = label.float()\n","                token = batch[\"token\"]\n","                \n","                # print(images,label,token)\n","\n","                images[0],images[1] = images[0].to(device),images[1].to(device)\n","                token[0],token[1] = token[0].to(device), token[1].to(device)\n","                label = label.to(device)\n","                \n","                # compute the model output\n","                yhat1 = model(images[0], token[0])\n","                yhat2 = model(images[1],token[1])\n","\n","                for j in yhat1:\n","                    embeddings1.append(j.cpu().detach().numpy())\n","                for j in yhat2:\n","                    embeddings2.append(j.cpu().detach().numpy())\n","                for j in label:\n","                    self.labels.append(float(j))\n","                \n","                if(i%30==0 and i!=0):\n","                    print(f'Completed {i} iterations')\n","\n","        cosine_scores = 1-paired_cosine_distances(embeddings1, embeddings2)\n","        manhattan_distances = paired_manhattan_distances(embeddings1, embeddings2)\n","        euclidean_distances = paired_euclidean_distances(embeddings1, embeddings2)\n","\n","\n","        labels = np.asarray(self.labels)\n","\n","        file_output_data = [epoch, steps]\n","\n","        main_score = None\n","        for name, scores, reverse in [['Cosine-Similarity', cosine_scores, True], ['Manhatten-Distance', manhattan_distances, False], ['Euclidean-Distance', euclidean_distances, False]]:\n","            acc, acc_threshold = self.find_best_acc_and_threshold(scores, labels, reverse)\n","            f1, precision, recall, f1_threshold = self.find_best_f1_and_threshold(scores, labels, reverse)\n","            ap = average_precision_score(labels, scores * (1 if reverse else -1))\n","\n","            logger.info(\"Accuracy with {}:           {:.2f}\\t(Threshold: {:.4f})\".format(name, acc * 100, acc_threshold))\n","            logger.info(\"F1 with {}:                 {:.2f}\\t(Threshold: {:.4f})\".format(name, f1 * 100, f1_threshold))\n","            logger.info(\"Precision with {}:          {:.2f}\".format(name, precision * 100))\n","            logger.info(\"Recall with {}:             {:.2f}\".format(name, recall * 100))\n","            logger.info(\"Average Precision with {}:  {:.2f}\\n\".format(name, ap * 100))\n","\n","            file_output_data.extend([acc, acc_threshold, f1, precision, recall, f1_threshold, ap])\n","\n","            if main_score is None: #Use AveragePrecision with Cosine-Similarity as main score\n","                main_score = ap\n","\n","        if output_path is not None and self.write_csv:\n","            csv_path = os.path.join(output_path, self.csv_file)\n","            if not os.path.isfile(csv_path):\n","                with open(csv_path, mode=\"w\", encoding=\"utf-8\") as f:\n","                    writer = csv.writer(f)\n","                    writer.writerow(self.csv_headers)\n","                    writer.writerow(file_output_data)\n","            else:\n","                with open(csv_path, mode=\"a\", encoding=\"utf-8\") as f:\n","                    writer = csv.writer(f)\n","                    writer.writerow(file_output_data)\n","\n","        return main_score\n","\n","    @staticmethod\n","    def find_best_acc_and_threshold(scores, labels, high_score_more_similar: bool):\n","        # assert len(scores) == len(labels)\n","        rows = list(zip(scores, labels))\n","\n","        rows = sorted(rows, key=lambda x: x[0], reverse=high_score_more_similar)\n","\n","        max_acc = 0\n","        best_threshold = -1\n","\n","        positive_so_far = 0\n","        remaining_negatives = sum(labels == 0)\n","\n","        for i in range(len(rows)-1):\n","            score, label = rows[i]\n","            if label == 1:\n","                positive_so_far += 1\n","            else:\n","                remaining_negatives -= 1\n","\n","            acc = (positive_so_far + remaining_negatives) / len(labels)\n","            if acc > max_acc:\n","                max_acc = acc\n","                best_threshold = (rows[i][0] + rows[i+1][0]) / 2\n","\n","        return max_acc, best_threshold\n","\n","    @staticmethod\n","    def find_best_f1_and_threshold(scores, labels, high_score_more_similar: bool):\n","        # assert len(scores) == len(labels)\n","\n","        scores = np.asarray(scores)\n","        labels = np.asarray(labels)\n","\n","        rows = list(zip(scores, labels))\n","\n","        rows = sorted(rows, key=lambda x: x[0], reverse=high_score_more_similar)\n","\n","        best_f1 = best_precision = best_recall = 0\n","        threshold = 0\n","        nextract = 0\n","        ncorrect = 0\n","        total_num_duplicates = sum(labels)\n","\n","        for i in range(len(rows)-1):\n","            score, label = rows[i]\n","            nextract += 1\n","\n","            if label == 1:\n","                ncorrect += 1\n","\n","            if ncorrect > 0:\n","                precision = ncorrect / nextract\n","                recall = ncorrect / total_num_duplicates\n","                f1 = 2 * precision * recall / (precision + recall)\n","                if f1 > best_f1:\n","                    best_f1 = f1\n","                    best_precision = precision\n","                    best_recall = recall\n","                    threshold = (rows[i][0] + rows[i + 1][0]) / 2\n","\n","        return best_f1, best_precision, best_recall, threshold"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iSnnK9vk-U_J"},"source":["dev_BCEvaluator = BinaryClassificationEvaluator(dev_dataset,batch_size=BATCH_SIZE,show_progress_bar=True)\n","os.makedirs(folder+'/dev',exist_ok = True)\n","dev_BCEvaluator(model,output_path=folder+'/dev')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pdO-HPtiEcIn"},"source":["train_BCEvaluator = BinaryClassificationEvaluator(train_dataset,batch_size=BATCH_SIZE,show_progress_bar=True)\n","os.makedirs(folder+'/train',exist_ok = True)\n","train_BCEvaluator(model,output_path=folder+'/train')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MQ3f5gO7Hhu0"},"source":["## Information retreival evaluator"]},{"cell_type":"code","metadata":{"id":"f4yW9Gn2DL9i"},"source":["import torch\n","import logging\n","from tqdm import tqdm, trange\n","import os\n","import numpy as np\n","from typing import List, Tuple, Dict, Set"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"izMNQie5XZmi"},"source":["class infodataset(Dataset):\n","    def __init__(self,qr,qr_idx,img_dir,transform = None):\n","        self.qr = qr\n","        self.qr_idx = qr_idx\n","        self.img_dir = img_dir\n","        self.transform = transform\n","\n","    def image_adder(self,id1):\n","        img_id1 = list()\n","        if((self.qr.at[id1,'Attachments'])!=None):\n","            for i in self.qr.at[id1,'Attachments']:\n","                try:\n","                    img_path = os.path.join(self.img_dir,i)\n","                    img = Image.open(img_path).convert('RGB')\n","                    if(self.transform):\n","                        img = self.transform(img)\n","                        img.reshape(3,224,224)\n","                    img_id1.append(img)\n","                except Exception as e: \n","                    print(e)\n","        else:\n","            img_id1.append(torch.zeros(3,224,224))\n","\n","        # Work on this, for few examples, it is still saying list index out of range\n","        if(len(img_id1)==0):\n","            # print('No attachments found for id {}'.format(id1))\n","            print(f'Something went wrong with the image of id {id1}')\n","            img_id1.append(torch.zeros(3,224,224))\n","\n","        return img_id1\n","    \n","    def __getitem__(self,idx):\n","        id1 = self.qr_idx[idx]\n","        img_id1 = self.image_adder(id1)\n","\n","        # print(len(img_id1))\n","\n","        # print('Printing id1 {} and len {} and id2 {} and len {} '.format(\n","        #     id1,len(img_id1),\n","        #     id2, len(img_id2)\n","        # ))\n","\n","        # print('Printing id1 shape {} and id2  shape {}'.format(\n","        #     img_id1[0].shape,\n","        #     img_id2[0].shape\n","        # ))        \n","\n","        sample = {\n","            'image': img_id1[0]    #Currently taking only one input image\n","        }\n","\n","        t1 = '[CLS]' + self.qr.loc[id1,'Title'] + ' ' + ' '.join(self.qr.loc[id1,'Tags']) + ' ' + self.qr.loc[id1,'Text'] + '[SEP]'\n","        tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","        t1_token = tokenizer.tokenize(t1)\n","        indexed_t1 = tokenizer.convert_tokens_to_ids(t1_token)\n","        \n","        while(len(indexed_t1)<512):\n","            indexed_t1.append(0)\n","        \n","        ten_t1 = torch.tensor(indexed_t1)[:512]\n","        \n","        try:\n","            sample[\"token\"] = ten_t1 # torch.Size([batch_size, 512])\n","        except Exception as e:\n","            print(e)\n","        \n","        return sample\n","\n","    def __len__(self):\n","        return len(self.qr_idx)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qhOlgHW9El3T"},"source":["class InformationRetreivalEvaluator():\n","    def __init__(self,\n","                 qr,\n","                 queries: Dict[str, str],  #qid => query\n","                 corpus: Dict[str, str],  #cid => doc\n","                 relevant_docs: Dict[str, Set[str]],  #qid => Set[cid]\n","                 corpus_chunk_size: int = 50000,\n","                 mrr_at_k: List[int] = [10],\n","                 ndcg_at_k: List[int] = [10],\n","                 accuracy_at_k: List[int] = [1, 3, 5, 10],\n","                 precision_recall_at_k: List[int] = [1, 3, 5, 10],\n","                 map_at_k: List[int] = [100],\n","                 show_progress_bar: bool = False,\n","                 batch_size: int = 32,\n","                 name: str = '',\n","                 write_csv: bool = True\n","                 ):\n","        \n","        self.qr = qr\n","        self.queries_ids = []\n","        for qid in queries:\n","            if qid in relevant_docs and len(relevant_docs[qid]) > 0:\n","                self.queries_ids.append(qid)\n","\n","        self.queries = [queries[qid] for qid in self.queries_ids]\n","\n","        self.corpus_ids = list(corpus.keys())\n","        self.corpus = [corpus[cid] for cid in self.corpus_ids]\n","\n","        self.relevant_docs = relevant_docs\n","        self.corpus_chunk_size = corpus_chunk_size\n","        self.mrr_at_k = mrr_at_k\n","        self.ndcg_at_k = ndcg_at_k\n","        self.accuracy_at_k = accuracy_at_k\n","        self.precision_recall_at_k = precision_recall_at_k\n","        self.map_at_k = map_at_k\n","\n","        self.show_progress_bar = show_progress_bar\n","        self.batch_size = batch_size\n","        self.name = name\n","        self.write_csv = write_csv\n","\n","        if name:\n","            name = \"_\" + name\n","\n","        self.csv_file: str = \"Information-Retrieval_evaluation\" + name + \"_results.csv\"\n","        self.csv_headers = [\"epoch\", \"steps\"]\n","\n","\n","        for k in accuracy_at_k:\n","            self.csv_headers.append(\"Accuracy@{}\".format(k))\n","\n","        for k in precision_recall_at_k:\n","            self.csv_headers.append(\"Precision@{}\".format(k))\n","            self.csv_headers.append(\"Recall@{}\".format(k))\n","\n","        for k in mrr_at_k:\n","            self.csv_headers.append(\"MRR@{}\".format(k))\n","\n","        for k in ndcg_at_k:\n","            self.csv_headers.append(\"NDCG@{}\".format(k))\n","\n","        for k in map_at_k:\n","            self.csv_headers.append(\"MAP@{}\".format(k))\n","    \n","    def __call__(self,model : BridgeModel,output_path: str = None,epoch: int = -1, steps: int = -1) ->float:\n","        if epoch != -1:\n","            out_txt = \" after epoch {}:\".format(epoch) if steps == -1 else \" in epoch {} after {} steps:\".format(epoch, steps)\n","        else:\n","            out_txt = \":\"\n","\n","        logger.info(\"Information Retrieval Evaluation on \" + self.name + \" dataset\" + out_txt)\n","\n","        max_k = max(max(self.mrr_at_k), max(self.ndcg_at_k), max(self.accuracy_at_k), max(self.precision_recall_at_k), max(self.map_at_k))\n","\n","        query_embeddings = self.get_embeddings(model,self.qr,self.queries_ids)\n","\n","        queries_result_list = [[] for _ in range(len(query_embeddings))]\n","\n","        itr = range(0, len(self.corpus), self.corpus_chunk_size)\n","\n","        if self.show_progress_bar:\n","            itr = tqdm(itr, desc='Corpus Chunks')\n","\n","        #Iterate over chunks of the corpus\n","        for corpus_start_idx in itr:\n","            corpus_end_idx = min(corpus_start_idx + self.corpus_chunk_size, len(self.corpus))\n","\n","            #Encode chunk of corpus\n","            sub_corpus_embeddings = self.get_embeddings(model,self.qr,self.corpus_ids[corpus_start_idx:corpus_end_idx])\n","\n","            #Compute cosine similarites\n","            cos_scores = pytorch_cos_sim(query_embeddings, sub_corpus_embeddings)\n","            del sub_corpus_embeddings\n","\n","            #Get top-k values\n","            cos_scores_top_k_values, cos_scores_top_k_idx = torch.topk(cos_scores, min(max_k, len(cos_scores[0])), dim=1, largest=True, sorted=False)\n","            cos_scores_top_k_values = cos_scores_top_k_values.cpu().tolist()\n","            cos_scores_top_k_idx = cos_scores_top_k_idx.cpu().tolist()\n","            del cos_scores\n","\n","            for query_itr in range(len(query_embeddings)):\n","                for sub_corpus_id, score in zip(cos_scores_top_k_idx[query_itr], cos_scores_top_k_values[query_itr]):\n","                    corpus_id = self.corpus_ids[corpus_start_idx+sub_corpus_id]\n","                    queries_result_list[query_itr].append({'corpus_id': corpus_id, 'score': score})\n","\n","\n","        #Compute scores\n","        scores = self.compute_metrics(queries_result_list)\n","\n","        #Output\n","        self.output_scores(scores)\n","\n","\n","        # logger.info(\"Queries: {}\".format(len(self.queries)))\n","        # logger.info(\"Corpus: {}\\n\".format(len(self.corpus)))\n","\n","        if output_path is not None and self.write_csv:\n","            csv_path = os.path.join(output_path, self.csv_file)\n","            if not os.path.isfile(csv_path):\n","                fOut = open(csv_path, mode=\"w\", encoding=\"utf-8\")\n","                fOut.write(\",\".join(self.csv_headers))\n","                fOut.write(\"\\n\")\n","\n","            else:\n","                fOut = open(csv_path, mode=\"a\", encoding=\"utf-8\")\n","\n","            output_data = [epoch, steps]\n","            for k in self.accuracy_at_k:\n","                output_data.append(scores['accuracy@k'][k])\n","\n","            for k in self.precision_recall_at_k:\n","                output_data.append(scores['precision@k'][k])\n","                output_data.append(scores['recall@k'][k])\n","\n","            for k in self.mrr_at_k:\n","                output_data.append(scores['mrr@k'][k])\n","\n","            for k in self.ndcg_at_k:\n","                output_data.append(scores['ndcg@k'][k])\n","\n","            for k in self.map_at_k:\n","                output_data.append(scores['map@k'][k])\n","\n","            fOut.write(\",\".join(map(str,output_data)))\n","            fOut.write(\"\\n\")\n","            fOut.close()\n","\n","        return scores['map@k'][max(self.map_at_k)]\n","\n","\n","    def compute_metrics(self, queries_result_list: List[object]):\n","        # Init score computation values\n","        num_hits_at_k = {k: 0 for k in self.accuracy_at_k}\n","        precisions_at_k = {k: [] for k in self.precision_recall_at_k}\n","        recall_at_k = {k: [] for k in self.precision_recall_at_k}\n","        MRR = {k: 0 for k in self.mrr_at_k}\n","        ndcg = {k: [] for k in self.ndcg_at_k}\n","        AveP_at_k = {k: [] for k in self.map_at_k}\n","\n","        # Compute scores on results\n","        for query_itr in range(len(queries_result_list)):\n","            query_id = self.queries_ids[query_itr]\n","\n","            # Sort scores\n","            top_hits = sorted(queries_result_list[query_itr], key=lambda x: x['score'], reverse=True)\n","            query_relevant_docs = self.relevant_docs[query_id]\n","\n","            # Accuracy@k - We count the result correct, if at least one relevant doc is accross the top-k documents\n","            for k_val in self.accuracy_at_k:\n","                for hit in top_hits[0:k_val]:\n","                    if hit['corpus_id'] in query_relevant_docs:\n","                        num_hits_at_k[k_val] += 1\n","                        break\n","\n","            # Precision and Recall@k\n","            for k_val in self.precision_recall_at_k:\n","                num_correct = 0\n","                for hit in top_hits[0:k_val]:\n","                    if hit['corpus_id'] in query_relevant_docs:\n","                        num_correct += 1\n","\n","                precisions_at_k[k_val].append(num_correct / k_val)\n","                recall_at_k[k_val].append(num_correct / len(query_relevant_docs))\n","\n","            # MRR@k\n","            for k_val in self.mrr_at_k:\n","                for rank, hit in enumerate(top_hits[0:k_val]):\n","                    if hit['corpus_id'] in query_relevant_docs:\n","                        MRR[k_val] += 1.0 / (rank + 1)\n","                        break\n","\n","            # NDCG@k\n","            for k_val in self.ndcg_at_k:\n","                predicted_relevance = [1 if top_hit['corpus_id'] in query_relevant_docs else 0 for top_hit in top_hits[0:k_val]]\n","                true_relevances = [1] * len(query_relevant_docs)\n","\n","                ndcg_value = self.compute_dcg_at_k(predicted_relevance, k_val) / self.compute_dcg_at_k(true_relevances, k_val)\n","                ndcg[k_val].append(ndcg_value)\n","\n","            # MAP@k\n","            for k_val in self.map_at_k:\n","                num_correct = 0\n","                sum_precisions = 0\n","\n","                for rank, hit in enumerate(top_hits[0:k_val]):\n","                    if hit['corpus_id'] in query_relevant_docs:\n","                        num_correct += 1\n","                        sum_precisions += num_correct / (rank + 1)\n","\n","                avg_precision = sum_precisions / min(k_val, len(query_relevant_docs))\n","                AveP_at_k[k_val].append(avg_precision)\n","\n","        # Compute averages\n","        for k in num_hits_at_k:\n","            num_hits_at_k[k] /= len(self.queries_ids)\n","\n","        for k in precisions_at_k:\n","            precisions_at_k[k] = np.mean(precisions_at_k[k])\n","\n","        for k in recall_at_k:\n","            recall_at_k[k] = np.mean(recall_at_k[k])\n","\n","        for k in ndcg:\n","            ndcg[k] = np.mean(ndcg[k])\n","\n","        for k in MRR:\n","            MRR[k] /= len(self.queries_ids)\n","\n","        for k in AveP_at_k:\n","            AveP_at_k[k] = np.mean(AveP_at_k[k])\n","\n","\n","        return {'accuracy@k': num_hits_at_k, 'precision@k': precisions_at_k, 'recall@k': recall_at_k, 'ndcg@k': ndcg, 'mrr@k': MRR, 'map@k': AveP_at_k}\n","\n","\n","    def output_scores(self, scores):\n","        for k in scores['accuracy@k']:\n","            logger.info(\"Accuracy@{}: {:.2f}%\".format(k, scores['accuracy@k'][k]*100))\n","\n","        for k in scores['precision@k']:\n","            logger.info(\"Precision@{}: {:.2f}%\".format(k, scores['precision@k'][k]*100))\n","\n","        for k in scores['recall@k']:\n","            logger.info(\"Recall@{}: {:.2f}%\".format(k, scores['recall@k'][k]*100))\n","\n","        for k in scores['mrr@k']:\n","            logger.info(\"MRR@{}: {:.4f}\".format(k, scores['mrr@k'][k]))\n","\n","        for k in scores['ndcg@k']:\n","            logger.info(\"NDCG@{}: {:.4f}\".format(k, scores['ndcg@k'][k]))\n","\n","        for k in scores['map@k']:\n","            logger.info(\"MAP@{}: {:.4f}\".format(k, scores['map@k'][k]))\n","\n","\n","    @staticmethod\n","    def compute_dcg_at_k(relevances, k):\n","        dcg = 0\n","        for i in range(min(len(relevances), k)):\n","            dcg += relevances[i] / np.log2(i + 2)  #+2 as we start our idx at 0\n","        return dcg\n","    \n","    def get_embeddings(self,model,qr,qr_idx):\n","        info_dataset = infodataset(qr,qr_idx,img_dir,transform = transform_pipe)\n","        info_loader = DataLoader(\n","            info_dataset,\n","            batch_size=BATCH_SIZE,\n","            pin_memory=True,\n","            num_workers = 8,\n","            )\n","        \n","        embeddings = list()\n","        since = time.time()\n","        for i,batch in enumerate(info_loader):\n","            model.eval()\n","\n","            text = batch['token']\n","            images = batch['image']\n","\n","            text,images = torch.tensor(text).to(device), torch.tensor(images).to(device)\n","\n","            with torch.no_grad():\n","                yhat = model.forward(images,text)\n","            \n","            for j in yhat:\n","                embeddings.append(j.cpu().detach().numpy())\n","            \n","            if(i%40==0):\n","                print(f'{i} iterations hase been completed, and model is running for {time.time()-since}')\n","        \n","        return embeddings\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pZFzkQT1oKEP"},"source":["with open(folder_quora+'/devinfo_100.txt','rb') as a:\n","    queries_dev = pickle.load(a)\n","    rel_docs_dev= pickle.load(a)\n","\n","with open(folder_quora+'/testinfo_100.txt','rb') as a:\n","    queries_test= pickle.load(a)\n","    rel_docs_test= pickle.load(a)\n","\n","with open(folder_quora+'/traininfo_100.txt','rb') as a:\n","    queries_train= pickle.load(a)\n","    rel_docs_train= pickle.load(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5XTuVNlbeF-5"},"source":["def corpus(qr):\n","    corpus = dict()\n","    for i in qr.index.values:\n","        corpus[i] = 'I dont care'\n","    return corpus"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zFwNBTJCoOS4"},"source":["train_inforet = InformationRetreivalEvaluator(train_qr,queries_train,corpus(train_qr),rel_docs_train)\n","os.makedirs(folder+'/train',exist_ok=True)\n","train_inforet(model,folder+'/train')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WktSxgTYEqg1"},"source":["dev_inforet = InformationRetreivalEvaluator(pd.concat([train_qr,dev_qr]),queries_dev,corpus(train_qr),rel_docs_dev)\n","os.makedirs(folder+'/dev',exist_ok=True)\n","dev_inforet(model,folder+'/dev')"],"execution_count":null,"outputs":[]}]}