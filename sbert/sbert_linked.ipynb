{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Linked_Model.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyN7qdj5IxDjnGmb4cpuUcaV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9d43d658674d43f2aa95a2c6f208bbcc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a86da5ca9d1442dcb3c0e9f9eee93d3a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_929f81b6033c42039948f0ac18aa6426","IPY_MODEL_fc0e0df9b268408fb67ccf60dc6b0773"]}},"a86da5ca9d1442dcb3c0e9f9eee93d3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"929f81b6033c42039948f0ac18aa6426":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_00f37eaf9700413c8657b811fdc47c2a","_dom_classes":[],"description":"Epoch:   0%","_model_name":"FloatProgressModel","bar_style":"danger","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ab19e78a968c4bebb7df9bc946f3fbe3"}},"fc0e0df9b268408fb67ccf60dc6b0773":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_52e617fd40a5476d957f74e0c3fad71f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/1 [00:06&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_adc3b2913cda4ac88cdd209e32886409"}},"00f37eaf9700413c8657b811fdc47c2a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ab19e78a968c4bebb7df9bc946f3fbe3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"52e617fd40a5476d957f74e0c3fad71f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"adc3b2913cda4ac88cdd209e32886409":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"be7dc3957c824fc2b900ae8715d6a72c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_99978bb0c6e14329ac636cfb5effa86a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e473006f475f449f95cbfd4cacbcf79d","IPY_MODEL_3d0ac671da164e1b88d778758f14aed5"]}},"99978bb0c6e14329ac636cfb5effa86a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e473006f475f449f95cbfd4cacbcf79d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_48735982e8c343ccba4394810bfcc929","_dom_classes":[],"description":"Iteration:   0%","_model_name":"FloatProgressModel","bar_style":"danger","max":1714,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4370c12705964d46bd69d67af5b3e02a"}},"3d0ac671da164e1b88d778758f14aed5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_53adbd195154497fa8f53162653b5477","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1714 [00:06&lt;3:16:27,  6.88s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_96d75c0ed11941ef8c84de8f0fa219b1"}},"48735982e8c343ccba4394810bfcc929":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4370c12705964d46bd69d67af5b3e02a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"53adbd195154497fa8f53162653b5477":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"96d75c0ed11941ef8c84de8f0fa219b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"QLrpqRlEyjiY"},"source":["##Creating Positive and negative pairs for cross encoders. It contains both (a,b) and (b,a)\r\n","\r\n","Structure: Title+Text+Body"]},{"cell_type":"code","metadata":{"id":"W2Sl69oGyv_6"},"source":["import pandas as pd\r\n","import json\r\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-_hw5gEA2B0r","executionInfo":{"status":"ok","timestamp":1610442397379,"user_tz":-330,"elapsed":21432,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"a58d12ab-1460-4138-c85c-0d35b50fe33f"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AKpkLcc42Man"},"source":["with open('/content/drive/MyDrive/sbert_supervised/android_l_nosw.json','r') as a:\r\n","    q = pd.read_json(a,orient='table')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZwRe58L_2gVH","executionInfo":{"status":"ok","timestamp":1610442561000,"user_tz":-330,"elapsed":1529,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"d55dc183-e4ba-479d-851b-c79c4c33cd97"},"source":["q.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Float64Index: 8797 entries, 31.0 to 218179.0\n","Data columns (total 12 columns):\n"," #   Column            Non-Null Count  Dtype  \n","---  ------            --------------  -----  \n"," 0   AcceptedAnswerId  3528 non-null   float64\n"," 1   AnswerCount       8797 non-null   float64\n"," 2   AnswerIds         3053 non-null   object \n"," 3   Attachments       897 non-null    object \n"," 4   OS                6642 non-null   object \n"," 5   PostTypeId        8797 non-null   float64\n"," 6   Related           3053 non-null   object \n"," 7   Score             8797 non-null   float64\n"," 8   Tags              8797 non-null   object \n"," 9   Text              8797 non-null   object \n"," 10  Title             8797 non-null   object \n"," 11  Linked            8797 non-null   object \n","dtypes: float64(4), object(8)\n","memory usage: 893.4+ KB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"juQ6rlSv2i7p","executionInfo":{"status":"ok","timestamp":1610443650877,"user_tz":-330,"elapsed":1828,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"76844dfc-37d2-4c28-c265-76979ef7aa23"},"source":["data = list()\r\n","score = list()\r\n","pairs = list()\r\n","for i in q.index.values:\r\n","    t1 = q.loc[i,'Title'] + q.loc[i,'Tags'] + q.loc[i,'Text']\r\n","    for j in list(q.loc[i,'Linked']):\r\n","        t2 = q.loc[j,'Title'] + q.loc[j,'Tags'] + q.loc[j,'Text']\r\n","        data.append([t1,t2])\r\n","        score.append(1)\r\n","        pairs.append([i,j])\r\n","\r\n","print(len(data))\r\n","print(len(score))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["15244\n","15244\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hee6-8fD3eCB","executionInfo":{"status":"ok","timestamp":1610443665842,"user_tz":-330,"elapsed":16389,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"d7ee21f4-bee5-4d81-b6bf-479040a8a32e"},"source":["p_neg = list()\r\n","for i in range(0,15224):\r\n","    id1 = random.choice(q.index.values)\r\n","    id2 = random.choice(q.index.values)\r\n","    while([id1,id2] in pairs or [id1,id2] in p_neg or id1==id2):\r\n","        id1 = random.choice(q.index.values)\r\n","        id2 = random.choice(q.index.values)\r\n","    t1 = q.loc[id1,'Title'] + q.loc[id1,'Tags'] + q.loc[id1,'Text']\r\n","    t2 = q.loc[id2,'Title'] + q.loc[id2,'Tags'] + q.loc[id2,'Text']\r\n","    data.append([t1,t2])\r\n","    score.append(0)\r\n","    p_neg.append([id1,id2])\r\n","print(len(data))\r\n","print(len(score))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["30468\n","30468\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0MPMjuoH6_F2"},"source":["import pickle\r\n","with open('/content/drive/MyDrive/Linked/data_android_l.txt','wb') as a:\r\n","    pickle.dump(data,a)\r\n","    pickle.dump(score,a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ViaFdx18B4Cw"},"source":["#Modeling Cross Encoder"]},{"cell_type":"code","metadata":{"id":"hhcmArV_CL-R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610471928564,"user_tz":-330,"elapsed":21537,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"977456ff-26d6-4112-ea36-ceabf5459769"},"source":["import pandas as pd\r\n","import json\r\n","import pickle\r\n","\r\n","from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yN3w3bHhB7jD"},"source":["with open('/content/drive/MyDrive/Linked/data_android_l.txt','rb') as a:\r\n","    data = pickle.load(a)\r\n","    score = pickle.load(a)\r\n","print(data[0][0])\r\n","print(type(score[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hx9iiTJUC3PZ"},"source":["##Training Cross Encoder"]},{"cell_type":"code","metadata":{"id":"VPr0Y5xKCtrK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610471939791,"user_tz":-330,"elapsed":11223,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"670eba34-e093-42b1-9724-3d8ef13964c4"},"source":["!pip install sentence-transformers"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting sentence-transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/e2/84d6acfcee2d83164149778a33b6bdd1a74e1bcb59b2b2cd1b861359b339/sentence-transformers-0.4.1.2.tar.gz (64kB)\n","\u001b[K     |████████████████████████████████| 71kB 3.5MB/s \n","\u001b[?25hCollecting transformers<5.0.0,>=3.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n","\u001b[K     |████████████████████████████████| 1.5MB 7.6MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.7.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.19.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 49.8MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.8)\n","Collecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 53.7MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.8)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 47.7MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.16.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (1.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n","Building wheels for collected packages: sentence-transformers, sacremoses\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-0.4.1.2-cp36-none-any.whl size=103068 sha256=24140979762cfc42046ff06c26934c6dd0477b4ab30960f377a6a2517672f445\n","  Stored in directory: /root/.cache/pip/wheels/3d/33/d1/5703dd56199c09d4a1b41e0c07fb4e7765a84d787cbdc48ac3\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=f447d7b11a77e7a61647ccd3251f358a672e40c344b191b1cc8f8262837c455c\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sentence-transformers sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers, sentencepiece, sentence-transformers\n","Successfully installed sacremoses-0.0.43 sentence-transformers-0.4.1.2 sentencepiece-0.1.95 tokenizers-0.9.4 transformers-4.1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t140DKhqC8Nm","executionInfo":{"status":"ok","timestamp":1610471945962,"user_tz":-330,"elapsed":17388,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}}},"source":["from sentence_transformers import models, losses, util, SentenceTransformer, SentencesDataset, InputExample, evaluation\r\n","from sentence_transformers.cross_encoder import CrossEncoder\r\n","from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator as ceb\r\n","\r\n","from torch import nn\r\n","from torch.utils.data import DataLoader\r\n","import torch\r\n","\r\n","from sklearn.model_selection import train_test_split\r\n","from sklearn.utils import shuffle"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZaEtL3hIDaPs","executionInfo":{"status":"ok","timestamp":1610470922272,"user_tz":-330,"elapsed":7275,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}}},"source":["data,score = shuffle(data,score)\r\n","train_data,dev_data,train_score,dev_score = train_test_split(data,score,test_size=0.1,random_state=13)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"DsEQ4p8GD2-9","executionInfo":{"status":"ok","timestamp":1610470939454,"user_tz":-330,"elapsed":23013,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}}},"source":["model = CrossEncoder('bert-base-uncased',num_labels=1)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lj2GMNRHEpbH","executionInfo":{"status":"ok","timestamp":1610470939457,"user_tz":-330,"elapsed":18176,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}}},"source":["train_examples = list()\r\n","dev_examples = list()\r\n","for i in range(0,len(train_data)):\r\n","    train_examples.append(InputExample(texts=[train_data[i][0],train_data[i][1]], label=train_score[i]))\r\n","\r\n","for i in range(0,len(dev_data)):\r\n","    dev_examples.append(InputExample(texts=[dev_data[i][0],dev_data[i][1]], label=dev_score[i]))"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"AgPRcGvOFISd","executionInfo":{"status":"ok","timestamp":1610472765751,"user_tz":-330,"elapsed":819,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}}},"source":["batch_size=8\r\n","num_epochs = 6\r\n","model_path = '/content/drive/MyDrive/Linked'"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"CHJ4IS6vHT0_","executionInfo":{"status":"ok","timestamp":1610471446461,"user_tz":-330,"elapsed":2107,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}}},"source":["train_dataset = SentencesDataset(train_examples, model)\r\n","train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jw6gg03lHUNW","executionInfo":{"status":"ok","timestamp":1610471446462,"user_tz":-330,"elapsed":1758,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"7a1ce8ad-91b5-45e1-a9c5-4b5910281998"},"source":["print(torch.cuda.is_available())"],"execution_count":22,"outputs":[{"output_type":"stream","text":["True\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u--_DzEGH6x9","executionInfo":{"status":"ok","timestamp":1610471486261,"user_tz":-330,"elapsed":1662,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}}},"source":["evaluator_dev = ceb.from_input_examples(dev_examples,name='android-dev')"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"ghzUUynHKdpz","executionInfo":{"status":"ok","timestamp":1610471489927,"user_tz":-330,"elapsed":1445,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}}},"source":["import math\r\n","warmup_steps = math.ceil(len(train_dataset) * num_epochs / batch_size * 0.1) #10% of train data for warm-up"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"LKKKRd-BKgra","executionInfo":{"status":"ok","timestamp":1610471489928,"user_tz":-330,"elapsed":1152,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}}},"source":["x = list()\r\n","y = list()\r\n","def callback_model(score,epoch,steps):\r\n","    if(steps==-1):\r\n","        steps=0\r\n","        epoch+=1\r\n","    x.append(float(epoch + steps/3428))\r\n","    y.append(float(score))\r\n","    print('Score after epoch ', str(epoch),' and steps ', str(steps),'is ',str(score))"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":496,"referenced_widgets":["9d43d658674d43f2aa95a2c6f208bbcc","a86da5ca9d1442dcb3c0e9f9eee93d3a","929f81b6033c42039948f0ac18aa6426","fc0e0df9b268408fb67ccf60dc6b0773","00f37eaf9700413c8657b811fdc47c2a","ab19e78a968c4bebb7df9bc946f3fbe3","52e617fd40a5476d957f74e0c3fad71f","adc3b2913cda4ac88cdd209e32886409","be7dc3957c824fc2b900ae8715d6a72c","99978bb0c6e14329ac636cfb5effa86a","e473006f475f449f95cbfd4cacbcf79d","3d0ac671da164e1b88d778758f14aed5","48735982e8c343ccba4394810bfcc929","4370c12705964d46bd69d67af5b3e02a","53adbd195154497fa8f53162653b5477","96d75c0ed11941ef8c84de8f0fa219b1"]},"id":"uQz5vvpOKoXg","executionInfo":{"status":"error","timestamp":1610471512660,"user_tz":-330,"elapsed":8012,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"8afbab1c-7a2c-4f88-abcd-fc3b56a2c3d9"},"source":["model.fit(train_dataloader=train_dataloader,\r\n","          evaluator=evaluator_dev,\r\n","          epochs=num_epochs,\r\n","          evaluation_steps=1200,\r\n","          warmup_steps = warmup_steps,\r\n","          output_path=model_path,\r\n","          use_amp = True,\r\n","          callback = callback_model\r\n","          )"],"execution_count":29,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d43d658674d43f2aa95a2c6f208bbcc","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be7dc3957c824fc2b900ae8715d6a72c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1714.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-a821bf09efc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0muse_amp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m           \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m           )\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_dataloader, evaluator, epochs, loss_fct, acitvation_fct, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_amp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                         \u001b[0mmodel_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m                         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macitvation_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m         )\n\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m         )\n\u001b[1;32m    874\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    505\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m                 )\n\u001b[1;32m    509\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         )\n\u001b[1;32m    428\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         )\n\u001b[1;32m    365\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;31m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# Mask heads if we want to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    981\u001b[0m     return (_VF.dropout_(input, p, training)\n\u001b[1;32m    982\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m             else _VF.dropout(input, p, training))\n\u001b[0m\u001b[1;32m    984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 11.17 GiB total capacity; 10.45 GiB already allocated; 44.81 MiB free; 10.68 GiB reserved in total by PyTorch)"]}]},{"cell_type":"code","metadata":{"id":"tE8bSfWSLBrQ"},"source":["%matplotlib inline\r\n","from matplotlib import pyplot as plt\r\n","plt.plot(x,y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bECUuzTcLNSi"},"source":["with open('/content/drive/MyDrive/Linked/Accuracy_score.txt','wb') as f:\r\n","    pickle.dump(x,f)\r\n","    pickle.dump(y,f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d7UWhMk_LTrB"},"source":["plt.savefig('/content/drive/MyDrive/Linked/final_accuarcy.png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nxPwzwPLl2_z"},"source":["#Labelling android dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aoVehHa7m9sN","executionInfo":{"status":"ok","timestamp":1610472802685,"user_tz":-330,"elapsed":17015,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"04e707c3-eef7-45ee-c05d-c67e458d6293"},"source":["ce_model = CrossEncoder('bert-base-uncased',max_length=512)\r\n","ce_model_m = CrossEncoder(model_path)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Lch_qGXiLdNc"},"source":["##Positive Pair Generation "]},{"cell_type":"code","metadata":{"id":"qIgZNqW75M-x","executionInfo":{"status":"ok","timestamp":1610472291638,"user_tz":-330,"elapsed":1466,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}}},"source":["with open('/content/drive/MyDrive/sbert_supervised/android_nosw.json','r') as f:\r\n","    qr = pd.read_json(f,orient='table')"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"U1Zp6nm-mjuH","executionInfo":{"status":"ok","timestamp":1610472291639,"user_tz":-330,"elapsed":1195,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}}},"source":["def lts(s):  \r\n","    \r\n","    # initialize an empty string \r\n","    str1 = \"\"  \r\n","    \r\n","    # traverse in the string   \r\n","    for ele in s:  \r\n","        str1 += str(ele) + ' '  \r\n","    \r\n","    # return string   \r\n","    return str1  "],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"WkO3ecevq-NZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610472291640,"user_tz":-330,"elapsed":909,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"14a2efbc-b40f-4c6f-a375-4154ae54b5a9"},"source":["qr.info()"],"execution_count":20,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 17132 entries, 31 to 219026\n","Data columns (total 11 columns):\n"," #   Column            Non-Null Count  Dtype  \n","---  ------            --------------  -----  \n"," 0   AcceptedAnswerId  6631 non-null   float64\n"," 1   AnswerCount       17132 non-null  float64\n"," 2   AnswerIds         17132 non-null  object \n"," 3   Attachments       4246 non-null   object \n"," 4   OS                4246 non-null   object \n"," 5   PostTypeId        17132 non-null  int64  \n"," 6   Related           17132 non-null  object \n"," 7   Score             17132 non-null  int64  \n"," 8   Tags              17132 non-null  object \n"," 9   Text              17132 non-null  object \n"," 10  Title             17132 non-null  object \n","dtypes: float64(2), int64(2), object(7)\n","memory usage: 1.6+ MB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HQFm2Bd9G9eM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610473327131,"user_tz":-330,"elapsed":857,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"0233128a-b2a6-49ff-e0fd-e524912f6844"},"source":["data_pos = list()\r\n","cnt = 0\r\n","num_pairs = 100\r\n","related_not_found = 0\r\n","al_pos = list()\r\n","for i in qr.index.values:\r\n","    if(cnt>=num_pairs):\r\n","        break\r\n","    if(len(qr.loc[i,'Related'])!=0):\r\n","        for j in qr.loc[i,'Related']:\r\n","            if( ( [j,i] in al_pos or [i,j] in al_pos) != 1 ):\r\n","                # try:\r\n","                t1 = qr.loc[i,'Title'] + lts(qr.loc[i,'Tags']) + qr.loc[i,'Text']\r\n","                t2 = qr.loc[j,'Title'] + lts(qr.loc[j,'Tags']) + qr.loc[j,'Text']\r\n","                cnt+=1\r\n","                data_pos.append([t1,t2])\r\n","                al_pos.append([ i , j ])\r\n","                    # break\r\n","                # except:\r\n","                #     related_not_found+=1\r\n","                #     # print('Related not found,parent id: ',i,' id:',j,'overall count: ',related_not_found)\r\n","print(related_not_found)\r\n","print(len(data_pos))"],"execution_count":35,"outputs":[{"output_type":"stream","text":["0\n","106\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pQpD17pAmZrh","executionInfo":{"status":"ok","timestamp":1610473715615,"user_tz":-330,"elapsed":331505,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"aa9076c8-e111-4bcc-a734-7e39427594d4"},"source":["acc_bert = ce_model.predict(data_pos)\r\n","print(sum(acc_bert)/106)\r\n","acc_m = ce_model_m.predict(data_pos)\r\n","print(sum(acc_m)/106)"],"execution_count":36,"outputs":[{"output_type":"stream","text":["0.5723572485851791\n","0.8591707203916674\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fnZq_dDyqq42","executionInfo":{"status":"ok","timestamp":1610473715616,"user_tz":-330,"elapsed":330844,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"807d7e99-429d-4389-cc39-6d2dfdc001ae"},"source":["import random\r\n","num_pairs = len(data_pos)\r\n","data_neg = list()\r\n","al_acc = list()\r\n","for i in range(0,num_pairs):\r\n","    id1 = random.choice(qr.index.values)\r\n","    id2 = random.choice(qr.index.values)\r\n","    while(id2 in qr.loc[id1,'Related'] or id1 in qr.loc[id2,'Related'] or [id1,id2] in al_acc or [id2,id1] in al_acc):\r\n","         id1 = random.choice(qr.index.values)\r\n","         id2 = random.choice(qr.index.values)\r\n","    t1 = qr.loc[id1,'Title'] + lts(qr.loc[id1,'Tags']) + qr.loc[id1,'Text']\r\n","    t2 = qr.loc[id2,'Title'] + lts(qr.loc[id2,'Tags']) + qr.loc[id2,'Text']\r\n","    data_neg.append([t1,t2])\r\n","    al_acc.append([id1,id2])\r\n","print(len(data_neg))"],"execution_count":37,"outputs":[{"output_type":"stream","text":["106\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_gFACQTq9G5","executionInfo":{"status":"ok","timestamp":1610474101086,"user_tz":-330,"elapsed":715683,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"f32195bb-008a-4cba-adc1-3f22ebbc652a"},"source":["acc_bert_n = ce_model.predict(data_neg)\r\n","print(sum(acc_bert_n)/106)\r\n","acc_m_n = ce_model_m.predict(data_neg)\r\n","print(sum(acc_m_n)/106)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["0.5840784496856186\n","0.06935349212859741\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7vnRRAeBvfKY","executionInfo":{"status":"ok","timestamp":1610475054622,"user_tz":-330,"elapsed":1059,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"593011a6-bcd7-4c51-933a-87d664a1fa01"},"source":["print((acc_m_n>0.9))"],"execution_count":73,"outputs":[{"output_type":"stream","text":["[False False False False False False  True False False False False  True\n"," False False False False False False False False False False False False\n"," False False  True False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False  True False False False False False False False False False  True\n"," False False False False False False False False  True False  True False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VDMsl0gNy1JN","executionInfo":{"status":"ok","timestamp":1610475130813,"user_tz":-330,"elapsed":1025,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"c314fed4-b90b-40c4-f448-db703bd53a12"},"source":["print(data_neg[7][0] + '\\n\\n' + data_neg[7][1]) "],"execution_count":76,"outputs":[{"output_type":"stream","text":["number pattern matching attempt android 2.3(maxx msd7 ) factory-reset start phone(maxx msd7),i get \" number invalid pattern matching attempt \" error android 2.3.i try perform factory reset starup . get recovery menu startup press volume - button not get factory reset option . test item test clear flash option . . ? \n","\n","create profile disable profile tasker couple profile define tasker like enable travel abroad . tasker not recognize state \" travel abroad \" , like set state manually disable profile relate meta profile \" travel abroad \" . envision tasker : category : travel abroad ( switch / ) profile 1 ( switch / ) profile 2 ( switch / ) profile 3 ( switch / ) ... \n"],"name":"stdout"}]}]}