{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sBERT_Scenario2_example.ipynb","provenance":[],"authorship_tag":"ABX9TyOAdBiCb1rzIDeCREblt4JE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xSwB5de8M-k3"},"source":["The script shows how to train Augmented SBERT (Domain-Transfer/Cross-Domain) strategy for STSb-QQP dataset.\r\n","For our example below we consider STSb (source) and QQP (target) datasets respectively.\r\n","Methodology:\r\n","Three steps are followed for AugSBERT data-augmentation strategy with Domain Trasfer / Cross-Domain - \r\n","1. Cross-Encoder aka BERT is trained over STSb (source) dataset.\r\n","2. Cross-Encoder is used to label QQP training (target) dataset (Assume no labels/no annotations are provided).\r\n","3. Bi-encoder aka SBERT is trained over the labeled QQP (target) dataset.\r\n","Citation: https://arxiv.org/abs/2010.08240\r\n","Usage:\r\n","python train_sts_qqp_crossdomain.py\r\n","OR\r\n","python train_sts_qqp_crossdomain.py pretrained_transformer_model_name"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9hPWKHpcPZv-","executionInfo":{"status":"ok","timestamp":1608956293762,"user_tz":-330,"elapsed":10385,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"efe3b1ae-916d-47ae-c6fa-e55ac1f3c36e"},"source":["#!pip install http://download.pytorch.org/whl/cu92/torch-1.7.0-cp36-cp36m-linux_x86_64.whl\r\n","!pip install torchvision\r\n","!pip install sentence-transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.8.1+cu101)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.19.4)\n","Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.7.0+cu101)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision) (0.16.0)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision) (0.8)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision) (3.7.4.3)\n","Collecting sentence-transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/9a/62beeb5501b70ab48b9e5bb92de290f00a661a1caa075c4aae56d452aaa0/sentence-transformers-0.4.0.tar.gz (65kB)\n","\u001b[K     |████████████████████████████████| 71kB 4.3MB/s \n","\u001b[?25hCollecting transformers<5.0.0,>=3.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n","\u001b[K     |████████████████████████████████| 1.5MB 8.9MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.7.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.19.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 28.3MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.8)\n","Collecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 35.9MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 58.6MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.8)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.16.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (1.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n","Building wheels for collected packages: sentence-transformers, sacremoses\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-0.4.0-cp36-none-any.whl size=102655 sha256=5a37aa62b818e7ebc3e47f747e9030eb9fa6fedd3d4fa717fb70602a1d4e819b\n","  Stored in directory: /root/.cache/pip/wheels/ff/76/65/50258d8b7930e909ea2f5bd006a23d520a16765af13ab45bb3\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=d384d1484e25bc9708538df8f01b427bd859cebd8f27ed3290aac423e11d63d5\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sentence-transformers sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers, sentencepiece, sentence-transformers\n","Successfully installed sacremoses-0.0.43 sentence-transformers-0.4.0 sentencepiece-0.1.94 tokenizers-0.9.4 transformers-4.1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5MCnLCCBND9Y","executionInfo":{"status":"ok","timestamp":1608956306109,"user_tz":-330,"elapsed":7161,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}}},"source":["from torch.utils.data import DataLoader\r\n","from sentence_transformers import models, losses, util\r\n","from sentence_transformers.cross_encoder import CrossEncoder\r\n","from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\r\n","from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer\r\n","from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, BinaryClassificationEvaluator\r\n","from sentence_transformers.readers import InputExample\r\n","from datetime import datetime\r\n","from zipfile import ZipFile\r\n","import logging\r\n","import csv\r\n","import sys\r\n","import torch\r\n","import math\r\n","import gzip\r\n","import os"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y0-LhHRvNJmM","executionInfo":{"status":"ok","timestamp":1608956313646,"user_tz":-330,"elapsed":881,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}}},"source":["#### Just some code to print debug information to stdout\r\n","logging.basicConfig(format='%(asctime)s - %(message)s',\r\n","                    datefmt='%Y-%m-%d %H:%M:%S',\r\n","                    level=logging.INFO,\r\n","                    handlers=[LoggingHandler()])\r\n","#### /print debug information to stdout"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"1MzbXx36NXHo","executionInfo":{"status":"ok","timestamp":1608956319406,"user_tz":-330,"elapsed":926,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}}},"source":["#You can specify any huggingface/transformers pre-trained model here, for example, bert-base-uncased, roberta-base, xlm-roberta-base\r\n","model_name = sys.argv[1] if len(sys.argv) > 1 else 'bert-base-uncased'\r\n","batch_size = 16\r\n","num_epochs = 1\r\n","max_seq_length = 128\r\n","use_cuda = torch.cuda.is_available()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"szJfgEnrN1Ld","executionInfo":{"status":"ok","timestamp":1608956342950,"user_tz":-330,"elapsed":19802,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"2bd1e4d5-57fb-4cb5-82e7-3308e0ed9322"},"source":["###### Read Datasets ######\r\n","sts_dataset_path = 'datasets/stsbenchmark.tsv.gz'\r\n","qqp_dataset_path = 'quora-IR-dataset'\r\n","\r\n","\r\n","# Check if the STSb dataset exsist. If not, download and extract it\r\n","if not os.path.exists(sts_dataset_path):\r\n","    util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', sts_dataset_path)\r\n","\r\n","\r\n","# Check if the QQP dataset exists. If not, download and extract\r\n","if not os.path.exists(qqp_dataset_path):\r\n","    logging.info(\"Dataset not found. Download\")\r\n","    zip_save_path = 'quora-IR-dataset.zip'\r\n","    util.http_get(url='https://sbert.net/datasets/quora-IR-dataset.zip', path=zip_save_path)\r\n","    with ZipFile(zip_save_path, 'r') as zipIn:\r\n","        zipIn.extractall(qqp_dataset_path)\r\n","\r\n","\r\n","cross_encoder_path = 'output/cross-encoder/stsb_indomain_'+model_name.replace(\"/\", \"-\")+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\r\n","bi_encoder_path = 'output/bi-encoder/qqp_cross_domain_'+model_name.replace(\"/\", \"-\")+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["100%|██████████| 392k/392k [00:01<00:00, 279kB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["2020-12-26 04:18:51 - Dataset not found. Download\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 93.6M/93.6M [00:08<00:00, 10.9MB/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":576},"id":"z4thJCTKN_P7","executionInfo":{"status":"error","timestamp":1608956476048,"user_tz":-330,"elapsed":1251,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"ffd76741-4117-431f-e610-40651dca1eb9"},"source":["###### Cross-encoder (simpletransformers) ######\r\n","\r\n","logging.info(\"Loading cross-encoder model: {}\".format(model_name))\r\n","# Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for cross-encoder model\r\n","cross_encoder = CrossEncoder(model_name, num_labels=1)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["2020-12-26 04:21:16 - Loading cross-encoder model: -f\n"],"name":"stdout"},{"output_type":"stream","text":["404 Client Error: Not Found for url: https://huggingface.co/-f/resolve/main/config.json\n"],"name":"stderr"},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1084\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m             \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Linked-Etag\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ETag\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/-f/resolve/main/config.json","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-74e70548190b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading cross-encoder model: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for cross-encoder model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcross_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name, num_labels, max_length, device, tokenizer_args)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \"\"\"\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mclassifier_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marchitectures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m'foo'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \"\"\"\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             )\n\u001b[0;32m--> 418\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Can't load config for '-f'. Make sure that:\n\n- '-f' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '-f' is the correct path to a directory containing a config.json file\n\n"]}]},{"cell_type":"code","metadata":{"id":"AKr3hMR0OKLF"},"source":["###### Bi-encoder (sentence-transformers) ######\r\n","\r\n","logging.info(\"Loading bi-encoder model: {}\".format(model_name))\r\n","\r\n","# Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings\r\n","word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\r\n","\r\n","# Apply mean pooling to get one fixed sized sentence vector\r\n","pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\r\n","                               pooling_mode_mean_tokens=True,\r\n","                               pooling_mode_cls_token=False,\r\n","                               pooling_mode_max_tokens=False)\r\n","\r\n","bi_encoder = SentenceTransformer(modules=[word_embedding_model, pooling_model])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DPvSjmDFOSLy"},"source":["# Step 1: Train cross-encoder model with STSbenchmark"]},{"cell_type":"code","metadata":{"id":"tMAi77WLOfe_"},"source":["logging.info(\"Step 1: Train cross-encoder: {} with STSbenchmark (source dataset)\".format(model_name))\r\n","\r\n","gold_samples = []\r\n","dev_samples = []\r\n","test_samples = []\r\n","\r\n","with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as fIn:\r\n","    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\r\n","    for row in reader:\r\n","        score = float(row['score']) / 5.0  # Normalize score to range 0 ... 1\r\n","\r\n","        if row['split'] == 'dev':\r\n","            dev_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\r\n","        elif row['split'] == 'test':\r\n","            test_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\r\n","        else:\r\n","            #As we want to get symmetric scores, i.e. CrossEncoder(A,B) = CrossEncoder(B,A), we pass both combinations to the train set\r\n","            gold_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\r\n","            gold_samples.append(InputExample(texts=[row['sentence2'], row['sentence1']], label=score))\r\n","\r\n","\r\n","# We wrap gold_samples (which is a List[InputExample]) into a pytorch DataLoader\r\n","train_dataloader = DataLoader(gold_samples, shuffle=True, batch_size=batch_size)\r\n","\r\n","\r\n","# We add an evaluator, which evaluates the performance during training\r\n","evaluator = CECorrelationEvaluator.from_input_examples(dev_samples, name='sts-dev')\r\n","\r\n","# Configure the training\r\n","warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\r\n","logging.info(\"Warmup-steps: {}\".format(warmup_steps))\r\n","\r\n","# Train the cross-encoder model\r\n","cross_encoder.fit(train_dataloader=train_dataloader,\r\n","          evaluator=evaluator,\r\n","          epochs=num_epochs,\r\n","          evaluation_steps=1000,\r\n","          warmup_steps=warmup_steps,\r\n","          output_path=cross_encoder_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IzqyjJh6OlyZ"},"source":["# Step 2: Label QQP train dataset using cross-encoder (BERT) model"]},{"cell_type":"code","metadata":{"id":"NRgO1ZOHOp2G"},"source":["logging.info(\"Step 2: Label QQP (target dataset) with cross-encoder: {}\".format(model_name))\r\n","\r\n","cross_encoder = CrossEncoder(cross_encoder_path)\r\n","\r\n","silver_data = []\r\n","\r\n","with open(os.path.join(qqp_dataset_path, \"classification/train_pairs.tsv\"), encoding='utf8') as fIn:\r\n","    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\r\n","    for row in reader:\r\n","        if row['is_duplicate'] == '1':\r\n","            silver_data.append([row['question1'], row['question2']])\r\n","\r\n","silver_scores = cross_encoder.predict(silver_data)\r\n","\r\n","# All model predictions should be between [0,1]\r\n","assert all(0.0 <= score <= 1.0 for score in silver_scores)\r\n","\r\n","binary_silver_scores = [1 if score >= 0.5 else 0 for score in silver_scores]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gU4UJw1TOtLV"},"source":["# Step 3: Train bi-encoder (SBERT) model with QQP dataset - Augmented SBERT"]},{"cell_type":"code","metadata":{"id":"05e6xGdeOxTA"},"source":["logging.info(\"Step 3: Train bi-encoder: {} over labeled QQP (target dataset)\".format(model_name))\r\n","\r\n","# Convert the dataset to a DataLoader ready for training\r\n","logging.info(\"Loading BERT labeled QQP dataset\")\r\n","qqp_train_data = list(InputExample(texts=[data[0], data[1]], label=score) for (data, score) in zip(silver_data, binary_silver_scores))\r\n","\r\n","train_dataset = SentencesDataset(qqp_train_data, bi_encoder)\r\n","train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\r\n","train_loss = losses.MultipleNegativesRankingLoss(bi_encoder)\r\n","\r\n","###### Classification ######\r\n","# Given (quesiton1, question2), is this a duplicate or not?\r\n","# The evaluator will compute the embeddings for both questions and then compute\r\n","# a cosine similarity. If the similarity is above a threshold, we have a duplicate.\r\n","logging.info(\"Read QQP dev dataset\")\r\n","\r\n","dev_sentences1 = []\r\n","dev_sentences2 = []\r\n","dev_labels = []\r\n","\r\n","with open(os.path.join(qqp_dataset_path, \"classification/dev_pairs.tsv\"), encoding='utf8') as fIn:\r\n","    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\r\n","    for row in reader:\r\n","        dev_sentences1.append(row['question1'])\r\n","        dev_sentences2.append(row['question2'])\r\n","        dev_labels.append(int(row['is_duplicate']))\r\n","\r\n","evaluator = BinaryClassificationEvaluator(dev_sentences1, dev_sentences2, dev_labels)\r\n","\r\n","# Configure the training.\r\n","warmup_steps = math.ceil(len(train_dataset) * num_epochs / batch_size * 0.1) #10% of train data for warm-up\r\n","logging.info(\"Warmup-steps: {}\".format(warmup_steps))\r\n","\r\n","# Train the bi-encoder model\r\n","bi_encoder.fit(train_objectives=[(train_dataloader, train_loss)],\r\n","          evaluator=evaluator,\r\n","          epochs=num_epochs,\r\n","          evaluation_steps=1000,\r\n","          warmup_steps=warmup_steps,\r\n","          output_path=bi_encoder_path,\r\n","          output_path_ignore_not_empty=True\r\n","          )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UusT3U8OO_uC"},"source":["Evaluate Augmented SBERT performance on QQP benchmark dataset\r\n"]},{"cell_type":"code","metadata":{"id":"A00Q6jWCM2n8"},"source":["# Loading the augmented sbert model \r\n","bi_encoder = SentenceTransformer(bi_encoder_path)\r\n","\r\n","logging.info(\"Read QQP test dataset\")\r\n","test_sentences1 = []\r\n","test_sentences2 = []\r\n","test_labels = []\r\n","\r\n","with open(os.path.join(qqp_dataset_path, \"classification/test_pairs.tsv\"), encoding='utf8') as fIn:\r\n","    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\r\n","    for row in reader:\r\n","        test_sentences1.append(row['question1'])\r\n","        test_sentences2.append(row['question2'])\r\n","        test_labels.append(int(row['is_duplicate']))\r\n","\r\n","evaluator = BinaryClassificationEvaluator(test_sentences1, test_sentences2, test_labels)\r\n","bi_encoder.evaluate(evaluator)"],"execution_count":null,"outputs":[]}]}