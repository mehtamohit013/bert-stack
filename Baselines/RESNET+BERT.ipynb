{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"RESNET+BERT.ipynb","provenance":[{"file_id":"1MOGb2IZiK8QxRX_fFTMrocK9RbMFP-08","timestamp":1616819937878}],"collapsed_sections":["JHAMYwNfv5Th","quJKUGIXv8zk","STnKB8R7NVEa","tgk_opqd2prV","xnOxcgwk5WiU","M2mT9dqsm5Vy","hV30ckW6NElG","rVzPL-EMTXpM","beqWAJVP2_aP","n7AQVZT7TCVe","J2Mw6eG5KqVn","hFtpg_GR4iNJ","3O5iEjYD7H-s","76ht5qjQMVsm","iaMPRBwZdBjN","mwKqXD75KiPI","ZuGYeCAq97yQ","kYlESuc6HmGF","MQ3f5gO7Hhu0"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rXKu5-xjGwOa","executionInfo":{"status":"ok","timestamp":1617269712497,"user_tz":-330,"elapsed":1315,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}},"outputId":"05f94457-4acb-46ca-e273-b557918d0891"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Thu Apr  1 09:35:11 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P0    28W /  70W |   1640MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mz76Bz490lqB","executionInfo":{"status":"ok","timestamp":1617269713141,"user_tz":-330,"elapsed":1952,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}},"outputId":"387a9a64-f0f8-4fec-f391-85e4e8b41f28"},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JHAMYwNfv5Th"},"source":["# Imports"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"_pwgo3m2wVQS","executionInfo":{"status":"ok","timestamp":1617269715736,"user_tz":-330,"elapsed":4543,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}},"outputId":"6a8538c0-26b7-4554-b4c2-11057c710034"},"source":["!pip install transformers"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jKIzzX6J3QtS","executionInfo":{"status":"ok","timestamp":1617269715737,"user_tz":-330,"elapsed":4539,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["import pandas as pd\n","import pickle\n","import random\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader, Dataset\n","import os\n","from enum import Enum\n","from torch.nn import functional as F\n","import time\n","import logging\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","logger = logging.getLogger(__name__)\n","random.seed(13)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"quJKUGIXv8zk"},"source":["# Utilities"]},{"cell_type":"code","metadata":{"id":"u1t9DLnOsFN_","executionInfo":{"status":"ok","timestamp":1617269715739,"user_tz":-330,"elapsed":4535,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["def pytorch_cos_sim(a: torch.Tensor, b: torch.Tensor):\n","    \"\"\"\n","    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n","    This function can be used as a faster replacement for 1-scipy.spatial.distance.cdist(a,b)\n","    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n","    \"\"\"\n","    if not isinstance(a, torch.Tensor):\n","        a = torch.tensor(a)\n","\n","    if not isinstance(b, torch.Tensor):\n","        b = torch.tensor(b)\n","\n","    if len(a.shape) == 1:\n","        a = a.unsqueeze(0)\n","\n","    if len(b.shape) == 1:\n","        b = b.unsqueeze(0)\n","\n","    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n","    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n","    return torch.mm(a_norm, b_norm.transpose(0, 1))"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"DQdj5p8fzl-1","executionInfo":{"status":"ok","timestamp":1617269715739,"user_tz":-330,"elapsed":4532,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["def string_sentence(i,qr):\n","    return qr.loc[i,'Title'] + ' '  + ' '.join(qr.loc[i,'Tags']) + ' '  + qr.loc[i,'Text']"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"STnKB8R7NVEa"},"source":["# Model Hyper-Parameters"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DebNqaCF2uE_","executionInfo":{"status":"ok","timestamp":1617269715739,"user_tz":-330,"elapsed":4528,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}},"outputId":"a7bccc0f-5b84-4df7-c97b-cc23422a704f"},"source":["folder_quora = '/gdrive/MyDrive/Linked/new_splits'\n","folder = '/gdrive/MyDrive/Linked/Multimodal'\n","img_dir = '/gdrive/MyDrive/MultiModal/data'\n","os.makedirs(folder,exist_ok = True)\n","BATCH_SIZE = 8\n","fin_BATCH_SIZE = 32\n","n_worker = 2\n","margin  = 0.8\n","#---------\n","#Note that amp cannot be used without sigmoid\n","use_amp = True\n","use_sig = True # For BCEloss\n","#----------\n","\n","use_sig_eval = False\n","\n","# eval_BATCH_SIZE = 4 \n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tgk_opqd2prV"},"source":["# Creating Dataset and Dataloader"]},{"cell_type":"markdown","metadata":{"id":"xnOxcgwk5WiU"},"source":["###Loading Data"]},{"cell_type":"code","metadata":{"id":"N1RqPrOk5Ynm","executionInfo":{"status":"ok","timestamp":1617269719720,"user_tz":-330,"elapsed":8506,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["with open(folder_quora+'/data/splits/pandas_split.txt','rb') as a:\n","    train_qr=pickle.load(a)\n","    dev_qr = pickle.load(a)\n","    test_qr = pickle.load(a)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z5qm94QOFH2V","executionInfo":{"status":"ok","timestamp":1617269719721,"user_tz":-330,"elapsed":8504,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["with open(folder_quora+'/data/splits/train_dev_test.txt','rb') as a:\n","    train_data=pickle.load(a)\n","    train_score = pickle.load(a)\n","    dev_data = pickle.load(a)\n","    dev_score = pickle.load(a)\n","    test_data = pickle.load(a)\n","    test_score = pickle.load(a)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M2mT9dqsm5Vy"},"source":["### Creating Dataset "]},{"cell_type":"code","metadata":{"id":"NqWN5n-Q8fOk","executionInfo":{"status":"ok","timestamp":1617269719721,"user_tz":-330,"elapsed":8500,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["# !pip install sentence-transformers"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"x78sEuM43ncK","executionInfo":{"status":"ok","timestamp":1617269719721,"user_tz":-330,"elapsed":8497,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["# from sentence_transformers import models, losses, util, SentenceTransformer, SentencesDataset, InputExample, evaluation\n","# from sentence_transformers.cross_encoder import CrossEncoder"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"X86sdgPk3lQx","executionInfo":{"status":"ok","timestamp":1617269719722,"user_tz":-330,"elapsed":8495,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["from PIL import Image\n","from torchvision import transforms\n","transform_pipe = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"E6I9YioKE8uK","executionInfo":{"status":"ok","timestamp":1617269719723,"user_tz":-330,"elapsed":8492,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["from transformers import BertTokenizerFast, BertConfig,BertModel\n","config = BertConfig()\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"KXPoomQx4Zby","executionInfo":{"status":"ok","timestamp":1617269719724,"user_tz":-330,"elapsed":8489,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["class custom_dataset(Dataset):\n","    def __init__(self,qr,qr_idx,img_dir,label,transform = None):\n","        self.qr = qr\n","        self.qr_idx = qr_idx\n","        self.img_dir = img_dir\n","        self.label = label\n","        self.transform = transform\n","\n","    def image_adder(self,id1):\n","        img_id1 = list()\n","        if((self.qr.at[id1,'Attachments'])!=None):\n","            for i in self.qr.at[id1,'Attachments']:\n","                try:\n","                    img_path = os.path.join(self.img_dir,i)\n","                    img = Image.open(img_path).convert('RGB')\n","                    if(self.transform):\n","                        img = self.transform(img)\n","                        img.reshape(3,224,224)\n","                    img_id1.append(img)\n","                except Exception as e: \n","                    print(e)\n","        else:\n","            img_id1.append(torch.randn(3,224,224))\n","\n","        # Work on this, for few examples, it is still saying list index out of range\n","        if(len(img_id1)==0):\n","            # print('No attachments found for id {}'.format(id1))\n","            print(f'Something went wrong with the image of id {id1}')\n","            img_id1.append(torch.randn(3,224,224))\n","\n","        return img_id1\n","    \n","    def __getitem__(self,idx):\n","        id1 = self.qr_idx[idx][0]\n","        id2 = self.qr_idx[idx][1]\n","        img_id1 = self.image_adder(id1)\n","        img_id2 = self.image_adder(id2)\n","\n","        # print(len(img_id1))\n","\n","        # print('Printing id1 {} and len {} and id2 {} and len {} '.format(\n","        #     id1,len(img_id1),\n","        #     id2, len(img_id2)\n","        # ))\n","\n","        # print('Printing id1 shape {} and id2  shape {}'.format(\n","        #     img_id1[0].shape,\n","        #     img_id2[0].shape\n","        # ))        \n","\n","        sample = {\n","            'image': [img_id1[0],img_id2[0]]    #Currently taking only one input image\n","        }\n","\n","        t1 = '[CLS]' + self.qr.loc[id1,'Title'] + ' ' + ' '.join(self.qr.loc[id1,'Tags']) + ' ' + self.qr.loc[id1,'Text'] + '[SEP]'\n","        t2 = '[CLS]' + self.qr.loc[id2,'Title']+ ' ' + ' '.join(self.qr.loc[id2,'Tags']) + ' ' + self.qr.loc[id2,'Text'] + '[SEP]'\n","        indexed_t1 = tokenizer.encode(t1,max_length = 512,truncation = True)\n","        indexed_t2 = tokenizer.encode(t2,max_length = 512,truncation = True)\n","\n","        while(len(indexed_t1)<512):\n","            indexed_t1.append(0)\n","        while(len(indexed_t2)<512):\n","            indexed_t2.append(0)\n","\n","        ten_t1 = torch.tensor(indexed_t1)[:512]\n","        ten_t2 = torch.tensor(indexed_t2)[:512]\n","\n","        try:\n","            sample[\"label\"] = self.label[idx]\n","            sample[\"token\"] = [ten_t1,ten_t2] # torch.Size([batch_size, 512])\n","        except Exception as e:\n","            print(e)\n","        \n","        return sample\n","\n","    def __len__(self):\n","        return len(self.label)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"8AgFP1sf7v5g","executionInfo":{"status":"ok","timestamp":1617269719724,"user_tz":-330,"elapsed":8486,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["# train_qr.loc[92104]"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"0-vLXST-8v9Y","executionInfo":{"status":"ok","timestamp":1617269720432,"user_tz":-330,"elapsed":9191,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["# img_path = os.path.join(img_dir,'images/38568_1.png')\n","# img = Image.open(img_path).convert('RGB')\n","# print(img.size)\n","# img = transform_pipe(img)\n","# img"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hV30ckW6NElG"},"source":["## Train, dev and test dataloader"]},{"cell_type":"code","metadata":{"id":"qRNteBxHOomg","executionInfo":{"status":"ok","timestamp":1617269720433,"user_tz":-330,"elapsed":9189,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["train_dataset = custom_dataset(train_qr,\n","                               train_data,\n","                               img_dir,\n","                               train_score,\n","                               transform = transform_pipe)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"d78ebAQBSdJI","executionInfo":{"status":"ok","timestamp":1617269720434,"user_tz":-330,"elapsed":9187,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    pin_memory=True,\n","    num_workers = n_worker,\n","    drop_last = True\n","    )"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Uwd7iofy9Jc","executionInfo":{"status":"ok","timestamp":1617269720435,"user_tz":-330,"elapsed":9185,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["# for i,sample in enumerate(train_loader):\n","#     try:\n","#         b=1\n","#         # print(i)\n","#         # print(len(sample))\n","#         # print()\n","#         # print(sample)\n","#     except Exception as e:\n","#         print(i,e)\n","#         break"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"IU4CNMybfgqI","executionInfo":{"status":"ok","timestamp":1617269720436,"user_tz":-330,"elapsed":9183,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["dev_dataset = custom_dataset(pd.concat([train_qr,dev_qr]),\n","                             dev_data,\n","                             img_dir,\n","                             dev_score,\n","                             transform = transform_pipe)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"AUvKZ6rifxQW","executionInfo":{"status":"ok","timestamp":1617269720436,"user_tz":-330,"elapsed":9181,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["dev_loader = DataLoader(\n","    dev_dataset,\n","    batch_size=BATCH_SIZE,\n","    pin_memory=True,\n","    num_workers = n_worker,\n","    drop_last = True\n","    )"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"fe1vYfiifhM2","executionInfo":{"status":"ok","timestamp":1617269720437,"user_tz":-330,"elapsed":9179,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["test_dataset = custom_dataset(pd.concat([train_qr,test_qr]),\n","                              test_data,\n","                              img_dir,\n","                              test_score,\n","                              transform = transform_pipe)"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"RP_N_Kb5f1jD","executionInfo":{"status":"ok","timestamp":1617269720437,"user_tz":-330,"elapsed":9175,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["test_loader = DataLoader(\n","    test_dataset,\n","    batch_size=BATCH_SIZE,\n","    pin_memory=True,\n","    num_workers = n_worker,\n","    drop_last = True\n","    )"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rVzPL-EMTXpM"},"source":["#Model"]},{"cell_type":"markdown","metadata":{"id":"beqWAJVP2_aP"},"source":["## Loading Model "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"Ye9P1XUUToUA","executionInfo":{"status":"ok","timestamp":1617269727665,"user_tz":-330,"elapsed":16400,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["from torchvision import models\n","res50 = models.resnet50(pretrained = True)\n","res50 = res50.to(device)\n","for param in res50.parameters():\n","    param.requires_grad = True\n","# res50.eval()\n","\n","bert = BertModel.from_pretrained('bert-base-uncased')\n","bert = bert.to(device)\n","for param in bert.parameters():\n","    param.requires_grad = True\n","# bert.eval()"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n7AQVZT7TCVe"},"source":["## Model class"]},{"cell_type":"code","metadata":{"id":"rqbP02FiSurJ","executionInfo":{"status":"ok","timestamp":1617269727666,"user_tz":-330,"elapsed":16397,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["class resbert(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.bert = bert\n","        self.res50 = res50\n","        self.fc1 = nn.Linear(768+1000,1500)\n","        self.fc2 = nn.Linear(1500,1000)\n","        self.fc3 = nn.Linear(1000,768)\n","    \n","    def forward(self,X1,X2):\n","        res_out = self.res50(X1) #[BATCH_SIZE,1000] \n","        bert_out = torch.mean(self.bert(X2).last_hidden_state,dim=1) #[BATCH_SIZE,768]\n","        fc1 = self.fc1(torch.cat((bert_out,res_out),dim=1))\n","        fc2 = self.fc2(fc1)\n","        out = self.fc3(fc2)\n","        return out"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"kt4F8PenUlq6","executionInfo":{"status":"ok","timestamp":1617269727667,"user_tz":-330,"elapsed":16395,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["model = resbert().to(device)"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J2Mw6eG5KqVn"},"source":["# Losses"]},{"cell_type":"markdown","metadata":{"id":"hFtpg_GR4iNJ"},"source":["## Multiple Negative Ranking loss"]},{"cell_type":"code","metadata":{"id":"xMIA3Ydod65P","executionInfo":{"status":"ok","timestamp":1617269727667,"user_tz":-330,"elapsed":16392,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["class negrankloss():\n","    def __init__(self,scale: float = 20.0):\n","        # self.cosine_sim = nn.CosineSimilarity()\n","        self.scale = scale\n","        self.cross_entropy = nn.CrossEntropyLoss()\n","\n","    def cal_loss(self,emb1: torch.Tensor,emb2: torch.Tensor, labels: torch.Tensor):\n","        scores  = pytorch_cos_sim(emb1,emb2) *self.scale\n","        # print(f'The scores of cosine similarity for MNRloss is {scores}')\n","        labels = torch.tensor(range(len(scores)), dtype=torch.long, device=scores.device)\n","        loss = self.cross_entropy(scores, labels)\n","        return loss"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"tMbsWT-DlN1G","executionInfo":{"status":"ok","timestamp":1617269727668,"user_tz":-330,"elapsed":16389,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["MNRloss = negrankloss()"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3O5iEjYD7H-s"},"source":["## Online Constrantive Loss\n"]},{"cell_type":"code","metadata":{"id":"pv3V30gQ7LKy","executionInfo":{"status":"ok","timestamp":1617269744568,"user_tz":-330,"elapsed":409,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["class SiameseDistanceMetric(Enum):\n","    \"\"\"\n","    The metric for the contrastive loss\n","    \"\"\"\n","    EUCLIDEAN = lambda x, y: F.pairwise_distance(x, y, p=2)\n","    MANHATTAN = lambda x, y: F.pairwise_distance(x, y, p=1)\n","    COSINE_DISTANCE = lambda x, y: 1-F.cosine_similarity(x, y)\n"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"0zSArl7MQ4sO","executionInfo":{"status":"ok","timestamp":1617269744570,"user_tz":-330,"elapsed":52,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["class OnlineConstrantiveLoss():\n","    def __init__(self,distance_metric=SiameseDistanceMetric.COSINE_DISTANCE,margin: float = 0.5):\n","        self.distance_metric = distance_metric\n","        self.margin = margin\n","\n","    def cal_loss(self,emb1,emb2,labels,size_average=False):\n","        distance_matrix = self.distance_metric(emb1,emb2)\n","        # print(f'distance matrix of OCloss is {distance_matrix}')\n","        negs = distance_matrix[labels == 0]\n","        poss = distance_matrix[labels == 1]\n","        # print(f'positive and negatives are {poss} and {negs}')\n","\n","        # select hard positive and hard negative pairs\n","        # But for current batch size of 1, it is impossible to consider hard positive and hard negative\n","        negative_pairs = negs[negs < (poss.max() if len(poss) > 1 else negs.mean())]\n","        positive_pairs = poss[poss > (negs.min() if len(negs) > 1 else poss.mean())]\n","\n","        negative_pairs = negs\n","        positive_pairs = poss\n","\n","        # print(f'positive and negative pairs are {positive_pairs} and {negative_pairs}')\n","\n","        positive_loss = positive_pairs.pow(2).sum()\n","        negative_loss = F.relu(self.margin - negative_pairs).pow(2).sum()\n","        # print(f'positive loss is {positive_loss} and negative loss is {negative_loss}')\n","        loss = positive_loss + negative_loss\n","        return loss\n","    "],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"cr5sO1wM8NIc","executionInfo":{"status":"ok","timestamp":1617269744571,"user_tz":-330,"elapsed":48,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["OCloss = OnlineConstrantiveLoss(margin = margin)"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"76ht5qjQMVsm"},"source":["## Cross Entropy Loss"]},{"cell_type":"code","metadata":{"id":"iSxbp43HMVHn","executionInfo":{"status":"ok","timestamp":1617269744572,"user_tz":-330,"elapsed":44,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["class crossentropy():\n","    def __init__(self):\n","        self.cross_entropy = nn.BCELoss()\n","        self.cos_sim =  nn.CosineSimilarity(dim=1,eps=1e-6)\n","        self.sig = nn.Sigmoid()\n","        self.BCE_with_sig = nn.BCEWithLogitsLoss()\n","    \n","    def cal_loss(self,emb1:torch.Tensor,emb2:torch.Tensor,label:torch.Tensor):\n","        sim = (self.cos_sim(emb1,emb2))\n","        sim = sim - 0.5\n","        \n","        if(use_sig == True):\n","            loss = self.BCE_with_sig(sim,label)\n","        else:\n","            sim[sim<0]=0\n","            loss = self.cross_entropy(sim,label)\n","        return loss"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"tfdnKk2HUW9X","executionInfo":{"status":"ok","timestamp":1617269744573,"user_tz":-330,"elapsed":40,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["BCEloss = crossentropy()"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iaMPRBwZdBjN"},"source":["# Area under curve : AUC(0.05)"]},{"cell_type":"code","metadata":{"id":"mAUnsRkLdQLC","executionInfo":{"status":"ok","timestamp":1617269744574,"user_tz":-330,"elapsed":35,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["import sklearn.metrics as skm"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"N2Z1Yk_TEPj9","executionInfo":{"status":"ok","timestamp":1617269744574,"user_tz":-330,"elapsed":31,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["class AUC():\n","    def __init__(self,max_fpr:float = 0.05):\n","        self.max_fpr = 0.05\n","        self.cos_sim =  nn.CosineSimilarity(dim=1,eps=1e-6)\n","        self.sig = nn.Sigmoid()\n","\n","    def cal(self,model:resbert,loader:DataLoader):\n","        y_pred = []\n","        y_true = []\n","        y_pred_sig = []\n","        start = time.time()\n","        for i,batch in enumerate(loader):\n","            with torch.no_grad():\n","                images = batch[\"image\"]\n","                label = batch[\"label\"]\n","                label = label.float()\n","                token = batch[\"token\"]\n","\n","                images[0],images[1] = images[0].to(device),images[1].to(device)\n","                token[0],token[1] = token[0].to(device), token[1].to(device)\n","                label = label.to(device)\n","            \n","                # compute the model output\n","                yhat1 = model(images[0], token[0])\n","                yhat2 = model(images[1],token[1])\n","\n","                if(i%1000==0):\n","                    print(f'    {i} iterations have been done in {time.time()-start} seconds')\n","\n","                sim = (self.cos_sim(yhat1,yhat2))\n","                a=self.sig(sim-0.5)\n","                a[a>0.5] = 1\n","                a[a<0.5] = 0\n","\n","                y_pred_sig.append(a.cpu().numpy())\n","                y_pred.append(sim.cpu().numpy())\n","                y_true.append(label.cpu().numpy())\n","\n","                del  label, token,yhat1,yhat2,sim,a,images \n","        \n","        y_true = np.array(y_true).flatten()\n","        y_pred = np.array(y_pred).flatten()\n","        y_pred_sig = np.array(y_pred_sig).flatten()\n","        Bin_score = skm.accuracy_score(y_true,y_pred_sig) \n","        AUC_score = skm.roc_auc_score(y_true,y_pred,max_fpr = self.max_fpr)\n","        conf_matrix = skm.confusion_matrix(y_true,y_pred_sig)\n","        plt.hist(y_pred,bins='auto')\n","        plt.show()\n","        return AUC_score,Bin_score,conf_matrix"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"CXrynrKZLn2M","executionInfo":{"status":"ok","timestamp":1617269744576,"user_tz":-330,"elapsed":28,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["AUC_evaluator = AUC()"],"execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mwKqXD75KiPI"},"source":["# Calculating loss and executing"]},{"cell_type":"code","metadata":{"id":"bKJz98PeGje6","executionInfo":{"status":"ok","timestamp":1617269744576,"user_tz":-330,"elapsed":25,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["save_dir = folder + '/model_state_dict'\n","os.makedirs(save_dir,exist_ok = True)"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"_gc2jj5PKkGP","executionInfo":{"status":"ok","timestamp":1617269744577,"user_tz":-330,"elapsed":22,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}}},"source":["from torch.optim import Adam\n","from torch.nn import BCELoss\n","import time\n","import copy\n","from transformers import get_linear_schedule_with_warmup\n","\n","def train_model(train_loader, model,num_epochs):\n","    model.train()\n","\n","    since = time.time()\n","\n","    optimizer = Adam(model.parameters(), lr=1e-4)\n","    train_steps = num_epochs\n","    \n","    best_acc = 0.0\n","\n","    acc_steps = fin_BATCH_SIZE/BATCH_SIZE\n","\n","    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n","\n","    # enumerate epochs\n","    for epoch in range(num_epochs):\n","        # enumerate mini batches\n","        # avg_loss = 0\n","        \n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","        \n","        running_OCloss = 0.0\n","        running_MNRloss = 0.0\n","        running_BCEloss = 0.0\n","\n","        model.zero_grad()\n","\n","        for i, batch in enumerate(train_loader):\n","\n","            with torch.cuda.amp.autocast(enabled=use_amp):\n","                \n","                if(i%5000 == 0):\n","                    print('Running dev_evaluator for 2000 samples i.e. 1000 iterations. Time taken: 5 min')\n","                    a,b,c = AUC_evaluator.cal(model,dev_loader)\n","                    print(f'AUC score is {a}  while binary score is {b} and the confusion matrix is {c} on dev loader')\n","                    torch.save(model.state_dict(), save_dir+'/bert_res.bin')\n","                    print(f'Model is saved after {i} iterations ')\n","        \n","                images = batch[\"image\"]\n","                label = batch[\"label\"]\n","                label = label.float()\n","                token = batch[\"token\"]\n","                \n","                images[0],images[1] = images[0].to(device),images[1].to(device)\n","                token[0],token[1] = token[0].to(device), token[1].to(device)\n","                label = label.to(device)\n","                \n","                yhat1= model(images[0],token[0])\n","                yhat2 = model(images[1],token[1])\n","\n","                # yhat1_pos = model(pos_token[0])\n","                # yhat2_pos = model(pos_token[1])\n","                \n","                BCEloss_val = BCEloss.cal_loss(yhat1,yhat2,label)\n","                running_BCEloss += BCEloss_val.item()*BATCH_SIZE\n","                BCEloss_val = BCEloss_val/acc_steps\n","\n","                # MNRloss_val = MNRloss.cal_loss(yhat1_pos,yhat2_pos)\n","                # running_MNRloss += MNRloss_val.item()*BATCH_SIZE\n","                # MNRloss_val = MNRloss_val/acc_steps\n","\n","                OCloss_val = OCloss.cal_loss(yhat1,yhat2,label)\n","                running_OCloss +=  OCloss_val.item()*BATCH_SIZE\n","                OCloss_val = OCloss_val/acc_steps\n","            \n","            scaler.scale(BCEloss_val).backward(retain_graph = True)\n","            # scaler.scale(MNRloss_val).backward(retain_graph=True)\n","            scaler.scale(OCloss_val).backward()\n","\n","            if((i+1)%acc_steps==0):\n","                scaler.step(optimizer)\n","                scaler.update()\n","                optimizer.zero_grad()\n","            \n","            if(i%1000 == 0 ):\n","                print('OCloss is {} and BCEloss is {} and MNRloss is {}  and time taken is {} after {} iterations'.format(\n","                    running_OCloss/((i+1)*BATCH_SIZE),\n","                    running_BCEloss/((i+1)*BATCH_SIZE),\n","                    running_MNRloss/(i+1)*BATCH_SIZE,\n","                    time.time()-since,\n","                    i))\n","            \n","            del OCloss_val,BCEloss_val, yhat1,yhat2, label, token, images\n","\n","        epoch_OC = running_OCloss / len(train_data)\n","        epoch_BCE = running_BCEloss / len(train_data)\n","        epoch_MNR = running_MNRloss / len(train_data)\n","        print(f'{epoch+1} Epoch completed. OCloss is {epoch_OC} and BCEloss is {epoch_BCE} and MNRloss is {epoch_MNR}')\n","        a,b,c = AUC_evaluator.cal(model,train_loader)\n","        print(f'AUC and Bin_acc after training {epoch+1} on train dataloader is {a,b} and confusion matrix is {c}')\n","        a,b,c = AUC_evaluator.cal(model,dev_loader)\n","        print(f'AUC and Bin_acc after training {epoch+1} on dev dataloader is {a,b} and the confusion matris is {c}')\n","        torch.save(model.state_dict(), save_dir+'/bert_res.bin')\n","        print(f'Model is saved after {epoch+1} epochs ')\n","        "],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zXsl7QFAQ1UR"},"source":["# Training the model"]},{"cell_type":"code","metadata":{"id":"D1Sv5OY55Bnz"},"source":["model.load_state_dict(torch.load(save_dir+'/bert_res.bin'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"GaOYt_HqWcSO","executionInfo":{"status":"error","timestamp":1617282151360,"user_tz":-330,"elapsed":15096,"user":{"displayName":"Mohit Mehta","photoUrl":"","userId":"10204087988175097062"}},"outputId":"62fb8afe-f283-411b-9c38-a9fa4a3cfd7e"},"source":["#Overall parameters in model\n","print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n","train_model(train_loader,model,5)"],"execution_count":39,"outputs":[{"output_type":"stream","text":["139962540\n","Epoch 0/4\n","----------\n","Running dev_evaluator for 2000 samples i.e. 1000 iterations. Time taken: 5 min\n","    0 iterations have been done in 0.983522891998291 seconds\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["AUC score is 0.5016934377993233  while binary score is 0.5163934426229508 and the confusion matrix is [[446  42]\n"," [430  58]] on dev loader\n","Model is saved after 0 iterations \n","OCloss is 0.7465550303459167 and BCEloss is 0.6377720832824707 and MNRloss is 0.0  and time taken is 41.847556829452515 after 0 iterations\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569.0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["OCloss is 1.5149225508952355 and BCEloss is 0.6975174913039575 and MNRloss is 0.0  and time taken is 1313.882955789566 after 1000 iterations\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["OCloss is 1.5246044636696114 and BCEloss is 0.6981994166545782 and MNRloss is 0.0  and time taken is 2534.5724251270294 after 2000 iterations\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569\n","OCloss is 1.522500430304223 and BCEloss is 0.6981336395170878 and MNRloss is 0.0  and time taken is 3751.703007698059 after 3000 iterations\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569.0\n","1 Epoch completed. OCloss is 1.5220917126676212 and BCEloss is 0.6980362711712429 and MNRloss is 0.0\n","    0 iterations have been done in 0.6460461616516113 seconds\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569.0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["    1000 iterations have been done in 693.3634943962097 seconds\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["    2000 iterations have been done in 1386.2093098163605 seconds\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569\n","    3000 iterations have been done in 2077.826290369034 seconds\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["AUC and Bin_acc after training 1 on train dataloader is (0.5024849240755774, 0.5187552683338016) and confusion matrix is [[ 3790 10446]\n"," [ 3256 10980]]\n","    0 iterations have been done in 0.3460836410522461 seconds\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["AUC and Bin_acc after training 1 on dev dataloader is (0.5003497129941634, 0.5245901639344263) and the confusion matris is [[142 346]\n"," [118 370]]\n","Model is saved after 1 epochs \n","Epoch 1/4\n","----------\n","Running dev_evaluator for 2000 samples i.e. 1000 iterations. Time taken: 5 min\n","    0 iterations have been done in 0.6248505115509033 seconds\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAPy0lEQVR4nO3de6xlZ13G8e9Dx4Ig0MschzJTnRIGdawKzUktIQFkiJaWtE0ktVVkwIkTEBHFBAb7R4lK0noBIUF0QisDwV6sYCcW1DK0aSS0cEprr9AOpaVTpp2D0HohQkd+/rFX48npmTl777XPZd5+P8nkrMu79v692Wees/a713p3qgpJUluettIFSJImz3CXpAYZ7pLUIMNdkhpkuEtSg9asdAEAa9eurY0bN650GZJ0RLn55pu/VVVTC+1bFeG+ceNGZmZmVroMSTqiJHngUPsclpGkBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAatijtUJbVj445rxj72/ovOnGAlT22euUtSgwx3SWqQ4S5JDTLcJalBi4Z7kkuTHEhyx5xtf5rkK0luS/KpJMfM2ffuJHuTfDXJLy1V4ZKkQxvmzP2jwOnztl0LnFxVPwvcA7wbIMlm4Dzgp7tj/jLJUROrVpI0lEXDvapuAL49b9u/VNXBbvVGYEO3fDZweVV9r6q+DuwFTp1gvZKkIUxizP03gM90y+uBB+fs29dte5Ik25PMJJmZnZ2dQBmSpCf0CvckFwAHgU+MemxV7ayq6aqanppa8CsAJUljGvsO1SRvBF4LbKmq6jY/BJw4p9mGbpukI0SfO0y1eox15p7kdOCdwFlV9d05u3YD5yV5epKTgE3AF/uXKUkaxaJn7kkuA14JrE2yD7iQwdUxTweuTQJwY1W9uaruTHIlcBeD4Zq3VtX/LlXxkqSFLRruVXX+ApsvOUz79wLv7VOUJKkf71CVpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBfkG2pFXDL9eeHM/cJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNWjTck1ya5ECSO+ZsOy7JtUnu7X4e221Pkg8m2ZvktiSnLGXxkqSFDXPm/lHg9HnbdgB7qmoTsKdbB3gNsKn7tx348GTKlCSNYtFwr6obgG/P23w2sKtb3gWcM2f7x2rgRuCYJCdMqlhJ0nDGHXNfV1X7u+WHgXXd8nrgwTnt9nXbniTJ9iQzSWZmZ2fHLEOStJDeH6hWVQE1xnE7q2q6qqanpqb6liFJmmPccH/kieGW7ueBbvtDwIlz2m3otkmSltG44b4b2NotbwWunrP9Dd1VM6cBj80ZvpEkLZM1izVIchnwSmBtkn3AhcBFwJVJtgEPAOd2zT8NnAHsBb4LvGkJapYkLWLRcK+q8w+xa8sCbQt4a9+iJEn9eIeqJDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBi35ZhyQdCTbuuGbsY++/6MwJVrI6eOYuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDeoV7kt9LcmeSO5JcluQZSU5KclOSvUmuSHL0pIqVJA1n7HBPsh74HWC6qk4GjgLOAy4G3l9VLwS+A2ybRKGSpOH1vUN1DfDDSR4HngnsB14F/Gq3fxfwHuDDPZ9Hesrxjkv1MfaZe1U9BPwZ8A0Gof4YcDPwaFUd7JrtA9YvdHyS7UlmkszMzs6OW4YkaQF9hmWOBc4GTgKeDzwLOH3Y46tqZ1VNV9X01NTUuGVIkhbQZ1jm1cDXq2oWIMkngZcBxyRZ0529bwAe6l+mpFH0GdJRG/pcLfMN4LQkz0wSYAtwF3Ad8LquzVbg6n4lSpJG1WfM/SbgKuDLwO3dY+0E3gW8I8le4HjgkgnUKUkaQa+rZarqQuDCeZvvA07t87iSpH68Q1WSGmS4S1KDDHdJapDfoSotwjtFdSQy3KUl5PXmWikOy0hSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkN6hXuSY5JclWSryS5O8lLkxyX5Nok93Y/j51UsZKk4fQ9c/8A8E9V9ZPAzwF3AzuAPVW1CdjTrUuSltHY4Z7kucDLgUsAqur7VfUocDawq2u2Czinb5GSpNH0OXM/CZgF/ibJLUk+kuRZwLqq2t+1eRhYt9DBSbYnmUkyMzs726MMSdJ8fcJ9DXAK8OGqegnw38wbgqmqAmqhg6tqZ1VNV9X01NRUjzIkSfP1Cfd9wL6quqlbv4pB2D+S5ASA7ueBfiVKkkY1drhX1cPAg0l+otu0BbgL2A1s7bZtBa7uVaEkaWRreh7/NuATSY4G7gPexOAPxpVJtgEPAOf2fA5J0oh6hXtV3QpML7BrS5/HlST14x2qktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QG9f0mJmnV27jjmpUuQVp2nrlLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktSg3uGe5KgktyT5x279pCQ3Jdmb5IokR/cvU5I0ikmcub8duHvO+sXA+6vqhcB3gG0TeA5J0gh6hXuSDcCZwEe69QCvAq7qmuwCzunzHJKk0fU9c/8L4J3AD7r144FHq+pgt74PWN/zOSRJIxo73JO8FjhQVTePefz2JDNJZmZnZ8ctQ5K0gD5n7i8DzkpyP3A5g+GYDwDHJHliQrINwEMLHVxVO6tquqqmp6amepQhSZpv7HCvqndX1Yaq2gicB3yuqn4NuA54XddsK3B17yolSSNZiil/3wVcnuSPgVuAS5bgOSRpYvpOC33/RWdOqJLJmUi4V9X1wPXd8n3AqZN4XEnSeLxDVZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktSgpbhDVVpQ37sAJQ3PM3dJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CCn/NVInLZXOjJ45i5JDRo73JOcmOS6JHcluTPJ27vtxyW5Nsm93c9jJ1euJGkYfc7cDwK/X1WbgdOAtybZDOwA9lTVJmBPty5JWkZjh3tV7a+qL3fL/wncDawHzgZ2dc12Aef0LVKSNJqJjLkn2Qi8BLgJWFdV+7tdDwPrDnHM9iQzSWZmZ2cnUYYkqdM73JP8CPD3wO9W1X/M3VdVBdRCx1XVzqqarqrpqampvmVIkuboFe5JfohBsH+iqj7ZbX4kyQnd/hOAA/1KlCSNqs/VMgEuAe6uqvfN2bUb2NotbwWuHr88SdI4+tzE9DLg14Hbk9zabfsD4CLgyiTbgAeAc/uVKEka1djhXlX/CuQQu7eM+7iSpP6cfuApyCkEpPY5/YAkNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDnBVyhfSZmfH+i86cYCWSWpTB15yurOnp6ZqZmVnpMkbm1LmS+upzspbk5qqaXmifwzKS1CDDXZIaZLhLUoOe8h+oOm4uqUWeuUtSgwx3SWqQ4S5JDTrix9wdM5ekJ1uyM/ckpyf5apK9SXYs1fNIkp5sScI9yVHAh4DXAJuB85NsXornkiQ92VKduZ8K7K2q+6rq+8DlwNlL9FySpHmWasx9PfDgnPV9wM/PbZBkO7C9W/2vJF9d4HHWAt9akgpXRmv9gfb6ZH9Wv6b6lIt79efHD7VjxT5QraqdwM7DtUkyc6hJcY5ErfUH2uuT/Vn9WuvTUvVnqYZlHgJOnLO+odsmSVoGSxXuXwI2JTkpydHAecDuJXouSdI8SzIsU1UHk/w28M/AUcClVXXnGA912GGbI1Br/YH2+mR/Vr/W+rQk/VkVX9YhSZospx+QpAYZ7pLUoFUV7kmOS3Jtknu7n8cu0ObFSb6Q5M4ktyX5lZWo9XAWm3ohydOTXNHtvynJxuWvcnhD9OcdSe7qXo89SQ557e1qMez0GEl+OUklWdWX3g3TnyTndq/TnUn+drlrHMUQv3M/luS6JLd0v3dnrESdw0pyaZIDSe44xP4k+WDX39uSnNL7Satq1fwD/gTY0S3vAC5eoM2LgE3d8vOB/cAxK137nPqOAr4GvAA4Gvg3YPO8Nr8F/FW3fB5wxUrX3bM/vwA8s1t+y2ruz7B96to9G7gBuBGYXum6e75Gm4BbgGO79R9d6bp79mcn8JZueTNw/0rXvUifXg6cAtxxiP1nAJ8BApwG3NT3OVfVmTuDKQp2dcu7gHPmN6iqe6rq3m75m8ABYGrZKlzcMFMvzO3nVcCWJFnGGkexaH+q6rqq+m63eiOD+xpWs2Gnx/gj4GLgf5azuDEM05/fBD5UVd8BqKoDy1zjKIbpTwHP6ZafC3xzGesbWVXdAHz7ME3OBj5WAzcCxyQ5oc9zrrZwX1dV+7vlh4F1h2uc5FQGf9m/ttSFjWChqRfWH6pNVR0EHgOOX5bqRjdMf+baxuAMZDVbtE/d2+ITq+pImFN6mNfoRcCLknw+yY1JTl+26kY3TH/eA7w+yT7g08Dblqe0JTPq/7NFLfv0A0k+CzxvgV0XzF2pqkpyyOs0u79qHwe2VtUPJlulxpHk9cA08IqVrqWPJE8D3ge8cYVLmaQ1DIZmXsngndUNSX6mqh5d0arGdz7w0ar68yQvBT6e5GSz4P8te7hX1asPtS/JI0lOqKr9XXgv+NYxyXOAa4ALurcwq8kwUy880WZfkjUM3lb++/KUN7KhppJI8moGf6BfUVXfW6baxrVYn54NnAxc342WPQ/YneSsqppZtiqHN8xrtI/BOO7jwNeT3MMg7L+0PCWOZJj+bANOB6iqLyR5BoMJxVbzcNPhTHzKltU2LLMb2NotbwWunt+gm87gUwzGp65axtqGNczUC3P7+Trgc9V9qrIKLdqfJC8B/ho4a5WP5T7hsH2qqseqam1VbayqjQw+R1itwQ7D/c79A4OzdpKsZTBMc99yFjmCYfrzDWALQJKfAp4BzC5rlZO1G3hDd9XMacBjc4aox7PSnyLP+8T4eGAPcC/wWeC4bvs08JFu+fXA48Ctc/69eKVrn9ePM4B7GHwWcEG37Q8ZBAQMfhH/DtgLfBF4wUrX3LM/nwUemfN67F7pmvv2aV7b61nFV8sM+RqFwVDTXcDtwHkrXXPP/mwGPs/gSppbgV9c6ZoX6c9lDK7se5zBu6htwJuBN895fT7U9ff2Sfy+Of2AJDVotQ3LSJImwHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDfo/bBUen/+/C78AAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["AUC score is 0.5071803140870595  while binary score is 0.5491803278688525 and the confusion matrix is [[162 326]\n"," [114 374]] on dev loader\n","Model is saved after 0 iterations \n","OCloss is 1.957892656326294 and BCEloss is 0.767629086971283 and MNRloss is 0.0  and time taken is 7023.2809138298035 after 0 iterations\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569.0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["OCloss is 1.5010188406223541 and BCEloss is 0.6959589685236182 and MNRloss is 0.0  and time taken is 8236.798710346222 after 1000 iterations\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["OCloss is 1.5080556696888687 and BCEloss is 0.6971492767333984 and MNRloss is 0.0  and time taken is 9448.8817050457 after 2000 iterations\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569\n","OCloss is 1.498464621660591 and BCEloss is 0.6969733875578779 and MNRloss is 0.0  and time taken is 10660.507197380066 after 3000 iterations\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569.0\n","2 Epoch completed. OCloss is 1.4933198050405578 and BCEloss is 0.69667083659712 and MNRloss is 0.0\n","    0 iterations have been done in 0.6566252708435059 seconds\n","image file is truncated (3 bytes not processed)\n","Something went wrong with the image of id 164569.0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["    1000 iterations have been done in 694.0932991504669 seconds\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-23b827d8fff8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Overall parameters in model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-38-3aa2a0a5c18d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, model, num_epochs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mepoch_MNR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_MNRloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{epoch+1} Epoch completed. OCloss is {epoch_OC} and BCEloss is {epoch_BCE} and MNRloss is {epoch_MNR}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAUC_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'AUC and Bin_acc after training {epoch+1} on train dataloader is {a,b} and confusion matrix is {c}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAUC_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-35-ecf8bf4f292e>\u001b[0m in \u001b[0;36mcal\u001b[0;34m(self, model, loader)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myhat2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"ZuGYeCAq97yQ"},"source":["# Evaluator"]},{"cell_type":"markdown","metadata":{"id":"kYlESuc6HmGF"},"source":["## Binary Classification Evaluator"]},{"cell_type":"code","metadata":{"id":"NJ-NKaASKXoM"},"source":["# from . import SentenceEvaluator\n","import logging\n","import os\n","import csv\n","from sklearn.metrics.pairwise import paired_cosine_distances, paired_euclidean_distances, paired_manhattan_distances\n","from sklearn.metrics import average_precision_score\n","import numpy as np\n","from typing import List\n","# from ..readers import InputExample\n","\n","\n","logger = logging.getLogger(__name__)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWrcz7PMHlaJ"},"source":["class BinaryClassificationEvaluator():\n","    \"\"\"\n","    Evaluate a model based on the similarity of the embeddings by calculating the accuracy of identifying similar and\n","    dissimilar sentences.\n","    The metrics are the cosine similarity as well as euclidean and Manhattan distance\n","    The returned score is the accuracy with a specified metric.\n","    The results are written in a CSV. If a CSV already exists, then values are appended.\n","    The labels need to be 0 for dissimilar pairs and 1 for similar pairs.\n","    :param sentences1: The first column of sentences\n","    :param sentences2: The second column of sentences\n","    :param labels: labels[i] is the label for the pair (sentences1[i], sentences2[i]). Must be 0 or 1\n","    :param name: Name for the output\n","    :param batch_size: Batch size used to compute embeddings\n","    :param show_progress_bar: If true, prints a progress bar\n","    :param write_csv: Write results to a CSV file\n","    \"\"\"\n","\n","    def __init__(self,\n","                 dataset,\n","                 name: str = '',\n","                 batch_size: int = 32,\n","                 show_progress_bar: bool = False,\n","                 write_csv: bool = True\n","                 ):\n","        \n","        self.dataset = dataset\n","        self.labels = list()\n","        self.write_csv = write_csv\n","        self.name = name\n","        self.batch_size = batch_size\n","        self.dataloader = DataLoader(\n","            self.dataset,\n","            batch_size=self.batch_size,\n","            pin_memory=True,\n","            num_workers = 8,\n","            shuffle = True\n","        )\n","\n","        if show_progress_bar is None:\n","            show_progress_bar = (logger.getEffectiveLevel() == logging.INFO or logger.getEffectiveLevel() == logging.DEBUG)\n","        self.show_progress_bar = show_progress_bar\n","\n","        self.csv_file = \"binary_classification_evaluation\" + (\"_\"+name if name else '') + \"_results.csv\"\n","        self.csv_headers = [\"epoch\", \"steps\",\n","                            \"cosine_acc\", \"cosine_acc_threshold\", \"cosine_f1\", \"cosine_precision\", \"cosine_recall\", \"cosine_f1_threshold\", \"cosine_average_precision\",\n","                            \"manhatten_acc\", \"manhatten_acc_threshold\", \"manhatten_f1\", \"manhatten_precision\", \"manhatten_recall\", \"manhatten_f1_threshold\", \"manhatten_average_precision\",\n","                            \"eucledian_acc\", \"eucledian_acc_threshold\", \"eucledian_f1\", \"eucledian_precision\", \"eucledian_recall\", \"eucledian_f1_threshold\", \"eucledian_average_precision\"]\n","\n","\n","    # @classmethod\n","    # def from_input_examples(cls, examples: List[InputExample], **kwargs):\n","    #     sentences1 = []\n","    #     sentences2 = []\n","    #     scores = []\n","\n","    #     for example in examples:\n","    #         sentences1.append(example.texts[0])\n","    #         sentences2.append(example.texts[1])\n","    #         scores.append(example.label)\n","    #     return cls(sentences1, sentences2, scores, **kwargs)\n","\n","    def __call__(self, model, output_path: str = None, epoch: int = -1, steps: int = -1) -> float:\n","\n","        if epoch != -1:\n","            if steps == -1:\n","                out_txt = f\" after epoch {epoch}:\"\n","            else:\n","                out_txt = f\" in epoch {epoch} after {steps} steps:\"\n","        else:\n","            out_txt = \":\"\n","\n","        logger.info(\"Binary Accuracy Evaluation of the model on \" + self.name + \" dataset\" + out_txt)\n","        \n","        embeddings1 = list()\n","        embeddings2 = list()\n","\n","        with torch.no_grad():\n","            model.eval()\n","            for i, batch in enumerate(self.dataloader):\n","                images = batch[\"image\"]\n","                label = batch[\"label\"]\n","                label = label.float()\n","                token = batch[\"token\"]\n","                \n","                # print(images,label,token)\n","\n","                images[0],images[1] = images[0].to(device),images[1].to(device)\n","                token[0],token[1] = token[0].to(device), token[1].to(device)\n","                label = label.to(device)\n","                \n","                # compute the model output\n","                yhat1 = model(images[0], token[0])\n","                yhat2 = model(images[1],token[1])\n","\n","                for j in yhat1:\n","                    embeddings1.append(j.cpu().detach().numpy())\n","                for j in yhat2:\n","                    embeddings2.append(j.cpu().detach().numpy())\n","                for j in label:\n","                    self.labels.append(float(j))\n","                \n","                if(i%30==0 and i!=0):\n","                    print(f'Completed {i} iterations')\n","\n","        cosine_scores = 1-paired_cosine_distances(embeddings1, embeddings2)\n","        manhattan_distances = paired_manhattan_distances(embeddings1, embeddings2)\n","        euclidean_distances = paired_euclidean_distances(embeddings1, embeddings2)\n","\n","\n","        labels = np.asarray(self.labels)\n","\n","        file_output_data = [epoch, steps]\n","\n","        main_score = None\n","        for name, scores, reverse in [['Cosine-Similarity', cosine_scores, True], ['Manhatten-Distance', manhattan_distances, False], ['Euclidean-Distance', euclidean_distances, False]]:\n","            acc, acc_threshold = self.find_best_acc_and_threshold(scores, labels, reverse)\n","            f1, precision, recall, f1_threshold = self.find_best_f1_and_threshold(scores, labels, reverse)\n","            ap = average_precision_score(labels, scores * (1 if reverse else -1))\n","\n","            logger.info(\"Accuracy with {}:           {:.2f}\\t(Threshold: {:.4f})\".format(name, acc * 100, acc_threshold))\n","            logger.info(\"F1 with {}:                 {:.2f}\\t(Threshold: {:.4f})\".format(name, f1 * 100, f1_threshold))\n","            logger.info(\"Precision with {}:          {:.2f}\".format(name, precision * 100))\n","            logger.info(\"Recall with {}:             {:.2f}\".format(name, recall * 100))\n","            logger.info(\"Average Precision with {}:  {:.2f}\\n\".format(name, ap * 100))\n","\n","            file_output_data.extend([acc, acc_threshold, f1, precision, recall, f1_threshold, ap])\n","\n","            if main_score is None: #Use AveragePrecision with Cosine-Similarity as main score\n","                main_score = ap\n","\n","        if output_path is not None and self.write_csv:\n","            csv_path = os.path.join(output_path, self.csv_file)\n","            if not os.path.isfile(csv_path):\n","                with open(csv_path, mode=\"w\", encoding=\"utf-8\") as f:\n","                    writer = csv.writer(f)\n","                    writer.writerow(self.csv_headers)\n","                    writer.writerow(file_output_data)\n","            else:\n","                with open(csv_path, mode=\"a\", encoding=\"utf-8\") as f:\n","                    writer = csv.writer(f)\n","                    writer.writerow(file_output_data)\n","\n","        return main_score\n","\n","    @staticmethod\n","    def find_best_acc_and_threshold(scores, labels, high_score_more_similar: bool):\n","        # assert len(scores) == len(labels)\n","        rows = list(zip(scores, labels))\n","\n","        rows = sorted(rows, key=lambda x: x[0], reverse=high_score_more_similar)\n","\n","        max_acc = 0\n","        best_threshold = -1\n","\n","        positive_so_far = 0\n","        remaining_negatives = sum(labels == 0)\n","\n","        for i in range(len(rows)-1):\n","            score, label = rows[i]\n","            if label == 1:\n","                positive_so_far += 1\n","            else:\n","                remaining_negatives -= 1\n","\n","            acc = (positive_so_far + remaining_negatives) / len(labels)\n","            if acc > max_acc:\n","                max_acc = acc\n","                best_threshold = (rows[i][0] + rows[i+1][0]) / 2\n","\n","        return max_acc, best_threshold\n","\n","    @staticmethod\n","    def find_best_f1_and_threshold(scores, labels, high_score_more_similar: bool):\n","        # assert len(scores) == len(labels)\n","\n","        scores = np.asarray(scores)\n","        labels = np.asarray(labels)\n","\n","        rows = list(zip(scores, labels))\n","\n","        rows = sorted(rows, key=lambda x: x[0], reverse=high_score_more_similar)\n","\n","        best_f1 = best_precision = best_recall = 0\n","        threshold = 0\n","        nextract = 0\n","        ncorrect = 0\n","        total_num_duplicates = sum(labels)\n","\n","        for i in range(len(rows)-1):\n","            score, label = rows[i]\n","            nextract += 1\n","\n","            if label == 1:\n","                ncorrect += 1\n","\n","            if ncorrect > 0:\n","                precision = ncorrect / nextract\n","                recall = ncorrect / total_num_duplicates\n","                f1 = 2 * precision * recall / (precision + recall)\n","                if f1 > best_f1:\n","                    best_f1 = f1\n","                    best_precision = precision\n","                    best_recall = recall\n","                    threshold = (rows[i][0] + rows[i + 1][0]) / 2\n","\n","        return best_f1, best_precision, best_recall, threshold"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iSnnK9vk-U_J","colab":{"base_uri":"https://localhost:8080/","height":264},"executionInfo":{"status":"error","timestamp":1616401366324,"user_tz":-330,"elapsed":3189,"user":{"displayName":"Ticket project _CKM","photoUrl":"","userId":"12122662661545911577"}},"outputId":"8b021db4-396b-4051-9d10-7864d94937ad"},"source":["dev_BCEvaluator = BinaryClassificationEvaluator(dev_dataset,batch_size=BATCH_SIZE,show_progress_bar=True)\n","os.makedirs(folder+'/dev',exist_ok = True)\n","dev_BCEvaluator(model,output_path=folder+'/dev')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-ddab3ad768a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdev_BCEvaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinaryClassificationEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/dev'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexist_ok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdev_BCEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/dev'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"code","metadata":{"id":"pdO-HPtiEcIn"},"source":["train_BCEvaluator = BinaryClassificationEvaluator(train_dataset,batch_size=BATCH_SIZE,show_progress_bar=True)\n","os.makedirs(folder+'/train',exist_ok = True)\n","train_BCEvaluator(model,output_path=folder+'/train')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MQ3f5gO7Hhu0"},"source":["## Information retreival evaluator"]},{"cell_type":"code","metadata":{"id":"f4yW9Gn2DL9i"},"source":["import torch\n","import logging\n","from tqdm import tqdm, trange\n","import os\n","import numpy as np\n","from typing import List, Tuple, Dict, Set"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"izMNQie5XZmi"},"source":["class infodataset(Dataset):\n","    def __init__(self,qr,qr_idx,img_dir,transform = None):\n","        self.qr = qr\n","        self.qr_idx = qr_idx\n","        self.img_dir = img_dir\n","        self.transform = transform\n","\n","    def image_adder(self,id1):\n","        img_id1 = list()\n","        if((self.qr.at[id1,'Attachments'])!=None):\n","            for i in self.qr.at[id1,'Attachments']:\n","                try:\n","                    img_path = os.path.join(self.img_dir,i)\n","                    img = Image.open(img_path).convert('RGB')\n","                    if(self.transform):\n","                        img = self.transform(img)\n","                        img.reshape(3,224,224)\n","                    img_id1.append(img)\n","                except Exception as e: \n","                    print(e)\n","        else:\n","            img_id1.append(torch.zeros(3,224,224))\n","\n","        # Work on this, for few examples, it is still saying list index out of range\n","        if(len(img_id1)==0):\n","            # print('No attachments found for id {}'.format(id1))\n","            print(f'Something went wrong with the image of id {id1}')\n","            img_id1.append(torch.zeros(3,224,224))\n","\n","        return img_id1\n","    \n","    def __getitem__(self,idx):\n","        id1 = self.qr_idx[idx]\n","        img_id1 = self.image_adder(id1)\n","\n","        # print(len(img_id1))\n","\n","        # print('Printing id1 {} and len {} and id2 {} and len {} '.format(\n","        #     id1,len(img_id1),\n","        #     id2, len(img_id2)\n","        # ))\n","\n","        # print('Printing id1 shape {} and id2  shape {}'.format(\n","        #     img_id1[0].shape,\n","        #     img_id2[0].shape\n","        # ))        \n","\n","        sample = {\n","            'image': img_id1[0]    #Currently taking only one input image\n","        }\n","\n","        t1 = '[CLS]' + self.qr.loc[id1,'Title'] + ' ' + ' '.join(self.qr.loc[id1,'Tags']) + ' ' + self.qr.loc[id1,'Text'] + '[SEP]'\n","        tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","        t1_token = tokenizer.tokenize(t1)\n","        indexed_t1 = tokenizer.convert_tokens_to_ids(t1_token)\n","        \n","        while(len(indexed_t1)<512):\n","            indexed_t1.append(0)\n","        \n","        ten_t1 = torch.tensor(indexed_t1)[:512]\n","        \n","        try:\n","            sample[\"token\"] = ten_t1 # torch.Size([batch_size, 512])\n","        except Exception as e:\n","            print(e)\n","        \n","        return sample\n","\n","    def __len__(self):\n","        return len(self.qr_idx)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qhOlgHW9El3T"},"source":["class InformationRetreivalEvaluator():\n","    def __init__(self,\n","                 qr,\n","                 queries: Dict[str, str],  #qid => query\n","                 corpus: Dict[str, str],  #cid => doc\n","                 relevant_docs: Dict[str, Set[str]],  #qid => Set[cid]\n","                 corpus_chunk_size: int = 50000,\n","                 mrr_at_k: List[int] = [10],\n","                 ndcg_at_k: List[int] = [10],\n","                 accuracy_at_k: List[int] = [1, 3, 5, 10],\n","                 precision_recall_at_k: List[int] = [1, 3, 5, 10],\n","                 map_at_k: List[int] = [100],\n","                 show_progress_bar: bool = False,\n","                 batch_size: int = 32,\n","                 name: str = '',\n","                 write_csv: bool = True\n","                 ):\n","        \n","        self.qr = qr\n","        self.queries_ids = []\n","        for qid in queries:\n","            if qid in relevant_docs and len(relevant_docs[qid]) > 0:\n","                self.queries_ids.append(qid)\n","\n","        self.queries = [queries[qid] for qid in self.queries_ids]\n","\n","        self.corpus_ids = list(corpus.keys())\n","        self.corpus = [corpus[cid] for cid in self.corpus_ids]\n","\n","        self.relevant_docs = relevant_docs\n","        self.corpus_chunk_size = corpus_chunk_size\n","        self.mrr_at_k = mrr_at_k\n","        self.ndcg_at_k = ndcg_at_k\n","        self.accuracy_at_k = accuracy_at_k\n","        self.precision_recall_at_k = precision_recall_at_k\n","        self.map_at_k = map_at_k\n","\n","        self.show_progress_bar = show_progress_bar\n","        self.batch_size = batch_size\n","        self.name = name\n","        self.write_csv = write_csv\n","\n","        if name:\n","            name = \"_\" + name\n","\n","        self.csv_file: str = \"Information-Retrieval_evaluation\" + name + \"_results.csv\"\n","        self.csv_headers = [\"epoch\", \"steps\"]\n","\n","\n","        for k in accuracy_at_k:\n","            self.csv_headers.append(\"Accuracy@{}\".format(k))\n","\n","        for k in precision_recall_at_k:\n","            self.csv_headers.append(\"Precision@{}\".format(k))\n","            self.csv_headers.append(\"Recall@{}\".format(k))\n","\n","        for k in mrr_at_k:\n","            self.csv_headers.append(\"MRR@{}\".format(k))\n","\n","        for k in ndcg_at_k:\n","            self.csv_headers.append(\"NDCG@{}\".format(k))\n","\n","        for k in map_at_k:\n","            self.csv_headers.append(\"MAP@{}\".format(k))\n","    \n","    def __call__(self,model : BridgeModel,output_path: str = None,epoch: int = -1, steps: int = -1) ->float:\n","        if epoch != -1:\n","            out_txt = \" after epoch {}:\".format(epoch) if steps == -1 else \" in epoch {} after {} steps:\".format(epoch, steps)\n","        else:\n","            out_txt = \":\"\n","\n","        logger.info(\"Information Retrieval Evaluation on \" + self.name + \" dataset\" + out_txt)\n","\n","        max_k = max(max(self.mrr_at_k), max(self.ndcg_at_k), max(self.accuracy_at_k), max(self.precision_recall_at_k), max(self.map_at_k))\n","\n","        query_embeddings = self.get_embeddings(model,self.qr,self.queries_ids)\n","\n","        queries_result_list = [[] for _ in range(len(query_embeddings))]\n","\n","        itr = range(0, len(self.corpus), self.corpus_chunk_size)\n","\n","        if self.show_progress_bar:\n","            itr = tqdm(itr, desc='Corpus Chunks')\n","\n","        #Iterate over chunks of the corpus\n","        for corpus_start_idx in itr:\n","            corpus_end_idx = min(corpus_start_idx + self.corpus_chunk_size, len(self.corpus))\n","\n","            #Encode chunk of corpus\n","            sub_corpus_embeddings = self.get_embeddings(model,self.qr,self.corpus_ids[corpus_start_idx:corpus_end_idx])\n","\n","            #Compute cosine similarites\n","            cos_scores = pytorch_cos_sim(query_embeddings, sub_corpus_embeddings)\n","            del sub_corpus_embeddings\n","\n","            #Get top-k values\n","            cos_scores_top_k_values, cos_scores_top_k_idx = torch.topk(cos_scores, min(max_k, len(cos_scores[0])), dim=1, largest=True, sorted=False)\n","            cos_scores_top_k_values = cos_scores_top_k_values.cpu().tolist()\n","            cos_scores_top_k_idx = cos_scores_top_k_idx.cpu().tolist()\n","            del cos_scores\n","\n","            for query_itr in range(len(query_embeddings)):\n","                for sub_corpus_id, score in zip(cos_scores_top_k_idx[query_itr], cos_scores_top_k_values[query_itr]):\n","                    corpus_id = self.corpus_ids[corpus_start_idx+sub_corpus_id]\n","                    queries_result_list[query_itr].append({'corpus_id': corpus_id, 'score': score})\n","\n","\n","        #Compute scores\n","        scores = self.compute_metrics(queries_result_list)\n","\n","        #Output\n","        self.output_scores(scores)\n","\n","\n","        # logger.info(\"Queries: {}\".format(len(self.queries)))\n","        # logger.info(\"Corpus: {}\\n\".format(len(self.corpus)))\n","\n","        if output_path is not None and self.write_csv:\n","            csv_path = os.path.join(output_path, self.csv_file)\n","            if not os.path.isfile(csv_path):\n","                fOut = open(csv_path, mode=\"w\", encoding=\"utf-8\")\n","                fOut.write(\",\".join(self.csv_headers))\n","                fOut.write(\"\\n\")\n","\n","            else:\n","                fOut = open(csv_path, mode=\"a\", encoding=\"utf-8\")\n","\n","            output_data = [epoch, steps]\n","            for k in self.accuracy_at_k:\n","                output_data.append(scores['accuracy@k'][k])\n","\n","            for k in self.precision_recall_at_k:\n","                output_data.append(scores['precision@k'][k])\n","                output_data.append(scores['recall@k'][k])\n","\n","            for k in self.mrr_at_k:\n","                output_data.append(scores['mrr@k'][k])\n","\n","            for k in self.ndcg_at_k:\n","                output_data.append(scores['ndcg@k'][k])\n","\n","            for k in self.map_at_k:\n","                output_data.append(scores['map@k'][k])\n","\n","            fOut.write(\",\".join(map(str,output_data)))\n","            fOut.write(\"\\n\")\n","            fOut.close()\n","\n","        return scores['map@k'][max(self.map_at_k)]\n","\n","\n","    def compute_metrics(self, queries_result_list: List[object]):\n","        # Init score computation values\n","        num_hits_at_k = {k: 0 for k in self.accuracy_at_k}\n","        precisions_at_k = {k: [] for k in self.precision_recall_at_k}\n","        recall_at_k = {k: [] for k in self.precision_recall_at_k}\n","        MRR = {k: 0 for k in self.mrr_at_k}\n","        ndcg = {k: [] for k in self.ndcg_at_k}\n","        AveP_at_k = {k: [] for k in self.map_at_k}\n","\n","        # Compute scores on results\n","        for query_itr in range(len(queries_result_list)):\n","            query_id = self.queries_ids[query_itr]\n","\n","            # Sort scores\n","            top_hits = sorted(queries_result_list[query_itr], key=lambda x: x['score'], reverse=True)\n","            query_relevant_docs = self.relevant_docs[query_id]\n","\n","            # Accuracy@k - We count the result correct, if at least one relevant doc is accross the top-k documents\n","            for k_val in self.accuracy_at_k:\n","                for hit in top_hits[0:k_val]:\n","                    if hit['corpus_id'] in query_relevant_docs:\n","                        num_hits_at_k[k_val] += 1\n","                        break\n","\n","            # Precision and Recall@k\n","            for k_val in self.precision_recall_at_k:\n","                num_correct = 0\n","                for hit in top_hits[0:k_val]:\n","                    if hit['corpus_id'] in query_relevant_docs:\n","                        num_correct += 1\n","\n","                precisions_at_k[k_val].append(num_correct / k_val)\n","                recall_at_k[k_val].append(num_correct / len(query_relevant_docs))\n","\n","            # MRR@k\n","            for k_val in self.mrr_at_k:\n","                for rank, hit in enumerate(top_hits[0:k_val]):\n","                    if hit['corpus_id'] in query_relevant_docs:\n","                        MRR[k_val] += 1.0 / (rank + 1)\n","                        break\n","\n","            # NDCG@k\n","            for k_val in self.ndcg_at_k:\n","                predicted_relevance = [1 if top_hit['corpus_id'] in query_relevant_docs else 0 for top_hit in top_hits[0:k_val]]\n","                true_relevances = [1] * len(query_relevant_docs)\n","\n","                ndcg_value = self.compute_dcg_at_k(predicted_relevance, k_val) / self.compute_dcg_at_k(true_relevances, k_val)\n","                ndcg[k_val].append(ndcg_value)\n","\n","            # MAP@k\n","            for k_val in self.map_at_k:\n","                num_correct = 0\n","                sum_precisions = 0\n","\n","                for rank, hit in enumerate(top_hits[0:k_val]):\n","                    if hit['corpus_id'] in query_relevant_docs:\n","                        num_correct += 1\n","                        sum_precisions += num_correct / (rank + 1)\n","\n","                avg_precision = sum_precisions / min(k_val, len(query_relevant_docs))\n","                AveP_at_k[k_val].append(avg_precision)\n","\n","        # Compute averages\n","        for k in num_hits_at_k:\n","            num_hits_at_k[k] /= len(self.queries_ids)\n","\n","        for k in precisions_at_k:\n","            precisions_at_k[k] = np.mean(precisions_at_k[k])\n","\n","        for k in recall_at_k:\n","            recall_at_k[k] = np.mean(recall_at_k[k])\n","\n","        for k in ndcg:\n","            ndcg[k] = np.mean(ndcg[k])\n","\n","        for k in MRR:\n","            MRR[k] /= len(self.queries_ids)\n","\n","        for k in AveP_at_k:\n","            AveP_at_k[k] = np.mean(AveP_at_k[k])\n","\n","\n","        return {'accuracy@k': num_hits_at_k, 'precision@k': precisions_at_k, 'recall@k': recall_at_k, 'ndcg@k': ndcg, 'mrr@k': MRR, 'map@k': AveP_at_k}\n","\n","\n","    def output_scores(self, scores):\n","        for k in scores['accuracy@k']:\n","            logger.info(\"Accuracy@{}: {:.2f}%\".format(k, scores['accuracy@k'][k]*100))\n","\n","        for k in scores['precision@k']:\n","            logger.info(\"Precision@{}: {:.2f}%\".format(k, scores['precision@k'][k]*100))\n","\n","        for k in scores['recall@k']:\n","            logger.info(\"Recall@{}: {:.2f}%\".format(k, scores['recall@k'][k]*100))\n","\n","        for k in scores['mrr@k']:\n","            logger.info(\"MRR@{}: {:.4f}\".format(k, scores['mrr@k'][k]))\n","\n","        for k in scores['ndcg@k']:\n","            logger.info(\"NDCG@{}: {:.4f}\".format(k, scores['ndcg@k'][k]))\n","\n","        for k in scores['map@k']:\n","            logger.info(\"MAP@{}: {:.4f}\".format(k, scores['map@k'][k]))\n","\n","\n","    @staticmethod\n","    def compute_dcg_at_k(relevances, k):\n","        dcg = 0\n","        for i in range(min(len(relevances), k)):\n","            dcg += relevances[i] / np.log2(i + 2)  #+2 as we start our idx at 0\n","        return dcg\n","    \n","    def get_embeddings(self,model,qr,qr_idx):\n","        info_dataset = infodataset(qr,qr_idx,img_dir,transform = transform_pipe)\n","        info_loader = DataLoader(\n","            info_dataset,\n","            batch_size=BATCH_SIZE,\n","            pin_memory=True,\n","            num_workers = 8,\n","            )\n","        \n","        embeddings = list()\n","        since = time.time()\n","        for i,batch in enumerate(info_loader):\n","            model.eval()\n","\n","            text = batch['token']\n","            images = batch['image']\n","\n","            text,images = torch.tensor(text).to(device), torch.tensor(images).to(device)\n","\n","            with torch.no_grad():\n","                yhat = model.forward(images,text)\n","            \n","            for j in yhat:\n","                embeddings.append(j.cpu().detach().numpy())\n","            \n","            if(i%40==0):\n","                print(f'{i} iterations hase been completed, and model is running for {time.time()-since}')\n","        \n","        return embeddings\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pZFzkQT1oKEP"},"source":["with open(folder_quora+'/devinfo_100.txt','rb') as a:\n","    queries_dev = pickle.load(a)\n","    rel_docs_dev= pickle.load(a)\n","\n","with open(folder_quora+'/testinfo_100.txt','rb') as a:\n","    queries_test= pickle.load(a)\n","    rel_docs_test= pickle.load(a)\n","\n","with open(folder_quora+'/traininfo_100.txt','rb') as a:\n","    queries_train= pickle.load(a)\n","    rel_docs_train= pickle.load(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5XTuVNlbeF-5"},"source":["def corpus(qr):\n","    corpus = dict()\n","    for i in qr.index.values:\n","        corpus[i] = 'I dont care'\n","    return corpus"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zFwNBTJCoOS4"},"source":["train_inforet = InformationRetreivalEvaluator(train_qr,queries_train,corpus(train_qr),rel_docs_train)\n","os.makedirs(folder+'/train',exist_ok=True)\n","train_inforet(model,folder+'/train')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WktSxgTYEqg1"},"source":["dev_inforet = InformationRetreivalEvaluator(pd.concat([train_qr,dev_qr]),queries_dev,corpus(train_qr),rel_docs_dev)\n","os.makedirs(folder+'/dev',exist_ok=True)\n","dev_inforet(model,folder+'/dev')"],"execution_count":null,"outputs":[]}]}